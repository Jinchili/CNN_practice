{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.python import keras\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Flatten, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def evl_metric :\n",
    "#    logloss\n",
    "##should be able to customize evl_metrix (quest for later variation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 1: Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir='D:\\CNN_practice\\leaf identification\\\\train.csv'\n",
    "raw_data=pd.read_csv(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir='D:\\CNN_practice\\leaf identification\\\\test.csv'\n",
    "test_data=pd.read_csv(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>species</th>\n",
       "      <th>margin1</th>\n",
       "      <th>margin2</th>\n",
       "      <th>margin3</th>\n",
       "      <th>margin4</th>\n",
       "      <th>margin5</th>\n",
       "      <th>margin6</th>\n",
       "      <th>margin7</th>\n",
       "      <th>margin8</th>\n",
       "      <th>...</th>\n",
       "      <th>texture55</th>\n",
       "      <th>texture56</th>\n",
       "      <th>texture57</th>\n",
       "      <th>texture58</th>\n",
       "      <th>texture59</th>\n",
       "      <th>texture60</th>\n",
       "      <th>texture61</th>\n",
       "      <th>texture62</th>\n",
       "      <th>texture63</th>\n",
       "      <th>texture64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Acer_Opalus</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Pterocarya_Stenoptera</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.022461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Quercus_Hartwissiana</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.068359</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.154300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>0.002930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Tilia_Tomentosa</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>Quercus_Variabilis</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.048828</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>Magnolia_Salicifolia</td>\n",
       "      <td>0.070312</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.152340</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.145510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041992</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.013672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>Quercus_Canariensis</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.042969</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040039</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.003906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>11</td>\n",
       "      <td>Quercus_Rubra</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.050781</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038086</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.036133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14</td>\n",
       "      <td>Quercus_Brantii</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.074219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15</td>\n",
       "      <td>Salix_Fragilis</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.072266</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070312</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.192380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074219</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>17</td>\n",
       "      <td>Zelkova_Serrata</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.024414</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>18</td>\n",
       "      <td>Betula_Austrosinensis</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.076172</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.014648</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.036133</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>20</td>\n",
       "      <td>Quercus_Pontica</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086914</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.002930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>21</td>\n",
       "      <td>Quercus_Afares</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018555</td>\n",
       "      <td>0.022461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>22</td>\n",
       "      <td>Quercus_Coccifera</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.111330</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034180</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.038086</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089844</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.001953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>25</td>\n",
       "      <td>Fagus_Sylvatica</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.066406</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.052734</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>26</td>\n",
       "      <td>Phildelphus</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.044922</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068359</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036133</td>\n",
       "      <td>0.000977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>27</td>\n",
       "      <td>Acer_Palmatum</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.111330</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.079102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>29</td>\n",
       "      <td>Quercus_Pubescens</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.049805</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>30</td>\n",
       "      <td>Populus_Adenopoda</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.333010</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.169920</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>31</td>\n",
       "      <td>Quercus_Trojana</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.016602</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.004883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>32</td>\n",
       "      <td>Quercus_Variabilis</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.080078</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.057617</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>34</td>\n",
       "      <td>Alnus_Sieboldiana</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103520</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>35</td>\n",
       "      <td>Quercus_Ilex</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.068359</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044922</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.069336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>37</td>\n",
       "      <td>Arundinaria_Simonii</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103520</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.077148</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052734</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>38</td>\n",
       "      <td>Acer_Platanoids</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.101560</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.014648</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>40</td>\n",
       "      <td>Quercus_Phillyraeoides</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.042969</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.007812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>42</td>\n",
       "      <td>Cornus_Chinensis</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>0.095703</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.148440</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018555</td>\n",
       "      <td>0.052734</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.041992</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.020508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>43</td>\n",
       "      <td>Quercus_Phillyraeoides</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.044922</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.024414</td>\n",
       "      <td>0.015625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>45</td>\n",
       "      <td>Fagus_Sylvatica</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076172</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049805</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>1541</td>\n",
       "      <td>Quercus_Vulcanica</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.055664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961</th>\n",
       "      <td>1543</td>\n",
       "      <td>Salix_Fragilis</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.072266</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.057617</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>1544</td>\n",
       "      <td>Tilia_Tomentosa</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030273</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963</th>\n",
       "      <td>1545</td>\n",
       "      <td>Populus_Grandidentata</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.121090</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213870</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>1547</td>\n",
       "      <td>Prunus_X_Shmittii</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022461</td>\n",
       "      <td>0.033203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>965</th>\n",
       "      <td>1548</td>\n",
       "      <td>Quercus_x_Hispanica</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.028320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>966</th>\n",
       "      <td>1549</td>\n",
       "      <td>Quercus_Suber</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.048828</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.050781</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032227</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967</th>\n",
       "      <td>1550</td>\n",
       "      <td>Alnus_Viridis</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.026367</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.041016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>968</th>\n",
       "      <td>1551</td>\n",
       "      <td>Acer_Palmatum</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.082031</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.144530</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>969</th>\n",
       "      <td>1552</td>\n",
       "      <td>Quercus_Rhysophylla</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.042969</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.147460</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.018555</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.022461</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>1554</td>\n",
       "      <td>Quercus_Cerris</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.067383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>1555</td>\n",
       "      <td>Quercus_Texana</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.072266</td>\n",
       "      <td>0.076172</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040039</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.022461</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082031</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.029297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>1556</td>\n",
       "      <td>Acer_Mono</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.056641</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.018555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.004883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>1557</td>\n",
       "      <td>Quercus_Vulcanica</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066406</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.096680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030273</td>\n",
       "      <td>0.011719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>1559</td>\n",
       "      <td>Quercus_Nigra</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.056641</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.068359</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.002930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>1561</td>\n",
       "      <td>Rhododendron_x_Russellianum</td>\n",
       "      <td>0.074219</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.119140</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030273</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.000977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>1562</td>\n",
       "      <td>Acer_Capillipes</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.048828</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.028320</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>1563</td>\n",
       "      <td>Quercus_Pubescens</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.070312</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>1566</td>\n",
       "      <td>Alnus_Viridis</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.056641</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.024414</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066406</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>1568</td>\n",
       "      <td>Quercus_x_Hispanica</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.016602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024414</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.003906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>1569</td>\n",
       "      <td>Quercus_Suber</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.058594</td>\n",
       "      <td>0.048828</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024414</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>1570</td>\n",
       "      <td>Viburnum_x_Rhytidophylloides</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.044922</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.083984</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036133</td>\n",
       "      <td>0.082031</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026367</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>1571</td>\n",
       "      <td>Quercus_Nigra</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.074219</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074219</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>1572</td>\n",
       "      <td>Quercus_Phellos</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.076172</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053711</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081055</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.019531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>1574</td>\n",
       "      <td>Ilex_Cornuta</td>\n",
       "      <td>0.080078</td>\n",
       "      <td>0.048828</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.091797</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.032227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>1575</td>\n",
       "      <td>Magnolia_Salicifolia</td>\n",
       "      <td>0.060547</td>\n",
       "      <td>0.119140</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.148440</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.242190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034180</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>1578</td>\n",
       "      <td>Acer_Pictum</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.107420</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.170900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>1581</td>\n",
       "      <td>Alnus_Maximowiczii</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.016602</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>1582</td>\n",
       "      <td>Quercus_Rubra</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.056641</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083008</td>\n",
       "      <td>0.030273</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.014648</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041992</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.002930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>1584</td>\n",
       "      <td>Quercus_Afares</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.022461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>990 rows × 194 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                       species   margin1   margin2   margin3  \\\n",
       "0       1                   Acer_Opalus  0.007812  0.023438  0.023438   \n",
       "1       2         Pterocarya_Stenoptera  0.005859  0.000000  0.031250   \n",
       "2       3          Quercus_Hartwissiana  0.005859  0.009766  0.019531   \n",
       "3       5               Tilia_Tomentosa  0.000000  0.003906  0.023438   \n",
       "4       6            Quercus_Variabilis  0.005859  0.003906  0.048828   \n",
       "5       8          Magnolia_Salicifolia  0.070312  0.093750  0.033203   \n",
       "6      10           Quercus_Canariensis  0.021484  0.031250  0.017578   \n",
       "7      11                 Quercus_Rubra  0.000000  0.000000  0.037109   \n",
       "8      14               Quercus_Brantii  0.005859  0.001953  0.033203   \n",
       "9      15                Salix_Fragilis  0.000000  0.000000  0.009766   \n",
       "10     17               Zelkova_Serrata  0.019531  0.031250  0.001953   \n",
       "11     18         Betula_Austrosinensis  0.001953  0.001953  0.023438   \n",
       "12     20               Quercus_Pontica  0.015625  0.011719  0.041016   \n",
       "13     21                Quercus_Afares  0.011719  0.011719  0.054688   \n",
       "14     22             Quercus_Coccifera  0.011719  0.007812  0.111330   \n",
       "15     25               Fagus_Sylvatica  0.027344  0.025391  0.066406   \n",
       "16     26                   Phildelphus  0.009766  0.062500  0.033203   \n",
       "17     27                 Acer_Palmatum  0.000000  0.000000  0.001953   \n",
       "18     29             Quercus_Pubescens  0.001953  0.000000  0.015625   \n",
       "19     30             Populus_Adenopoda  0.005859  0.027344  0.017578   \n",
       "20     31               Quercus_Trojana  0.019531  0.019531  0.064453   \n",
       "21     32            Quercus_Variabilis  0.019531  0.029297  0.080078   \n",
       "22     34             Alnus_Sieboldiana  0.000000  0.005859  0.023438   \n",
       "23     35                  Quercus_Ilex  0.029297  0.033203  0.033203   \n",
       "24     37           Arundinaria_Simonii  0.001953  0.023438  0.005859   \n",
       "25     38               Acer_Platanoids  0.015625  0.031250  0.101560   \n",
       "26     40        Quercus_Phillyraeoides  0.027344  0.009766  0.021484   \n",
       "27     42              Cornus_Chinensis  0.085938  0.095703  0.007812   \n",
       "28     43        Quercus_Phillyraeoides  0.001953  0.003906  0.017578   \n",
       "29     45               Fagus_Sylvatica  0.019531  0.041016  0.093750   \n",
       "..    ...                           ...       ...       ...       ...   \n",
       "960  1541             Quercus_Vulcanica  0.003906  0.000000  0.013672   \n",
       "961  1543                Salix_Fragilis  0.000000  0.000000  0.007812   \n",
       "962  1544               Tilia_Tomentosa  0.000000  0.017578  0.015625   \n",
       "963  1545         Populus_Grandidentata  0.009766  0.011719  0.121090   \n",
       "964  1547             Prunus_X_Shmittii  0.001953  0.001953  0.007812   \n",
       "965  1548           Quercus_x_Hispanica  0.003906  0.003906  0.033203   \n",
       "966  1549                 Quercus_Suber  0.011719  0.005859  0.048828   \n",
       "967  1550                 Alnus_Viridis  0.000000  0.000000  0.005859   \n",
       "968  1551                 Acer_Palmatum  0.000000  0.000000  0.003906   \n",
       "969  1552           Quercus_Rhysophylla  0.025391  0.042969  0.025391   \n",
       "970  1554                Quercus_Cerris  0.011719  0.013672  0.021484   \n",
       "971  1555                Quercus_Texana  0.000000  0.005859  0.072266   \n",
       "972  1556                     Acer_Mono  0.029297  0.005859  0.056641   \n",
       "973  1557             Quercus_Vulcanica  0.011719  0.005859  0.001953   \n",
       "974  1559                 Quercus_Nigra  0.037109  0.056641  0.019531   \n",
       "975  1561   Rhododendron_x_Russellianum  0.074219  0.093750  0.007812   \n",
       "976  1562               Acer_Capillipes  0.000000  0.000000  0.013672   \n",
       "977  1563             Quercus_Pubescens  0.001953  0.000000  0.041016   \n",
       "978  1566                 Alnus_Viridis  0.000000  0.000000  0.007812   \n",
       "979  1568           Quercus_x_Hispanica  0.009766  0.003906  0.025391   \n",
       "980  1569                 Quercus_Suber  0.031250  0.009766  0.078125   \n",
       "981  1570  Viburnum_x_Rhytidophylloides  0.027344  0.044922  0.023438   \n",
       "982  1571                 Quercus_Nigra  0.017578  0.074219  0.015625   \n",
       "983  1572               Quercus_Phellos  0.023438  0.076172  0.015625   \n",
       "984  1574                  Ilex_Cornuta  0.080078  0.048828  0.023438   \n",
       "985  1575          Magnolia_Salicifolia  0.060547  0.119140  0.007812   \n",
       "986  1578                   Acer_Pictum  0.001953  0.003906  0.021484   \n",
       "987  1581            Alnus_Maximowiczii  0.001953  0.003906  0.000000   \n",
       "988  1582                 Quercus_Rubra  0.000000  0.000000  0.046875   \n",
       "989  1584                Quercus_Afares  0.023438  0.019531  0.031250   \n",
       "\n",
       "      margin4   margin5   margin6   margin7   margin8    ...      texture55  \\\n",
       "0    0.003906  0.011719  0.009766  0.027344  0.000000    ...       0.007812   \n",
       "1    0.015625  0.025391  0.001953  0.019531  0.000000    ...       0.000977   \n",
       "2    0.007812  0.003906  0.005859  0.068359  0.000000    ...       0.154300   \n",
       "3    0.005859  0.021484  0.019531  0.023438  0.000000    ...       0.000000   \n",
       "4    0.009766  0.013672  0.015625  0.005859  0.000000    ...       0.096680   \n",
       "5    0.001953  0.000000  0.152340  0.007812  0.000000    ...       0.145510   \n",
       "6    0.009766  0.001953  0.042969  0.039062  0.000000    ...       0.085938   \n",
       "7    0.050781  0.003906  0.000000  0.003906  0.000000    ...       0.038086   \n",
       "8    0.015625  0.001953  0.000000  0.023438  0.000000    ...       0.000000   \n",
       "9    0.037109  0.072266  0.000000  0.000000  0.000000    ...       0.000000   \n",
       "10   0.005859  0.003906  0.013672  0.033203  0.000000    ...       0.009766   \n",
       "11   0.025391  0.076172  0.000000  0.029297  0.000000    ...       0.013672   \n",
       "12   0.003906  0.023438  0.015625  0.019531  0.000000    ...       0.003906   \n",
       "13   0.017578  0.007812  0.009766  0.011719  0.007812    ...       0.002930   \n",
       "14   0.027344  0.023438  0.039062  0.013672  0.000000    ...       0.000977   \n",
       "15   0.007812  0.003906  0.052734  0.031250  0.000000    ...       0.008789   \n",
       "16   0.029297  0.011719  0.044922  0.005859  0.000000    ...       0.000000   \n",
       "17   0.029297  0.111330  0.000000  0.000000  0.000000    ...       0.012695   \n",
       "18   0.031250  0.011719  0.000000  0.021484  0.001953    ...       0.000000   \n",
       "19   0.041016  0.007812  0.017578  0.046875  0.001953    ...       0.000000   \n",
       "20   0.025391  0.007812  0.005859  0.023438  0.000000    ...       0.000000   \n",
       "21   0.033203  0.025391  0.023438  0.009766  0.005859    ...       0.021484   \n",
       "22   0.001953  0.013672  0.001953  0.015625  0.000000    ...       0.000000   \n",
       "23   0.021484  0.001953  0.068359  0.037109  0.000000    ...       0.000000   \n",
       "24   0.029297  0.000000  0.000000  0.000000  0.000000    ...       0.103520   \n",
       "25   0.015625  0.005859  0.017578  0.000000  0.000000    ...       0.029297   \n",
       "26   0.042969  0.017578  0.009766  0.037109  0.031250    ...       0.000000   \n",
       "27   0.003906  0.000000  0.148440  0.007812  0.000000    ...       0.018555   \n",
       "28   0.044922  0.041016  0.011719  0.039062  0.013672    ...       0.000000   \n",
       "29   0.005859  0.011719  0.037109  0.035156  0.000000    ...       0.076172   \n",
       "..        ...       ...       ...       ...       ...    ...            ...   \n",
       "960  0.029297  0.007812  0.005859  0.019531  0.000000    ...       0.000000   \n",
       "961  0.015625  0.072266  0.000000  0.011719  0.003906    ...       0.000000   \n",
       "962  0.011719  0.037109  0.003906  0.019531  0.000000    ...       0.000000   \n",
       "963  0.003906  0.009766  0.009766  0.007812  0.005859    ...       0.213870   \n",
       "964  0.019531  0.037109  0.000000  0.013672  0.000000    ...       0.000000   \n",
       "965  0.037109  0.003906  0.003906  0.031250  0.000000    ...       0.000000   \n",
       "966  0.009766  0.003906  0.046875  0.050781  0.000000    ...       0.000000   \n",
       "967  0.013672  0.031250  0.000000  0.011719  0.000000    ...       0.000000   \n",
       "968  0.039062  0.082031  0.000000  0.000000  0.000000    ...       0.007812   \n",
       "969  0.033203  0.001953  0.033203  0.011719  0.000000    ...       0.000000   \n",
       "970  0.031250  0.017578  0.003906  0.025391  0.003906    ...       0.000000   \n",
       "971  0.076172  0.007812  0.000000  0.000000  0.000000    ...       0.040039   \n",
       "972  0.031250  0.000000  0.021484  0.031250  0.003906    ...       0.109380   \n",
       "973  0.003906  0.011719  0.011719  0.031250  0.000000    ...       0.066406   \n",
       "974  0.021484  0.001953  0.068359  0.015625  0.000000    ...       0.046875   \n",
       "975  0.013672  0.000000  0.119140  0.003906  0.000000    ...       0.000000   \n",
       "976  0.015625  0.048828  0.000000  0.033203  0.000000    ...       0.094727   \n",
       "977  0.015625  0.015625  0.000000  0.009766  0.000000    ...       0.000000   \n",
       "978  0.007812  0.039062  0.000000  0.001953  0.005859    ...       0.000000   \n",
       "979  0.017578  0.005859  0.003906  0.035156  0.000000    ...       0.000000   \n",
       "980  0.017578  0.003906  0.058594  0.048828  0.000000    ...       0.000977   \n",
       "981  0.017578  0.001953  0.025391  0.083984  0.000000    ...       0.000000   \n",
       "982  0.013672  0.000000  0.074219  0.005859  0.000000    ...       0.025391   \n",
       "983  0.019531  0.000000  0.039062  0.011719  0.000000    ...       0.053711   \n",
       "984  0.007812  0.000000  0.091797  0.009766  0.000000    ...       0.003906   \n",
       "985  0.003906  0.000000  0.148440  0.017578  0.000000    ...       0.242190   \n",
       "986  0.107420  0.001953  0.000000  0.000000  0.000000    ...       0.170900   \n",
       "987  0.021484  0.078125  0.003906  0.007812  0.000000    ...       0.004883   \n",
       "988  0.056641  0.009766  0.000000  0.000000  0.000000    ...       0.083008   \n",
       "989  0.015625  0.005859  0.019531  0.035156  0.000000    ...       0.000000   \n",
       "\n",
       "     texture56  texture57  texture58  texture59  texture60  texture61  \\\n",
       "0     0.000000   0.002930   0.002930   0.035156   0.000000   0.000000   \n",
       "1     0.000000   0.000000   0.000977   0.023438   0.000000   0.000000   \n",
       "2     0.000000   0.005859   0.000977   0.007812   0.000000   0.000000   \n",
       "3     0.000977   0.000000   0.000000   0.020508   0.000000   0.000000   \n",
       "4     0.000000   0.021484   0.000000   0.000000   0.000000   0.000000   \n",
       "5     0.000000   0.041992   0.000000   0.005859   0.000000   0.000000   \n",
       "6     0.000000   0.040039   0.000000   0.009766   0.000000   0.000000   \n",
       "7     0.025391   0.009766   0.002930   0.021484   0.000000   0.037109   \n",
       "8     0.000000   0.008789   0.000000   0.017578   0.000000   0.000000   \n",
       "9     0.000000   0.000000   0.070312   0.013672   0.192380   0.000000   \n",
       "10    0.000000   0.000000   0.002930   0.024414   0.000000   0.000000   \n",
       "11    0.003906   0.014648   0.003906   0.036133   0.000000   0.000000   \n",
       "12    0.000000   0.086914   0.000000   0.013672   0.000000   0.000000   \n",
       "13    0.000000   0.015625   0.000000   0.015625   0.000000   0.000000   \n",
       "14    0.001953   0.000000   0.034180   0.007812   0.038086   0.000000   \n",
       "15    0.000000   0.004883   0.004883   0.009766   0.001953   0.000000   \n",
       "16    0.000000   0.004883   0.000000   0.068359   0.000000   0.000000   \n",
       "17    0.000000   0.079102   0.000000   0.000000   0.000000   0.000000   \n",
       "18    0.000000   0.001953   0.005859   0.049805   0.000000   0.000000   \n",
       "19    0.000977   0.000000   0.020508   0.001953   0.333010   0.000000   \n",
       "20    0.000977   0.001953   0.016602   0.023438   0.004883   0.000000   \n",
       "21    0.000000   0.057617   0.000000   0.002930   0.000000   0.000000   \n",
       "22    0.019531   0.000000   0.031250   0.005859   0.054688   0.000000   \n",
       "23    0.000000   0.000000   0.044922   0.003906   0.004883   0.000000   \n",
       "24    0.009766   0.000000   0.002930   0.002930   0.077148   0.000000   \n",
       "25    0.033203   0.007812   0.006836   0.014648   0.000000   0.000000   \n",
       "26    0.000000   0.003906   0.000977   0.009766   0.001953   0.000000   \n",
       "27    0.052734   0.005859   0.000977   0.019531   0.000000   0.011719   \n",
       "28    0.000000   0.000000   0.000977   0.007812   0.000000   0.000000   \n",
       "29    0.000000   0.049805   0.000000   0.000977   0.000000   0.000000   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "960   0.000000   0.013672   0.000000   0.019531   0.000000   0.000000   \n",
       "961   0.000000   0.000000   0.041016   0.057617   0.011719   0.000000   \n",
       "962   0.002930   0.000000   0.015625   0.029297   0.000000   0.000000   \n",
       "963   0.000000   0.041016   0.000000   0.003906   0.000000   0.000000   \n",
       "964   0.000000   0.003906   0.000977   0.039062   0.000000   0.000000   \n",
       "965   0.000000   0.000000   0.011719   0.028320   0.000000   0.000000   \n",
       "966   0.000000   0.000000   0.039062   0.000977   0.008789   0.000000   \n",
       "967   0.020508   0.002930   0.009766   0.026367   0.000000   0.000000   \n",
       "968   0.000000   0.144530   0.000000   0.000000   0.000000   0.000000   \n",
       "969   0.147460   0.000977   0.018555   0.008789   0.009766   0.000000   \n",
       "970   0.000000   0.002930   0.000000   0.019531   0.000000   0.000000   \n",
       "971   0.000977   0.006836   0.002930   0.022461   0.000000   0.082031   \n",
       "972   0.000000   0.013672   0.004883   0.018555   0.000000   0.000000   \n",
       "973   0.000000   0.096680   0.000000   0.006836   0.000000   0.000000   \n",
       "974   0.000000   0.009766   0.000000   0.010742   0.000000   0.000977   \n",
       "975   0.000000   0.000000   0.005859   0.007812   0.000000   0.000000   \n",
       "976   0.000000   0.016602   0.000000   0.018555   0.000000   0.000000   \n",
       "977   0.000000   0.017578   0.011719   0.070312   0.000000   0.000000   \n",
       "978   0.056641   0.000000   0.035156   0.011719   0.024414   0.000000   \n",
       "979   0.000000   0.000000   0.008789   0.016602   0.000000   0.000000   \n",
       "980   0.000000   0.024414   0.006836   0.017578   0.000977   0.000000   \n",
       "981   0.000000   0.000000   0.036133   0.082031   0.000000   0.000000   \n",
       "982   0.000000   0.003906   0.000000   0.009766   0.000000   0.012695   \n",
       "983   0.000000   0.002930   0.000977   0.005859   0.000000   0.081055   \n",
       "984   0.003906   0.041016   0.006836   0.017578   0.000000   0.000000   \n",
       "985   0.000000   0.034180   0.000000   0.010742   0.000000   0.000000   \n",
       "986   0.000000   0.018555   0.000000   0.011719   0.000000   0.000000   \n",
       "987   0.000977   0.004883   0.027344   0.016602   0.007812   0.000000   \n",
       "988   0.030273   0.000977   0.002930   0.014648   0.000000   0.041992   \n",
       "989   0.000000   0.002930   0.000000   0.012695   0.000000   0.000000   \n",
       "\n",
       "     texture62  texture63  texture64  \n",
       "0     0.004883   0.000000   0.025391  \n",
       "1     0.000977   0.039062   0.022461  \n",
       "2     0.000000   0.020508   0.002930  \n",
       "3     0.017578   0.000000   0.047852  \n",
       "4     0.000000   0.000000   0.031250  \n",
       "5     0.000000   0.001953   0.013672  \n",
       "6     0.000000   0.039062   0.003906  \n",
       "7     0.006836   0.002930   0.036133  \n",
       "8     0.000977   0.033203   0.074219  \n",
       "9     0.074219   0.000000   0.000000  \n",
       "10    0.006836   0.000000   0.004883  \n",
       "11    0.012695   0.005859   0.000000  \n",
       "12    0.000000   0.011719   0.002930  \n",
       "13    0.000000   0.018555   0.022461  \n",
       "14    0.089844   0.023438   0.001953  \n",
       "15    0.003906   0.000000   0.036133  \n",
       "16    0.000000   0.036133   0.000977  \n",
       "17    0.000000   0.000000   0.019531  \n",
       "18    0.006836   0.000000   0.001953  \n",
       "19    0.169920   0.000000   0.000000  \n",
       "20    0.003906   0.013672   0.004883  \n",
       "21    0.000000   0.000000   0.035156  \n",
       "22    0.103520   0.000977   0.000000  \n",
       "23    0.069336   0.000000   0.000000  \n",
       "24    0.052734   0.000000   0.000000  \n",
       "25    0.020508   0.000000   0.035156  \n",
       "26    0.001953   0.023438   0.007812  \n",
       "27    0.041992   0.004883   0.020508  \n",
       "28    0.007812   0.024414   0.015625  \n",
       "29    0.000000   0.000000   0.001953  \n",
       "..         ...        ...        ...  \n",
       "960   0.003906   0.029297   0.055664  \n",
       "961   0.010742   0.000000   0.002930  \n",
       "962   0.030273   0.000000   0.012695  \n",
       "963   0.000000   0.000000   0.014648  \n",
       "964   0.000000   0.022461   0.033203  \n",
       "965   0.011719   0.008789   0.000000  \n",
       "966   0.032227   0.000000   0.000977  \n",
       "967   0.009766   0.011719   0.041016  \n",
       "968   0.000000   0.000000   0.010742  \n",
       "969   0.078125   0.022461   0.000000  \n",
       "970   0.001953   0.010742   0.067383  \n",
       "971   0.001953   0.005859   0.029297  \n",
       "972   0.001953   0.000977   0.004883  \n",
       "973   0.000000   0.030273   0.011719  \n",
       "974   0.005859   0.011719   0.002930  \n",
       "975   0.030273   0.012695   0.000977  \n",
       "976   0.000977   0.028320   0.000000  \n",
       "977   0.000977   0.000000   0.012695  \n",
       "978   0.066406   0.013672   0.000000  \n",
       "979   0.024414   0.037109   0.003906  \n",
       "980   0.016602   0.000000   0.041016  \n",
       "981   0.026367   0.008789   0.000000  \n",
       "982   0.000000   0.009766   0.000000  \n",
       "983   0.000000   0.003906   0.019531  \n",
       "984   0.003906   0.000977   0.032227  \n",
       "985   0.000000   0.000000   0.018555  \n",
       "986   0.000977   0.000000   0.021484  \n",
       "987   0.027344   0.000000   0.001953  \n",
       "988   0.000000   0.001953   0.002930  \n",
       "989   0.023438   0.025391   0.022461  \n",
       "\n",
       "[990 rows x 194 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##let's take a look\n",
    "raw_data\n",
    "###data description:99 species with 16 samples\n",
    "###features are :a shape contiguous descriptor,interior texture histogram, and a ﬁne-scale margin histogram. 3*64=192\n",
    "###990rows 194 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##figure out number of classes\n",
    "col_n=pd.pivot_table(raw_data.loc[:,:'species'],columns=\"species\",aggfunc=np.count_nonzero)\n",
    "##thus they are evenly distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3*64  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data description checked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_label=raw_data.pop('species')\n",
    "D_ID=raw_data.pop('id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get data one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "###get one hot encoding\n",
    "label_cat=pd.get_dummies(D_label)\n",
    "##label_cat=keras.utils.to_categorical(label.values,num_classes=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DN,CN=label_cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DN2,AN=raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#print (sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was trying to separate the training data into two set but realize that there is one parameter in model.fit could do it for me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def dt_ran_sample(DN,p):\n",
    "#    X=np.random.random_integers(low=0,high=1,size=p*DN)\n",
    "#    Y=np.random.random_integers(low=0,high=1,size=(1-p)*DN)\n",
    "#    return [X,Y]\n",
    "#from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data_train,Data_valid=train_test_split(range(DN),train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(Data_train),len(Data_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Let's build a quick model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##let'r try out a 2 FC layer\n",
    "model = Sequential()\n",
    "model.add(Dense(128,input_dim=192,activation='relu'))\n",
    "model.add(Dense(CN, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer='rmsprop', metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 792 samples, validate on 198 samples\n",
      "Epoch 1/150\n",
      "792/792 [==============================] - 4s 5ms/step - loss: 4.5819 - acc: 0.0265 - val_loss: 4.5715 - val_acc: 0.0404\n",
      "Epoch 2/150\n",
      "792/792 [==============================] - 0s 109us/step - loss: 4.5444 - acc: 0.1540 - val_loss: 4.5456 - val_acc: 0.0707\n",
      "Epoch 3/150\n",
      "792/792 [==============================] - 0s 113us/step - loss: 4.5029 - acc: 0.2071 - val_loss: 4.5150 - val_acc: 0.0859\n",
      "Epoch 4/150\n",
      "792/792 [==============================] - 0s 113us/step - loss: 4.4543 - acc: 0.2210 - val_loss: 4.4798 - val_acc: 0.0960\n",
      "Epoch 5/150\n",
      "792/792 [==============================] - 0s 115us/step - loss: 4.3979 - acc: 0.2500 - val_loss: 4.4402 - val_acc: 0.0960\n",
      "Epoch 6/150\n",
      "792/792 [==============================] - 0s 115us/step - loss: 4.3340 - acc: 0.2513 - val_loss: 4.3950 - val_acc: 0.1162\n",
      "Epoch 7/150\n",
      "792/792 [==============================] - 0s 118us/step - loss: 4.2622 - acc: 0.2778 - val_loss: 4.3436 - val_acc: 0.1313\n",
      "Epoch 8/150\n",
      "792/792 [==============================] - 0s 125us/step - loss: 4.1845 - acc: 0.2765 - val_loss: 4.2886 - val_acc: 0.1313\n",
      "Epoch 9/150\n",
      "792/792 [==============================] - 0s 116us/step - loss: 4.1013 - acc: 0.2955 - val_loss: 4.2273 - val_acc: 0.1313\n",
      "Epoch 10/150\n",
      "792/792 [==============================] - 0s 110us/step - loss: 4.0134 - acc: 0.3043 - val_loss: 4.1602 - val_acc: 0.1364\n",
      "Epoch 11/150\n",
      "792/792 [==============================] - 0s 112us/step - loss: 3.9205 - acc: 0.3207 - val_loss: 4.0877 - val_acc: 0.1313\n",
      "Epoch 12/150\n",
      "792/792 [==============================] - 0s 105us/step - loss: 3.8250 - acc: 0.3447 - val_loss: 4.0113 - val_acc: 0.1566\n",
      "Epoch 13/150\n",
      "792/792 [==============================] - 0s 96us/step - loss: 3.7272 - acc: 0.3548 - val_loss: 3.9315 - val_acc: 0.1667\n",
      "Epoch 14/150\n",
      "792/792 [==============================] - 0s 109us/step - loss: 3.6272 - acc: 0.3687 - val_loss: 3.8501 - val_acc: 0.1616\n",
      "Epoch 15/150\n",
      "792/792 [==============================] - 0s 98us/step - loss: 3.5260 - acc: 0.3965 - val_loss: 3.7635 - val_acc: 0.1768\n",
      "Epoch 16/150\n",
      "792/792 [==============================] - 0s 100us/step - loss: 3.4244 - acc: 0.4053 - val_loss: 3.6746 - val_acc: 0.1919\n",
      "Epoch 17/150\n",
      "792/792 [==============================] - 0s 99us/step - loss: 3.3233 - acc: 0.4356 - val_loss: 3.5828 - val_acc: 0.2071\n",
      "Epoch 18/150\n",
      "792/792 [==============================] - 0s 101us/step - loss: 3.2245 - acc: 0.4621 - val_loss: 3.4920 - val_acc: 0.2222\n",
      "Epoch 19/150\n",
      "792/792 [==============================] - 0s 103us/step - loss: 3.1252 - acc: 0.4760 - val_loss: 3.4039 - val_acc: 0.2273\n",
      "Epoch 20/150\n",
      "792/792 [==============================] - 0s 101us/step - loss: 3.0302 - acc: 0.4722 - val_loss: 3.3091 - val_acc: 0.2374\n",
      "Epoch 21/150\n",
      "792/792 [==============================] - 0s 100us/step - loss: 2.9343 - acc: 0.5088 - val_loss: 3.2181 - val_acc: 0.2576\n",
      "Epoch 22/150\n",
      "792/792 [==============================] - 0s 97us/step - loss: 2.8417 - acc: 0.5379 - val_loss: 3.1271 - val_acc: 0.2879\n",
      "Epoch 23/150\n",
      "792/792 [==============================] - 0s 95us/step - loss: 2.7504 - acc: 0.5593 - val_loss: 3.0348 - val_acc: 0.3384\n",
      "Epoch 24/150\n",
      "792/792 [==============================] - 0s 95us/step - loss: 2.6630 - acc: 0.5896 - val_loss: 2.9476 - val_acc: 0.3838\n",
      "Epoch 25/150\n",
      "792/792 [==============================] - 0s 97us/step - loss: 2.5765 - acc: 0.6225 - val_loss: 2.8623 - val_acc: 0.4040\n",
      "Epoch 26/150\n",
      "792/792 [==============================] - 0s 96us/step - loss: 2.4932 - acc: 0.6414 - val_loss: 2.7790 - val_acc: 0.4495\n",
      "Epoch 27/150\n",
      "792/792 [==============================] - 0s 95us/step - loss: 2.4134 - acc: 0.6540 - val_loss: 2.6966 - val_acc: 0.4899\n",
      "Epoch 28/150\n",
      "792/792 [==============================] - 0s 94us/step - loss: 2.3351 - acc: 0.6856 - val_loss: 2.6207 - val_acc: 0.5202\n",
      "Epoch 29/150\n",
      "792/792 [==============================] - 0s 95us/step - loss: 2.2613 - acc: 0.6957 - val_loss: 2.5443 - val_acc: 0.5404\n",
      "Epoch 30/150\n",
      "792/792 [==============================] - 0s 102us/step - loss: 2.1889 - acc: 0.7374 - val_loss: 2.4718 - val_acc: 0.5556\n",
      "Epoch 31/150\n",
      "792/792 [==============================] - 0s 103us/step - loss: 2.1202 - acc: 0.7222 - val_loss: 2.3989 - val_acc: 0.5808\n",
      "Epoch 32/150\n",
      "792/792 [==============================] - 0s 106us/step - loss: 2.0503 - acc: 0.7538 - val_loss: 2.3265 - val_acc: 0.6162\n",
      "Epoch 33/150\n",
      "792/792 [==============================] - 0s 108us/step - loss: 1.9858 - acc: 0.7551 - val_loss: 2.2585 - val_acc: 0.6364\n",
      "Epoch 34/150\n",
      "792/792 [==============================] - 0s 101us/step - loss: 1.9196 - acc: 0.7854 - val_loss: 2.1896 - val_acc: 0.6566\n",
      "Epoch 35/150\n",
      "792/792 [==============================] - 0s 96us/step - loss: 1.8589 - acc: 0.7992 - val_loss: 2.1273 - val_acc: 0.6717\n",
      "Epoch 36/150\n",
      "792/792 [==============================] - 0s 97us/step - loss: 1.7999 - acc: 0.8169 - val_loss: 2.0688 - val_acc: 0.6919\n",
      "Epoch 37/150\n",
      "792/792 [==============================] - 0s 96us/step - loss: 1.7416 - acc: 0.8270 - val_loss: 2.0090 - val_acc: 0.6970\n",
      "Epoch 38/150\n",
      "792/792 [==============================] - 0s 98us/step - loss: 1.6866 - acc: 0.8232 - val_loss: 1.9557 - val_acc: 0.7071\n",
      "Epoch 39/150\n",
      "792/792 [==============================] - 0s 96us/step - loss: 1.6331 - acc: 0.8295 - val_loss: 1.8943 - val_acc: 0.7323\n",
      "Epoch 40/150\n",
      "792/792 [==============================] - 0s 96us/step - loss: 1.5826 - acc: 0.8371 - val_loss: 1.8426 - val_acc: 0.7323\n",
      "Epoch 41/150\n",
      "792/792 [==============================] - 0s 96us/step - loss: 1.5326 - acc: 0.8472 - val_loss: 1.7950 - val_acc: 0.7374\n",
      "Epoch 42/150\n",
      "792/792 [==============================] - 0s 96us/step - loss: 1.4859 - acc: 0.8447 - val_loss: 1.7485 - val_acc: 0.7374\n",
      "Epoch 43/150\n",
      "792/792 [==============================] - 0s 97us/step - loss: 1.4378 - acc: 0.8611 - val_loss: 1.6977 - val_acc: 0.7525\n",
      "Epoch 44/150\n",
      "792/792 [==============================] - 0s 99us/step - loss: 1.3929 - acc: 0.8649 - val_loss: 1.6428 - val_acc: 0.7525\n",
      "Epoch 45/150\n",
      "792/792 [==============================] - 0s 120us/step - loss: 1.3480 - acc: 0.8687 - val_loss: 1.5988 - val_acc: 0.7576\n",
      "Epoch 46/150\n",
      "792/792 [==============================] - 0s 111us/step - loss: 1.3075 - acc: 0.8725 - val_loss: 1.5555 - val_acc: 0.7778\n",
      "Epoch 47/150\n",
      "792/792 [==============================] - 0s 95us/step - loss: 1.2658 - acc: 0.8687 - val_loss: 1.5130 - val_acc: 0.7778\n",
      "Epoch 48/150\n",
      "792/792 [==============================] - 0s 93us/step - loss: 1.2260 - acc: 0.8838 - val_loss: 1.4705 - val_acc: 0.7828\n",
      "Epoch 49/150\n",
      "792/792 [==============================] - 0s 92us/step - loss: 1.1880 - acc: 0.8763 - val_loss: 1.4338 - val_acc: 0.8030\n",
      "Epoch 50/150\n",
      "792/792 [==============================] - 0s 103us/step - loss: 1.1536 - acc: 0.8737 - val_loss: 1.3953 - val_acc: 0.8030\n",
      "Epoch 51/150\n",
      "792/792 [==============================] - 0s 96us/step - loss: 1.1165 - acc: 0.8737 - val_loss: 1.3561 - val_acc: 0.8030\n",
      "Epoch 52/150\n",
      "792/792 [==============================] - 0s 93us/step - loss: 1.0827 - acc: 0.8889 - val_loss: 1.3193 - val_acc: 0.8081\n",
      "Epoch 53/150\n",
      "792/792 [==============================] - 0s 95us/step - loss: 1.0510 - acc: 0.8826 - val_loss: 1.2810 - val_acc: 0.8182\n",
      "Epoch 54/150\n",
      "792/792 [==============================] - 0s 92us/step - loss: 1.0183 - acc: 0.8952 - val_loss: 1.2506 - val_acc: 0.8182\n",
      "Epoch 55/150\n",
      "792/792 [==============================] - 0s 93us/step - loss: 0.9857 - acc: 0.8914 - val_loss: 1.2153 - val_acc: 0.8283\n",
      "Epoch 56/150\n",
      "792/792 [==============================] - 0s 92us/step - loss: 0.9574 - acc: 0.9091 - val_loss: 1.1840 - val_acc: 0.8283\n",
      "Epoch 57/150\n",
      "792/792 [==============================] - 0s 96us/step - loss: 0.9274 - acc: 0.9028 - val_loss: 1.1558 - val_acc: 0.8232\n",
      "Epoch 58/150\n",
      "792/792 [==============================] - 0s 106us/step - loss: 0.8992 - acc: 0.9015 - val_loss: 1.1310 - val_acc: 0.8333\n",
      "Epoch 59/150\n",
      "792/792 [==============================] - 0s 102us/step - loss: 0.8721 - acc: 0.9154 - val_loss: 1.1016 - val_acc: 0.8333\n",
      "Epoch 60/150\n",
      "792/792 [==============================] - 0s 103us/step - loss: 0.8479 - acc: 0.9015 - val_loss: 1.0782 - val_acc: 0.8333\n",
      "Epoch 61/150\n",
      "792/792 [==============================] - 0s 93us/step - loss: 0.8226 - acc: 0.9091 - val_loss: 1.0455 - val_acc: 0.8333\n",
      "Epoch 62/150\n",
      "792/792 [==============================] - 0s 93us/step - loss: 0.7961 - acc: 0.9205 - val_loss: 1.0192 - val_acc: 0.8384\n",
      "Epoch 63/150\n",
      "792/792 [==============================] - 0s 98us/step - loss: 0.7742 - acc: 0.9141 - val_loss: 0.9921 - val_acc: 0.8333\n",
      "Epoch 64/150\n",
      "792/792 [==============================] - 0s 93us/step - loss: 0.7503 - acc: 0.9205 - val_loss: 0.9719 - val_acc: 0.8283\n",
      "Epoch 65/150\n",
      "792/792 [==============================] - 0s 92us/step - loss: 0.7299 - acc: 0.9293 - val_loss: 0.9520 - val_acc: 0.8384\n",
      "Epoch 66/150\n",
      "792/792 [==============================] - 0s 92us/step - loss: 0.7084 - acc: 0.9268 - val_loss: 0.9254 - val_acc: 0.8384\n",
      "Epoch 67/150\n",
      "792/792 [==============================] - 0s 93us/step - loss: 0.6877 - acc: 0.9306 - val_loss: 0.9073 - val_acc: 0.8434\n",
      "Epoch 68/150\n",
      "792/792 [==============================] - 0s 92us/step - loss: 0.6675 - acc: 0.9293 - val_loss: 0.8828 - val_acc: 0.8485\n",
      "Epoch 69/150\n",
      "792/792 [==============================] - 0s 96us/step - loss: 0.6501 - acc: 0.9356 - val_loss: 0.8655 - val_acc: 0.8434\n",
      "Epoch 70/150\n",
      "792/792 [==============================] - 0s 94us/step - loss: 0.6294 - acc: 0.9331 - val_loss: 0.8397 - val_acc: 0.8737\n",
      "Epoch 71/150\n",
      "792/792 [==============================] - 0s 94us/step - loss: 0.6141 - acc: 0.9419 - val_loss: 0.8219 - val_acc: 0.8788\n",
      "Epoch 72/150\n",
      "792/792 [==============================] - 0s 96us/step - loss: 0.5942 - acc: 0.9495 - val_loss: 0.8088 - val_acc: 0.8485\n",
      "Epoch 73/150\n",
      "792/792 [==============================] - 0s 91us/step - loss: 0.5789 - acc: 0.9407 - val_loss: 0.7969 - val_acc: 0.8687\n",
      "Epoch 74/150\n",
      "792/792 [==============================] - 0s 96us/step - loss: 0.5616 - acc: 0.9558 - val_loss: 0.7808 - val_acc: 0.8687\n",
      "Epoch 75/150\n",
      "792/792 [==============================] - 0s 92us/step - loss: 0.5463 - acc: 0.9495 - val_loss: 0.7644 - val_acc: 0.8838\n",
      "Epoch 76/150\n",
      "792/792 [==============================] - 0s 91us/step - loss: 0.5319 - acc: 0.9571 - val_loss: 0.7403 - val_acc: 0.8939\n",
      "Epoch 77/150\n",
      "792/792 [==============================] - 0s 91us/step - loss: 0.5138 - acc: 0.9520 - val_loss: 0.7271 - val_acc: 0.8838\n",
      "Epoch 78/150\n",
      "792/792 [==============================] - 0s 93us/step - loss: 0.5018 - acc: 0.9533 - val_loss: 0.7162 - val_acc: 0.8788\n",
      "Epoch 79/150\n",
      "792/792 [==============================] - 0s 94us/step - loss: 0.4872 - acc: 0.9520 - val_loss: 0.7032 - val_acc: 0.8939\n",
      "Epoch 80/150\n",
      "792/792 [==============================] - 0s 102us/step - loss: 0.4729 - acc: 0.9571 - val_loss: 0.6868 - val_acc: 0.8990\n",
      "Epoch 81/150\n",
      "792/792 [==============================] - 0s 91us/step - loss: 0.4599 - acc: 0.9634 - val_loss: 0.6664 - val_acc: 0.8990\n",
      "Epoch 82/150\n",
      "792/792 [==============================] - 0s 93us/step - loss: 0.4473 - acc: 0.9571 - val_loss: 0.6511 - val_acc: 0.8990\n",
      "Epoch 83/150\n",
      "792/792 [==============================] - 0s 93us/step - loss: 0.4363 - acc: 0.9659 - val_loss: 0.6410 - val_acc: 0.8939\n",
      "Epoch 84/150\n",
      "792/792 [==============================] - 0s 100us/step - loss: 0.4245 - acc: 0.9609 - val_loss: 0.6301 - val_acc: 0.8990\n",
      "Epoch 85/150\n",
      "792/792 [==============================] - 0s 103us/step - loss: 0.4118 - acc: 0.9659 - val_loss: 0.6203 - val_acc: 0.9040\n",
      "Epoch 86/150\n",
      "792/792 [==============================] - 0s 95us/step - loss: 0.4030 - acc: 0.9596 - val_loss: 0.6103 - val_acc: 0.8990\n",
      "Epoch 87/150\n",
      "792/792 [==============================] - 0s 91us/step - loss: 0.3910 - acc: 0.9672 - val_loss: 0.5941 - val_acc: 0.9040\n",
      "Epoch 88/150\n",
      "792/792 [==============================] - 0s 93us/step - loss: 0.3814 - acc: 0.9710 - val_loss: 0.5869 - val_acc: 0.9040\n",
      "Epoch 89/150\n",
      "792/792 [==============================] - 0s 94us/step - loss: 0.3694 - acc: 0.9659 - val_loss: 0.5772 - val_acc: 0.8990\n",
      "Epoch 90/150\n",
      "792/792 [==============================] - 0s 93us/step - loss: 0.3606 - acc: 0.9722 - val_loss: 0.5645 - val_acc: 0.9040\n",
      "Epoch 91/150\n",
      "792/792 [==============================] - 0s 93us/step - loss: 0.3510 - acc: 0.9684 - val_loss: 0.5589 - val_acc: 0.8990\n",
      "Epoch 92/150\n",
      "792/792 [==============================] - 0s 93us/step - loss: 0.3416 - acc: 0.9735 - val_loss: 0.5414 - val_acc: 0.9040\n",
      "Epoch 93/150\n",
      "792/792 [==============================] - 0s 96us/step - loss: 0.3321 - acc: 0.9747 - val_loss: 0.5370 - val_acc: 0.8990\n",
      "Epoch 94/150\n",
      "792/792 [==============================] - 0s 93us/step - loss: 0.3224 - acc: 0.9735 - val_loss: 0.5280 - val_acc: 0.9040\n",
      "Epoch 95/150\n",
      "792/792 [==============================] - 0s 95us/step - loss: 0.3156 - acc: 0.9785 - val_loss: 0.5208 - val_acc: 0.9040\n",
      "Epoch 96/150\n",
      "792/792 [==============================] - 0s 97us/step - loss: 0.3065 - acc: 0.9747 - val_loss: 0.5090 - val_acc: 0.9040\n",
      "Epoch 97/150\n",
      "792/792 [==============================] - 0s 96us/step - loss: 0.2997 - acc: 0.9773 - val_loss: 0.5046 - val_acc: 0.9040\n",
      "Epoch 98/150\n",
      "792/792 [==============================] - 0s 95us/step - loss: 0.2905 - acc: 0.9735 - val_loss: 0.4944 - val_acc: 0.9091\n",
      "Epoch 99/150\n",
      "792/792 [==============================] - 0s 99us/step - loss: 0.2832 - acc: 0.9773 - val_loss: 0.4836 - val_acc: 0.9091\n",
      "Epoch 100/150\n",
      "792/792 [==============================] - 0s 96us/step - loss: 0.2759 - acc: 0.9773 - val_loss: 0.4778 - val_acc: 0.9091\n",
      "Epoch 101/150\n",
      "792/792 [==============================] - 0s 93us/step - loss: 0.2684 - acc: 0.9798 - val_loss: 0.4660 - val_acc: 0.9091\n",
      "Epoch 102/150\n",
      "792/792 [==============================] - 0s 92us/step - loss: 0.2628 - acc: 0.9760 - val_loss: 0.4627 - val_acc: 0.9091\n",
      "Epoch 103/150\n",
      "792/792 [==============================] - 0s 93us/step - loss: 0.2554 - acc: 0.9785 - val_loss: 0.4564 - val_acc: 0.9141\n",
      "Epoch 104/150\n",
      "792/792 [==============================] - 0s 93us/step - loss: 0.2490 - acc: 0.9848 - val_loss: 0.4483 - val_acc: 0.9091\n",
      "Epoch 105/150\n",
      "792/792 [==============================] - 0s 93us/step - loss: 0.2438 - acc: 0.9848 - val_loss: 0.4443 - val_acc: 0.9091\n",
      "Epoch 106/150\n",
      "792/792 [==============================] - 0s 96us/step - loss: 0.2370 - acc: 0.9798 - val_loss: 0.4349 - val_acc: 0.9091\n",
      "Epoch 107/150\n",
      "792/792 [==============================] - 0s 92us/step - loss: 0.2300 - acc: 0.9848 - val_loss: 0.4287 - val_acc: 0.9091\n",
      "Epoch 108/150\n",
      "792/792 [==============================] - 0s 93us/step - loss: 0.2248 - acc: 0.9823 - val_loss: 0.4227 - val_acc: 0.9091\n",
      "Epoch 109/150\n",
      "792/792 [==============================] - 0s 91us/step - loss: 0.2189 - acc: 0.9861 - val_loss: 0.4152 - val_acc: 0.9091\n",
      "Epoch 110/150\n",
      "792/792 [==============================] - 0s 92us/step - loss: 0.2141 - acc: 0.9798 - val_loss: 0.4115 - val_acc: 0.9141\n",
      "Epoch 111/150\n",
      "792/792 [==============================] - 0s 102us/step - loss: 0.2088 - acc: 0.9886 - val_loss: 0.4088 - val_acc: 0.9141\n",
      "Epoch 112/150\n",
      "792/792 [==============================] - 0s 98us/step - loss: 0.2028 - acc: 0.9848 - val_loss: 0.3984 - val_acc: 0.9141\n",
      "Epoch 113/150\n",
      "792/792 [==============================] - 0s 100us/step - loss: 0.1981 - acc: 0.9848 - val_loss: 0.3925 - val_acc: 0.9091\n",
      "Epoch 114/150\n",
      "792/792 [==============================] - 0s 95us/step - loss: 0.1936 - acc: 0.9874 - val_loss: 0.3877 - val_acc: 0.9141\n",
      "Epoch 115/150\n",
      "792/792 [==============================] - 0s 93us/step - loss: 0.1879 - acc: 0.9836 - val_loss: 0.3847 - val_acc: 0.9141\n",
      "Epoch 116/150\n",
      "792/792 [==============================] - 0s 92us/step - loss: 0.1848 - acc: 0.9874 - val_loss: 0.3742 - val_acc: 0.9141\n",
      "Epoch 117/150\n",
      "792/792 [==============================] - 0s 93us/step - loss: 0.1788 - acc: 0.9886 - val_loss: 0.3712 - val_acc: 0.9192\n",
      "Epoch 118/150\n",
      "792/792 [==============================] - 0s 92us/step - loss: 0.1743 - acc: 0.9836 - val_loss: 0.3667 - val_acc: 0.9091\n",
      "Epoch 119/150\n",
      "792/792 [==============================] - 0s 105us/step - loss: 0.1710 - acc: 0.9912 - val_loss: 0.3617 - val_acc: 0.9192\n",
      "Epoch 120/150\n",
      "792/792 [==============================] - 0s 94us/step - loss: 0.1658 - acc: 0.9912 - val_loss: 0.3601 - val_acc: 0.9293\n",
      "Epoch 121/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "792/792 [==============================] - 0s 95us/step - loss: 0.1616 - acc: 0.9924 - val_loss: 0.3633 - val_acc: 0.9192\n",
      "Epoch 122/150\n",
      "792/792 [==============================] - 0s 93us/step - loss: 0.1581 - acc: 0.9924 - val_loss: 0.3520 - val_acc: 0.9192\n",
      "Epoch 123/150\n",
      "792/792 [==============================] - 0s 91us/step - loss: 0.1547 - acc: 0.9937 - val_loss: 0.3426 - val_acc: 0.9192\n",
      "Epoch 124/150\n",
      "792/792 [==============================] - 0s 105us/step - loss: 0.1493 - acc: 0.9949 - val_loss: 0.3396 - val_acc: 0.9242\n",
      "Epoch 125/150\n",
      "792/792 [==============================] - 0s 93us/step - loss: 0.1460 - acc: 0.9924 - val_loss: 0.3341 - val_acc: 0.9242\n",
      "Epoch 126/150\n",
      "792/792 [==============================] - 0s 95us/step - loss: 0.1431 - acc: 0.9912 - val_loss: 0.3337 - val_acc: 0.9293\n",
      "Epoch 127/150\n",
      "792/792 [==============================] - 0s 95us/step - loss: 0.1393 - acc: 0.9924 - val_loss: 0.3309 - val_acc: 0.9242\n",
      "Epoch 128/150\n",
      "792/792 [==============================] - 0s 93us/step - loss: 0.1365 - acc: 0.9937 - val_loss: 0.3275 - val_acc: 0.9242\n",
      "Epoch 129/150\n",
      "792/792 [==============================] - 0s 93us/step - loss: 0.1327 - acc: 0.9987 - val_loss: 0.3203 - val_acc: 0.9192\n",
      "Epoch 130/150\n",
      "792/792 [==============================] - 0s 101us/step - loss: 0.1295 - acc: 0.9975 - val_loss: 0.3172 - val_acc: 0.9192\n",
      "Epoch 131/150\n",
      "792/792 [==============================] - 0s 92us/step - loss: 0.1261 - acc: 0.9975 - val_loss: 0.3189 - val_acc: 0.9293\n",
      "Epoch 132/150\n",
      "792/792 [==============================] - 0s 93us/step - loss: 0.1226 - acc: 0.9975 - val_loss: 0.3155 - val_acc: 0.9343\n",
      "Epoch 133/150\n",
      "792/792 [==============================] - 0s 91us/step - loss: 0.1205 - acc: 0.9975 - val_loss: 0.3133 - val_acc: 0.9293\n",
      "Epoch 134/150\n",
      "792/792 [==============================] - 0s 91us/step - loss: 0.1169 - acc: 0.9987 - val_loss: 0.3128 - val_acc: 0.9242\n",
      "Epoch 135/150\n",
      "792/792 [==============================] - 0s 95us/step - loss: 0.1139 - acc: 0.9975 - val_loss: 0.3073 - val_acc: 0.9293\n",
      "Epoch 136/150\n",
      "792/792 [==============================] - 0s 95us/step - loss: 0.1118 - acc: 0.9987 - val_loss: 0.2992 - val_acc: 0.9343\n",
      "Epoch 137/150\n",
      "792/792 [==============================] - 0s 97us/step - loss: 0.1087 - acc: 0.9987 - val_loss: 0.3008 - val_acc: 0.9192\n",
      "Epoch 138/150\n",
      "792/792 [==============================] - 0s 94us/step - loss: 0.1069 - acc: 0.9987 - val_loss: 0.2958 - val_acc: 0.9242\n",
      "Epoch 139/150\n",
      "792/792 [==============================] - 0s 105us/step - loss: 0.1037 - acc: 0.9987 - val_loss: 0.2928 - val_acc: 0.9293\n",
      "Epoch 140/150\n",
      "792/792 [==============================] - 0s 92us/step - loss: 0.1015 - acc: 1.0000 - val_loss: 0.2884 - val_acc: 0.9242\n",
      "Epoch 141/150\n",
      "792/792 [==============================] - 0s 95us/step - loss: 0.0988 - acc: 0.9987 - val_loss: 0.2872 - val_acc: 0.9242\n",
      "Epoch 142/150\n",
      "792/792 [==============================] - 0s 93us/step - loss: 0.0960 - acc: 0.9987 - val_loss: 0.2817 - val_acc: 0.9343\n",
      "Epoch 143/150\n",
      "792/792 [==============================] - 0s 91us/step - loss: 0.0944 - acc: 0.9987 - val_loss: 0.2800 - val_acc: 0.9343\n",
      "Epoch 144/150\n",
      "792/792 [==============================] - 0s 93us/step - loss: 0.0913 - acc: 1.0000 - val_loss: 0.2728 - val_acc: 0.9343\n",
      "Epoch 145/150\n",
      "792/792 [==============================] - 0s 91us/step - loss: 0.0898 - acc: 1.0000 - val_loss: 0.2781 - val_acc: 0.9343\n",
      "Epoch 146/150\n",
      "792/792 [==============================] - 0s 94us/step - loss: 0.0876 - acc: 1.0000 - val_loss: 0.2757 - val_acc: 0.9293\n",
      "Epoch 147/150\n",
      "792/792 [==============================] - 0s 100us/step - loss: 0.0859 - acc: 1.0000 - val_loss: 0.2732 - val_acc: 0.9343\n",
      "Epoch 148/150\n",
      "792/792 [==============================] - 0s 95us/step - loss: 0.0822 - acc: 1.0000 - val_loss: 0.2702 - val_acc: 0.9343\n",
      "Epoch 149/150\n",
      "792/792 [==============================] - 0s 91us/step - loss: 0.0815 - acc: 1.0000 - val_loss: 0.2688 - val_acc: 0.9343\n",
      "Epoch 150/150\n",
      "792/792 [==============================] - 0s 103us/step - loss: 0.0799 - acc: 1.0000 - val_loss: 0.2644 - val_acc: 0.9343\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x1e5db414748>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(raw_data,label_cat,batch_size=50,epochs=150,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to fight with over fitting by changing the framework "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "###seems not that bad, but very likely overfit a little bit\n",
    "model3 = Sequential()\n",
    "model3.add(Dense(128,input_dim=192,activation='relu'))\n",
    "model3.add(Dropout(0.33))\n",
    "model3.add(Dense(128,activation='relu'))\n",
    "model3.add(Dense(CN, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.compile(loss='categorical_crossentropy',optimizer='rmsprop', metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 792 samples, validate on 198 samples\n",
      "Epoch 1/100\n",
      "792/792 [==============================] - 0s 514us/step - loss: 4.5884 - acc: 0.0278 - val_loss: 4.5789 - val_acc: 0.0303\n",
      "Epoch 2/100\n",
      "792/792 [==============================] - 0s 116us/step - loss: 4.5574 - acc: 0.0568 - val_loss: 4.5529 - val_acc: 0.0354\n",
      "Epoch 3/100\n",
      "792/792 [==============================] - 0s 135us/step - loss: 4.5108 - acc: 0.0694 - val_loss: 4.5141 - val_acc: 0.0404\n",
      "Epoch 4/100\n",
      "792/792 [==============================] - 0s 127us/step - loss: 4.4383 - acc: 0.0909 - val_loss: 4.4587 - val_acc: 0.0455\n",
      "Epoch 5/100\n",
      "792/792 [==============================] - 0s 125us/step - loss: 4.3343 - acc: 0.1111 - val_loss: 4.3793 - val_acc: 0.0455\n",
      "Epoch 6/100\n",
      "792/792 [==============================] - 0s 123us/step - loss: 4.2033 - acc: 0.1338 - val_loss: 4.2745 - val_acc: 0.0556\n",
      "Epoch 7/100\n",
      "792/792 [==============================] - 0s 122us/step - loss: 4.0477 - acc: 0.1818 - val_loss: 4.1516 - val_acc: 0.0556\n",
      "Epoch 8/100\n",
      "792/792 [==============================] - 0s 121us/step - loss: 3.8661 - acc: 0.1578 - val_loss: 4.0060 - val_acc: 0.0657\n",
      "Epoch 9/100\n",
      "792/792 [==============================] - 0s 119us/step - loss: 3.6811 - acc: 0.1818 - val_loss: 3.8362 - val_acc: 0.0909\n",
      "Epoch 10/100\n",
      "792/792 [==============================] - 0s 118us/step - loss: 3.4890 - acc: 0.2071 - val_loss: 3.6623 - val_acc: 0.1111\n",
      "Epoch 11/100\n",
      "792/792 [==============================] - 0s 121us/step - loss: 3.3134 - acc: 0.2601 - val_loss: 3.4852 - val_acc: 0.1263\n",
      "Epoch 12/100\n",
      "792/792 [==============================] - 0s 122us/step - loss: 3.1470 - acc: 0.2828 - val_loss: 3.2956 - val_acc: 0.1667\n",
      "Epoch 13/100\n",
      "792/792 [==============================] - 0s 130us/step - loss: 2.9673 - acc: 0.3283 - val_loss: 3.1086 - val_acc: 0.2121\n",
      "Epoch 14/100\n",
      "792/792 [==============================] - 0s 121us/step - loss: 2.8112 - acc: 0.3611 - val_loss: 2.9514 - val_acc: 0.2424\n",
      "Epoch 15/100\n",
      "792/792 [==============================] - 0s 118us/step - loss: 2.6816 - acc: 0.3624 - val_loss: 2.8048 - val_acc: 0.3182\n",
      "Epoch 16/100\n",
      "792/792 [==============================] - 0s 122us/step - loss: 2.5282 - acc: 0.4381 - val_loss: 2.6411 - val_acc: 0.3687\n",
      "Epoch 17/100\n",
      "792/792 [==============================] - 0s 126us/step - loss: 2.4254 - acc: 0.4331 - val_loss: 2.5024 - val_acc: 0.4444\n",
      "Epoch 18/100\n",
      "792/792 [==============================] - 0s 116us/step - loss: 2.3062 - acc: 0.4697 - val_loss: 2.4033 - val_acc: 0.4343\n",
      "Epoch 19/100\n",
      "792/792 [==============================] - 0s 118us/step - loss: 2.1948 - acc: 0.4558 - val_loss: 2.2590 - val_acc: 0.4949\n",
      "Epoch 20/100\n",
      "792/792 [==============================] - 0s 115us/step - loss: 2.1066 - acc: 0.5013 - val_loss: 2.1517 - val_acc: 0.5808\n",
      "Epoch 21/100\n",
      "792/792 [==============================] - 0s 123us/step - loss: 2.0016 - acc: 0.5429 - val_loss: 2.0498 - val_acc: 0.5404\n",
      "Epoch 22/100\n",
      "792/792 [==============================] - 0s 123us/step - loss: 1.9181 - acc: 0.5568 - val_loss: 1.9499 - val_acc: 0.5960\n",
      "Epoch 23/100\n",
      "792/792 [==============================] - 0s 130us/step - loss: 1.8547 - acc: 0.5530 - val_loss: 1.8741 - val_acc: 0.6162\n",
      "Epoch 24/100\n",
      "792/792 [==============================] - 0s 169us/step - loss: 1.7700 - acc: 0.5745 - val_loss: 1.8004 - val_acc: 0.6162\n",
      "Epoch 25/100\n",
      "792/792 [==============================] - 0s 148us/step - loss: 1.7033 - acc: 0.5808 - val_loss: 1.7169 - val_acc: 0.6263\n",
      "Epoch 26/100\n",
      "792/792 [==============================] - 0s 142us/step - loss: 1.6070 - acc: 0.6098 - val_loss: 1.6629 - val_acc: 0.5960\n",
      "Epoch 27/100\n",
      "792/792 [==============================] - 0s 150us/step - loss: 1.5738 - acc: 0.6124 - val_loss: 1.5953 - val_acc: 0.6162\n",
      "Epoch 28/100\n",
      "792/792 [==============================] - 0s 143us/step - loss: 1.4840 - acc: 0.6301 - val_loss: 1.5082 - val_acc: 0.6465\n",
      "Epoch 29/100\n",
      "792/792 [==============================] - 0s 149us/step - loss: 1.4387 - acc: 0.6465 - val_loss: 1.4261 - val_acc: 0.7121\n",
      "Epoch 30/100\n",
      "792/792 [==============================] - 0s 140us/step - loss: 1.3937 - acc: 0.6515 - val_loss: 1.3991 - val_acc: 0.6869\n",
      "Epoch 31/100\n",
      "792/792 [==============================] - 0s 143us/step - loss: 1.3508 - acc: 0.6604 - val_loss: 1.3357 - val_acc: 0.7121\n",
      "Epoch 32/100\n",
      "792/792 [==============================] - 0s 148us/step - loss: 1.3067 - acc: 0.6553 - val_loss: 1.3023 - val_acc: 0.7071\n",
      "Epoch 33/100\n",
      "792/792 [==============================] - 0s 143us/step - loss: 1.2395 - acc: 0.6856 - val_loss: 1.2481 - val_acc: 0.7273\n",
      "Epoch 34/100\n",
      "792/792 [==============================] - 0s 142us/step - loss: 1.2033 - acc: 0.6843 - val_loss: 1.2187 - val_acc: 0.7020\n",
      "Epoch 35/100\n",
      "792/792 [==============================] - 0s 140us/step - loss: 1.1562 - acc: 0.7184 - val_loss: 1.1794 - val_acc: 0.7273\n",
      "Epoch 36/100\n",
      "792/792 [==============================] - 0s 140us/step - loss: 1.1397 - acc: 0.6982 - val_loss: 1.1188 - val_acc: 0.7677\n",
      "Epoch 37/100\n",
      "792/792 [==============================] - 0s 143us/step - loss: 1.1202 - acc: 0.7172 - val_loss: 1.0844 - val_acc: 0.7727\n",
      "Epoch 38/100\n",
      "792/792 [==============================] - 0s 141us/step - loss: 1.0650 - acc: 0.7336 - val_loss: 1.0647 - val_acc: 0.7828\n",
      "Epoch 39/100\n",
      "792/792 [==============================] - 0s 145us/step - loss: 1.0500 - acc: 0.7172 - val_loss: 1.0370 - val_acc: 0.8182\n",
      "Epoch 40/100\n",
      "792/792 [==============================] - 0s 156us/step - loss: 0.9945 - acc: 0.7538 - val_loss: 0.9684 - val_acc: 0.8283\n",
      "Epoch 41/100\n",
      "792/792 [==============================] - 0s 138us/step - loss: 0.9871 - acc: 0.7386 - val_loss: 0.9481 - val_acc: 0.8081\n",
      "Epoch 42/100\n",
      "792/792 [==============================] - 0s 115us/step - loss: 0.9504 - acc: 0.7500 - val_loss: 0.9390 - val_acc: 0.8030\n",
      "Epoch 43/100\n",
      "792/792 [==============================] - 0s 119us/step - loss: 0.9051 - acc: 0.7652 - val_loss: 0.8897 - val_acc: 0.7980\n",
      "Epoch 44/100\n",
      "792/792 [==============================] - 0s 119us/step - loss: 0.8887 - acc: 0.7652 - val_loss: 0.8828 - val_acc: 0.8131\n",
      "Epoch 45/100\n",
      "792/792 [==============================] - 0s 113us/step - loss: 0.8455 - acc: 0.7790 - val_loss: 0.8874 - val_acc: 0.8081\n",
      "Epoch 46/100\n",
      "792/792 [==============================] - 0s 111us/step - loss: 0.8632 - acc: 0.7626 - val_loss: 0.8490 - val_acc: 0.8030\n",
      "Epoch 47/100\n",
      "792/792 [==============================] - 0s 116us/step - loss: 0.8407 - acc: 0.7601 - val_loss: 0.8057 - val_acc: 0.8283\n",
      "Epoch 48/100\n",
      "792/792 [==============================] - 0s 116us/step - loss: 0.8115 - acc: 0.7841 - val_loss: 0.7862 - val_acc: 0.8232\n",
      "Epoch 49/100\n",
      "792/792 [==============================] - 0s 122us/step - loss: 0.7762 - acc: 0.7992 - val_loss: 0.7789 - val_acc: 0.8283\n",
      "Epoch 50/100\n",
      "792/792 [==============================] - 0s 121us/step - loss: 0.7504 - acc: 0.7980 - val_loss: 0.7779 - val_acc: 0.8283\n",
      "Epoch 51/100\n",
      "792/792 [==============================] - 0s 129us/step - loss: 0.7539 - acc: 0.7955 - val_loss: 0.7303 - val_acc: 0.8434\n",
      "Epoch 52/100\n",
      "792/792 [==============================] - 0s 127us/step - loss: 0.7130 - acc: 0.8068 - val_loss: 0.7155 - val_acc: 0.8283\n",
      "Epoch 53/100\n",
      "792/792 [==============================] - 0s 124us/step - loss: 0.6928 - acc: 0.8131 - val_loss: 0.7018 - val_acc: 0.8384\n",
      "Epoch 54/100\n",
      "792/792 [==============================] - 0s 118us/step - loss: 0.6887 - acc: 0.8030 - val_loss: 0.7038 - val_acc: 0.8434\n",
      "Epoch 55/100\n",
      "792/792 [==============================] - 0s 120us/step - loss: 0.6476 - acc: 0.8422 - val_loss: 0.6602 - val_acc: 0.8636\n",
      "Epoch 56/100\n",
      "792/792 [==============================] - 0s 113us/step - loss: 0.6491 - acc: 0.8321 - val_loss: 0.6639 - val_acc: 0.8485\n",
      "Epoch 57/100\n",
      "792/792 [==============================] - 0s 120us/step - loss: 0.6277 - acc: 0.8258 - val_loss: 0.6625 - val_acc: 0.8283\n",
      "Epoch 58/100\n",
      "792/792 [==============================] - 0s 116us/step - loss: 0.6388 - acc: 0.8371 - val_loss: 0.5994 - val_acc: 0.8586\n",
      "Epoch 59/100\n",
      "792/792 [==============================] - 0s 116us/step - loss: 0.5994 - acc: 0.8422 - val_loss: 0.6048 - val_acc: 0.8485\n",
      "Epoch 60/100\n",
      "792/792 [==============================] - 0s 111us/step - loss: 0.6051 - acc: 0.8422 - val_loss: 0.6245 - val_acc: 0.8687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/100\n",
      "792/792 [==============================] - 0s 120us/step - loss: 0.6010 - acc: 0.8131 - val_loss: 0.6073 - val_acc: 0.8535\n",
      "Epoch 62/100\n",
      "792/792 [==============================] - 0s 105us/step - loss: 0.5454 - acc: 0.8624 - val_loss: 0.5764 - val_acc: 0.8687\n",
      "Epoch 63/100\n",
      "792/792 [==============================] - 0s 106us/step - loss: 0.5523 - acc: 0.8497 - val_loss: 0.5903 - val_acc: 0.8636\n",
      "Epoch 64/100\n",
      "792/792 [==============================] - 0s 103us/step - loss: 0.5197 - acc: 0.8548 - val_loss: 0.5420 - val_acc: 0.8636\n",
      "Epoch 65/100\n",
      "792/792 [==============================] - 0s 105us/step - loss: 0.5192 - acc: 0.8674 - val_loss: 0.5669 - val_acc: 0.8586\n",
      "Epoch 66/100\n",
      "792/792 [==============================] - 0s 113us/step - loss: 0.5175 - acc: 0.8687 - val_loss: 0.5698 - val_acc: 0.8687\n",
      "Epoch 67/100\n",
      "792/792 [==============================] - 0s 113us/step - loss: 0.4904 - acc: 0.8737 - val_loss: 0.5364 - val_acc: 0.8737\n",
      "Epoch 68/100\n",
      "792/792 [==============================] - 0s 110us/step - loss: 0.5099 - acc: 0.8548 - val_loss: 0.5220 - val_acc: 0.8687\n",
      "Epoch 69/100\n",
      "792/792 [==============================] - 0s 105us/step - loss: 0.5018 - acc: 0.8649 - val_loss: 0.5256 - val_acc: 0.8636\n",
      "Epoch 70/100\n",
      "792/792 [==============================] - 0s 107us/step - loss: 0.4553 - acc: 0.8737 - val_loss: 0.4954 - val_acc: 0.8788\n",
      "Epoch 71/100\n",
      "792/792 [==============================] - 0s 107us/step - loss: 0.4716 - acc: 0.8788 - val_loss: 0.5095 - val_acc: 0.8737\n",
      "Epoch 72/100\n",
      "792/792 [==============================] - 0s 106us/step - loss: 0.4501 - acc: 0.8864 - val_loss: 0.4873 - val_acc: 0.8788\n",
      "Epoch 73/100\n",
      "792/792 [==============================] - 0s 111us/step - loss: 0.4415 - acc: 0.8725 - val_loss: 0.4789 - val_acc: 0.8636\n",
      "Epoch 74/100\n",
      "792/792 [==============================] - 0s 106us/step - loss: 0.4362 - acc: 0.8813 - val_loss: 0.4847 - val_acc: 0.8636\n",
      "Epoch 75/100\n",
      "792/792 [==============================] - 0s 108us/step - loss: 0.4116 - acc: 0.8889 - val_loss: 0.4636 - val_acc: 0.8687\n",
      "Epoch 76/100\n",
      "792/792 [==============================] - 0s 109us/step - loss: 0.4506 - acc: 0.8838 - val_loss: 0.4918 - val_acc: 0.8889\n",
      "Epoch 77/100\n",
      "792/792 [==============================] - 0s 106us/step - loss: 0.4014 - acc: 0.8801 - val_loss: 0.4680 - val_acc: 0.8687\n",
      "Epoch 78/100\n",
      "792/792 [==============================] - 0s 106us/step - loss: 0.3994 - acc: 0.8965 - val_loss: 0.4419 - val_acc: 0.8838\n",
      "Epoch 79/100\n",
      "792/792 [==============================] - 0s 108us/step - loss: 0.3671 - acc: 0.9116 - val_loss: 0.4381 - val_acc: 0.8788\n",
      "Epoch 80/100\n",
      "792/792 [==============================] - 0s 106us/step - loss: 0.3835 - acc: 0.8990 - val_loss: 0.4586 - val_acc: 0.8788\n",
      "Epoch 81/100\n",
      "792/792 [==============================] - 0s 106us/step - loss: 0.4123 - acc: 0.8914 - val_loss: 0.4428 - val_acc: 0.8586\n",
      "Epoch 82/100\n",
      "792/792 [==============================] - 0s 113us/step - loss: 0.3561 - acc: 0.9028 - val_loss: 0.4253 - val_acc: 0.8687\n",
      "Epoch 83/100\n",
      "792/792 [==============================] - 0s 108us/step - loss: 0.3711 - acc: 0.8977 - val_loss: 0.4031 - val_acc: 0.8889\n",
      "Epoch 84/100\n",
      "792/792 [==============================] - 0s 109us/step - loss: 0.3617 - acc: 0.8977 - val_loss: 0.3954 - val_acc: 0.8838\n",
      "Epoch 85/100\n",
      "792/792 [==============================] - 0s 129us/step - loss: 0.3656 - acc: 0.8927 - val_loss: 0.3875 - val_acc: 0.9040\n",
      "Epoch 86/100\n",
      "792/792 [==============================] - 0s 107us/step - loss: 0.3392 - acc: 0.9028 - val_loss: 0.4011 - val_acc: 0.8838\n",
      "Epoch 87/100\n",
      "792/792 [==============================] - 0s 110us/step - loss: 0.3353 - acc: 0.9116 - val_loss: 0.3898 - val_acc: 0.8889\n",
      "Epoch 88/100\n",
      "792/792 [==============================] - 0s 105us/step - loss: 0.3077 - acc: 0.9242 - val_loss: 0.3812 - val_acc: 0.9141\n",
      "Epoch 89/100\n",
      "792/792 [==============================] - 0s 108us/step - loss: 0.3399 - acc: 0.8914 - val_loss: 0.3749 - val_acc: 0.8939\n",
      "Epoch 90/100\n",
      "792/792 [==============================] - 0s 111us/step - loss: 0.3172 - acc: 0.9268 - val_loss: 0.3731 - val_acc: 0.8990\n",
      "Epoch 91/100\n",
      "792/792 [==============================] - 0s 105us/step - loss: 0.3121 - acc: 0.9104 - val_loss: 0.3727 - val_acc: 0.9040\n",
      "Epoch 92/100\n",
      "792/792 [==============================] - 0s 111us/step - loss: 0.3262 - acc: 0.9015 - val_loss: 0.3670 - val_acc: 0.8939\n",
      "Epoch 93/100\n",
      "792/792 [==============================] - 0s 109us/step - loss: 0.2941 - acc: 0.9242 - val_loss: 0.3708 - val_acc: 0.8788\n",
      "Epoch 94/100\n",
      "792/792 [==============================] - 0s 107us/step - loss: 0.3120 - acc: 0.9053 - val_loss: 0.3776 - val_acc: 0.8838\n",
      "Epoch 95/100\n",
      "792/792 [==============================] - 0s 106us/step - loss: 0.2843 - acc: 0.9192 - val_loss: 0.3684 - val_acc: 0.8838\n",
      "Epoch 96/100\n",
      "792/792 [==============================] - 0s 114us/step - loss: 0.2901 - acc: 0.9192 - val_loss: 0.3643 - val_acc: 0.8788\n",
      "Epoch 97/100\n",
      "792/792 [==============================] - 0s 113us/step - loss: 0.2858 - acc: 0.9192 - val_loss: 0.3535 - val_acc: 0.8889\n",
      "Epoch 98/100\n",
      "792/792 [==============================] - 0s 106us/step - loss: 0.2881 - acc: 0.9179 - val_loss: 0.3628 - val_acc: 0.8889\n",
      "Epoch 99/100\n",
      "792/792 [==============================] - 0s 108us/step - loss: 0.2791 - acc: 0.9306 - val_loss: 0.3553 - val_acc: 0.8889\n",
      "Epoch 100/100\n",
      "792/792 [==============================] - 0s 109us/step - loss: 0.2887 - acc: 0.9192 - val_loss: 0.3328 - val_acc: 0.8939\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x1e5e7920fd0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(raw_data,label_cat,batch_size=50,epochs=100,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 792 samples, validate on 198 samples\n",
      "Epoch 1/100\n",
      "792/792 [==============================] - 0s 53us/step - loss: 0.2413 - acc: 0.9394 - val_loss: 0.3147 - val_acc: 0.9141\n",
      "Epoch 2/100\n",
      "792/792 [==============================] - 0s 63us/step - loss: 0.2526 - acc: 0.9331 - val_loss: 0.3266 - val_acc: 0.8889\n",
      "Epoch 3/100\n",
      "792/792 [==============================] - 0s 65us/step - loss: 0.2291 - acc: 0.9457 - val_loss: 0.3499 - val_acc: 0.8838\n",
      "Epoch 4/100\n",
      "792/792 [==============================] - 0s 63us/step - loss: 0.2406 - acc: 0.9306 - val_loss: 0.3362 - val_acc: 0.9091\n",
      "Epoch 5/100\n",
      "792/792 [==============================] - 0s 59us/step - loss: 0.2515 - acc: 0.9356 - val_loss: 0.3373 - val_acc: 0.8838\n",
      "Epoch 6/100\n",
      "792/792 [==============================] - 0s 63us/step - loss: 0.2418 - acc: 0.9394 - val_loss: 0.3338 - val_acc: 0.8990\n",
      "Epoch 7/100\n",
      "792/792 [==============================] - 0s 61us/step - loss: 0.2471 - acc: 0.9293 - val_loss: 0.3302 - val_acc: 0.8990\n",
      "Epoch 8/100\n",
      "792/792 [==============================] - 0s 63us/step - loss: 0.2315 - acc: 0.9457 - val_loss: 0.3382 - val_acc: 0.8889\n",
      "Epoch 9/100\n",
      "792/792 [==============================] - 0s 61us/step - loss: 0.2303 - acc: 0.9268 - val_loss: 0.3281 - val_acc: 0.9141\n",
      "Epoch 10/100\n",
      "792/792 [==============================] - 0s 60us/step - loss: 0.2483 - acc: 0.9217 - val_loss: 0.3460 - val_acc: 0.8838\n",
      "Epoch 11/100\n",
      "792/792 [==============================] - 0s 63us/step - loss: 0.2223 - acc: 0.9444 - val_loss: 0.3246 - val_acc: 0.9040\n",
      "Epoch 12/100\n",
      "792/792 [==============================] - 0s 66us/step - loss: 0.2426 - acc: 0.9369 - val_loss: 0.3167 - val_acc: 0.8990\n",
      "Epoch 13/100\n",
      "792/792 [==============================] - 0s 67us/step - loss: 0.2434 - acc: 0.9331 - val_loss: 0.3233 - val_acc: 0.8990\n",
      "Epoch 14/100\n",
      "792/792 [==============================] - 0s 61us/step - loss: 0.2508 - acc: 0.9205 - val_loss: 0.3337 - val_acc: 0.8990\n",
      "Epoch 15/100\n",
      "792/792 [==============================] - 0s 62us/step - loss: 0.2345 - acc: 0.9293 - val_loss: 0.3199 - val_acc: 0.9040\n",
      "Epoch 16/100\n",
      "792/792 [==============================] - 0s 63us/step - loss: 0.2336 - acc: 0.9293 - val_loss: 0.3210 - val_acc: 0.9040\n",
      "Epoch 17/100\n",
      "792/792 [==============================] - 0s 59us/step - loss: 0.2229 - acc: 0.9280 - val_loss: 0.3161 - val_acc: 0.8939\n",
      "Epoch 18/100\n",
      "792/792 [==============================] - 0s 65us/step - loss: 0.2048 - acc: 0.9495 - val_loss: 0.2920 - val_acc: 0.9040\n",
      "Epoch 19/100\n",
      "792/792 [==============================] - 0s 66us/step - loss: 0.2135 - acc: 0.9394 - val_loss: 0.3161 - val_acc: 0.8939\n",
      "Epoch 20/100\n",
      "792/792 [==============================] - 0s 61us/step - loss: 0.2283 - acc: 0.9381 - val_loss: 0.3092 - val_acc: 0.9091\n",
      "Epoch 21/100\n",
      "792/792 [==============================] - 0s 59us/step - loss: 0.2087 - acc: 0.9419 - val_loss: 0.3239 - val_acc: 0.8990\n",
      "Epoch 22/100\n",
      "792/792 [==============================] - 0s 62us/step - loss: 0.2124 - acc: 0.9356 - val_loss: 0.3216 - val_acc: 0.8889\n",
      "Epoch 23/100\n",
      "792/792 [==============================] - 0s 59us/step - loss: 0.1904 - acc: 0.9634 - val_loss: 0.3085 - val_acc: 0.9192\n",
      "Epoch 24/100\n",
      "792/792 [==============================] - 0s 61us/step - loss: 0.2035 - acc: 0.9508 - val_loss: 0.3209 - val_acc: 0.9091\n",
      "Epoch 25/100\n",
      "792/792 [==============================] - 0s 65us/step - loss: 0.2009 - acc: 0.9457 - val_loss: 0.3061 - val_acc: 0.9192\n",
      "Epoch 26/100\n",
      "792/792 [==============================] - 0s 60us/step - loss: 0.1912 - acc: 0.9545 - val_loss: 0.2936 - val_acc: 0.9242\n",
      "Epoch 27/100\n",
      "792/792 [==============================] - 0s 62us/step - loss: 0.1932 - acc: 0.9520 - val_loss: 0.3128 - val_acc: 0.9192\n",
      "Epoch 28/100\n",
      "792/792 [==============================] - 0s 57us/step - loss: 0.2012 - acc: 0.9419 - val_loss: 0.3507 - val_acc: 0.8838\n",
      "Epoch 29/100\n",
      "792/792 [==============================] - 0s 61us/step - loss: 0.2000 - acc: 0.9407 - val_loss: 0.2988 - val_acc: 0.9091\n",
      "Epoch 30/100\n",
      "792/792 [==============================] - 0s 60us/step - loss: 0.1885 - acc: 0.9545 - val_loss: 0.2821 - val_acc: 0.9141\n",
      "Epoch 31/100\n",
      "792/792 [==============================] - 0s 65us/step - loss: 0.1854 - acc: 0.9444 - val_loss: 0.2968 - val_acc: 0.9192\n",
      "Epoch 32/100\n",
      "792/792 [==============================] - 0s 59us/step - loss: 0.2024 - acc: 0.9343 - val_loss: 0.2797 - val_acc: 0.9141\n",
      "Epoch 33/100\n",
      "792/792 [==============================] - 0s 59us/step - loss: 0.1768 - acc: 0.9495 - val_loss: 0.2939 - val_acc: 0.9343\n",
      "Epoch 34/100\n",
      "792/792 [==============================] - 0s 62us/step - loss: 0.1821 - acc: 0.9596 - val_loss: 0.3196 - val_acc: 0.8889\n",
      "Epoch 35/100\n",
      "792/792 [==============================] - 0s 65us/step - loss: 0.1664 - acc: 0.9583 - val_loss: 0.3110 - val_acc: 0.9040\n",
      "Epoch 36/100\n",
      "792/792 [==============================] - 0s 61us/step - loss: 0.1969 - acc: 0.9407 - val_loss: 0.3026 - val_acc: 0.9091\n",
      "Epoch 37/100\n",
      "792/792 [==============================] - 0s 59us/step - loss: 0.1933 - acc: 0.9457 - val_loss: 0.2924 - val_acc: 0.9141\n",
      "Epoch 38/100\n",
      "792/792 [==============================] - 0s 58us/step - loss: 0.1832 - acc: 0.9508 - val_loss: 0.3036 - val_acc: 0.9141\n",
      "Epoch 39/100\n",
      "792/792 [==============================] - 0s 55us/step - loss: 0.1855 - acc: 0.9508 - val_loss: 0.3209 - val_acc: 0.9040\n",
      "Epoch 40/100\n",
      "792/792 [==============================] - 0s 58us/step - loss: 0.1770 - acc: 0.9482 - val_loss: 0.2861 - val_acc: 0.8990\n",
      "Epoch 41/100\n",
      "792/792 [==============================] - 0s 58us/step - loss: 0.1732 - acc: 0.9583 - val_loss: 0.2716 - val_acc: 0.9091\n",
      "Epoch 42/100\n",
      "792/792 [==============================] - 0s 54us/step - loss: 0.1748 - acc: 0.9533 - val_loss: 0.2623 - val_acc: 0.9192\n",
      "Epoch 43/100\n",
      "792/792 [==============================] - 0s 61us/step - loss: 0.1675 - acc: 0.9571 - val_loss: 0.2928 - val_acc: 0.9040\n",
      "Epoch 44/100\n",
      "792/792 [==============================] - 0s 62us/step - loss: 0.1843 - acc: 0.9457 - val_loss: 0.2845 - val_acc: 0.9293\n",
      "Epoch 45/100\n",
      "792/792 [==============================] - 0s 60us/step - loss: 0.1682 - acc: 0.9495 - val_loss: 0.2688 - val_acc: 0.9293\n",
      "Epoch 46/100\n",
      "792/792 [==============================] - 0s 55us/step - loss: 0.1779 - acc: 0.9558 - val_loss: 0.2899 - val_acc: 0.9091\n",
      "Epoch 47/100\n",
      "792/792 [==============================] - 0s 57us/step - loss: 0.1551 - acc: 0.9659 - val_loss: 0.2861 - val_acc: 0.9242\n",
      "Epoch 48/100\n",
      "792/792 [==============================] - 0s 56us/step - loss: 0.1476 - acc: 0.9646 - val_loss: 0.2740 - val_acc: 0.9192\n",
      "Epoch 49/100\n",
      "792/792 [==============================] - 0s 58us/step - loss: 0.1411 - acc: 0.9684 - val_loss: 0.3092 - val_acc: 0.9040\n",
      "Epoch 50/100\n",
      "792/792 [==============================] - 0s 54us/step - loss: 0.1742 - acc: 0.9520 - val_loss: 0.2978 - val_acc: 0.9091\n",
      "Epoch 51/100\n",
      "792/792 [==============================] - 0s 61us/step - loss: 0.1765 - acc: 0.9520 - val_loss: 0.2984 - val_acc: 0.9040\n",
      "Epoch 52/100\n",
      "792/792 [==============================] - 0s 57us/step - loss: 0.1620 - acc: 0.9533 - val_loss: 0.2950 - val_acc: 0.8990\n",
      "Epoch 53/100\n",
      "792/792 [==============================] - 0s 64us/step - loss: 0.1413 - acc: 0.9659 - val_loss: 0.2770 - val_acc: 0.9141\n",
      "Epoch 54/100\n",
      "792/792 [==============================] - 0s 56us/step - loss: 0.1665 - acc: 0.9634 - val_loss: 0.2656 - val_acc: 0.9141\n",
      "Epoch 55/100\n",
      "792/792 [==============================] - 0s 57us/step - loss: 0.1719 - acc: 0.9533 - val_loss: 0.2705 - val_acc: 0.9141\n",
      "Epoch 56/100\n",
      "792/792 [==============================] - 0s 59us/step - loss: 0.1567 - acc: 0.9646 - val_loss: 0.2753 - val_acc: 0.9141\n",
      "Epoch 57/100\n",
      "792/792 [==============================] - 0s 58us/step - loss: 0.1442 - acc: 0.9609 - val_loss: 0.2635 - val_acc: 0.9141\n",
      "Epoch 58/100\n",
      "792/792 [==============================] - 0s 61us/step - loss: 0.1543 - acc: 0.9558 - val_loss: 0.2625 - val_acc: 0.9242\n",
      "Epoch 59/100\n",
      "792/792 [==============================] - 0s 61us/step - loss: 0.1457 - acc: 0.9697 - val_loss: 0.2658 - val_acc: 0.9293\n",
      "Epoch 60/100\n",
      "792/792 [==============================] - 0s 61us/step - loss: 0.1697 - acc: 0.9533 - val_loss: 0.2666 - val_acc: 0.9242\n",
      "Epoch 61/100\n",
      "792/792 [==============================] - 0s 60us/step - loss: 0.1549 - acc: 0.9558 - val_loss: 0.2645 - val_acc: 0.9091\n",
      "Epoch 62/100\n",
      "792/792 [==============================] - 0s 53us/step - loss: 0.1319 - acc: 0.9621 - val_loss: 0.2674 - val_acc: 0.9040\n",
      "Epoch 63/100\n",
      "792/792 [==============================] - 0s 54us/step - loss: 0.1461 - acc: 0.9646 - val_loss: 0.2495 - val_acc: 0.9242\n",
      "Epoch 64/100\n",
      "792/792 [==============================] - 0s 53us/step - loss: 0.1741 - acc: 0.9482 - val_loss: 0.2549 - val_acc: 0.9040\n",
      "Epoch 65/100\n",
      "792/792 [==============================] - 0s 56us/step - loss: 0.1462 - acc: 0.9609 - val_loss: 0.2717 - val_acc: 0.8990\n",
      "Epoch 66/100\n",
      "792/792 [==============================] - 0s 55us/step - loss: 0.1604 - acc: 0.9609 - val_loss: 0.2890 - val_acc: 0.8939\n",
      "Epoch 67/100\n",
      "792/792 [==============================] - 0s 53us/step - loss: 0.1349 - acc: 0.9722 - val_loss: 0.2753 - val_acc: 0.8939\n",
      "Epoch 68/100\n",
      "792/792 [==============================] - 0s 51us/step - loss: 0.1303 - acc: 0.9710 - val_loss: 0.2679 - val_acc: 0.9192\n",
      "Epoch 69/100\n",
      "792/792 [==============================] - 0s 52us/step - loss: 0.1485 - acc: 0.9621 - val_loss: 0.2927 - val_acc: 0.8990\n",
      "Epoch 70/100\n",
      "792/792 [==============================] - 0s 53us/step - loss: 0.1379 - acc: 0.9710 - val_loss: 0.2644 - val_acc: 0.9141\n",
      "Epoch 71/100\n",
      "792/792 [==============================] - 0s 54us/step - loss: 0.1271 - acc: 0.9735 - val_loss: 0.2744 - val_acc: 0.9141\n",
      "Epoch 72/100\n",
      "792/792 [==============================] - 0s 53us/step - loss: 0.1317 - acc: 0.9672 - val_loss: 0.2741 - val_acc: 0.9091\n",
      "Epoch 73/100\n",
      "792/792 [==============================] - 0s 58us/step - loss: 0.1368 - acc: 0.9697 - val_loss: 0.2646 - val_acc: 0.9242\n",
      "Epoch 74/100\n",
      "792/792 [==============================] - 0s 53us/step - loss: 0.1261 - acc: 0.9684 - val_loss: 0.2468 - val_acc: 0.9242\n",
      "Epoch 75/100\n",
      "792/792 [==============================] - 0s 57us/step - loss: 0.1235 - acc: 0.9735 - val_loss: 0.2770 - val_acc: 0.8990\n",
      "Epoch 76/100\n",
      "792/792 [==============================] - 0s 59us/step - loss: 0.1225 - acc: 0.9722 - val_loss: 0.2861 - val_acc: 0.9040\n",
      "Epoch 77/100\n",
      "792/792 [==============================] - 0s 56us/step - loss: 0.1245 - acc: 0.9684 - val_loss: 0.2807 - val_acc: 0.9141\n",
      "Epoch 78/100\n",
      "792/792 [==============================] - 0s 56us/step - loss: 0.1379 - acc: 0.9646 - val_loss: 0.2463 - val_acc: 0.9091\n",
      "Epoch 79/100\n",
      "792/792 [==============================] - 0s 52us/step - loss: 0.1467 - acc: 0.9558 - val_loss: 0.2758 - val_acc: 0.9040\n",
      "Epoch 80/100\n",
      "792/792 [==============================] - 0s 54us/step - loss: 0.1132 - acc: 0.9747 - val_loss: 0.2576 - val_acc: 0.9192\n",
      "Epoch 81/100\n",
      "792/792 [==============================] - 0s 53us/step - loss: 0.1223 - acc: 0.9722 - val_loss: 0.2595 - val_acc: 0.8990\n",
      "Epoch 82/100\n",
      "792/792 [==============================] - 0s 53us/step - loss: 0.1390 - acc: 0.9596 - val_loss: 0.2719 - val_acc: 0.9141\n",
      "Epoch 83/100\n",
      "792/792 [==============================] - 0s 53us/step - loss: 0.1224 - acc: 0.9609 - val_loss: 0.2548 - val_acc: 0.8990\n",
      "Epoch 84/100\n",
      "792/792 [==============================] - 0s 52us/step - loss: 0.1284 - acc: 0.9646 - val_loss: 0.2593 - val_acc: 0.9141\n",
      "Epoch 85/100\n",
      "792/792 [==============================] - 0s 55us/step - loss: 0.1119 - acc: 0.9722 - val_loss: 0.2935 - val_acc: 0.8990\n",
      "Epoch 86/100\n",
      "792/792 [==============================] - 0s 51us/step - loss: 0.1007 - acc: 0.9747 - val_loss: 0.2817 - val_acc: 0.9141\n",
      "Epoch 87/100\n",
      "792/792 [==============================] - 0s 54us/step - loss: 0.1261 - acc: 0.9659 - val_loss: 0.2753 - val_acc: 0.9091\n",
      "Epoch 88/100\n",
      "792/792 [==============================] - 0s 54us/step - loss: 0.1308 - acc: 0.9659 - val_loss: 0.2647 - val_acc: 0.9141\n",
      "Epoch 89/100\n",
      "792/792 [==============================] - 0s 54us/step - loss: 0.1323 - acc: 0.9634 - val_loss: 0.2483 - val_acc: 0.9091\n",
      "Epoch 90/100\n",
      "792/792 [==============================] - 0s 56us/step - loss: 0.1061 - acc: 0.9811 - val_loss: 0.2266 - val_acc: 0.9293\n",
      "Epoch 91/100\n",
      "792/792 [==============================] - 0s 51us/step - loss: 0.1215 - acc: 0.9747 - val_loss: 0.2618 - val_acc: 0.9141\n",
      "Epoch 92/100\n",
      "792/792 [==============================] - 0s 54us/step - loss: 0.1104 - acc: 0.9747 - val_loss: 0.2540 - val_acc: 0.9192\n",
      "Epoch 93/100\n",
      "792/792 [==============================] - 0s 54us/step - loss: 0.1099 - acc: 0.9785 - val_loss: 0.2640 - val_acc: 0.9091\n",
      "Epoch 94/100\n",
      "792/792 [==============================] - 0s 53us/step - loss: 0.1282 - acc: 0.9672 - val_loss: 0.2262 - val_acc: 0.9293\n",
      "Epoch 95/100\n",
      "792/792 [==============================] - 0s 54us/step - loss: 0.0987 - acc: 0.9785 - val_loss: 0.2398 - val_acc: 0.9192\n",
      "Epoch 96/100\n",
      "792/792 [==============================] - 0s 53us/step - loss: 0.0977 - acc: 0.9823 - val_loss: 0.2445 - val_acc: 0.9192\n",
      "Epoch 97/100\n",
      "792/792 [==============================] - 0s 54us/step - loss: 0.1089 - acc: 0.9684 - val_loss: 0.2264 - val_acc: 0.9242\n",
      "Epoch 98/100\n",
      "792/792 [==============================] - 0s 56us/step - loss: 0.1195 - acc: 0.9634 - val_loss: 0.2412 - val_acc: 0.9141\n",
      "Epoch 99/100\n",
      "792/792 [==============================] - 0s 63us/step - loss: 0.0931 - acc: 0.9785 - val_loss: 0.2352 - val_acc: 0.9343\n",
      "Epoch 100/100\n",
      "792/792 [==============================] - 0s 53us/step - loss: 0.1150 - acc: 0.9634 - val_loss: 0.2346 - val_acc: 0.9192\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x1e5f1c070f0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(raw_data,label_cat,batch_size=150,epochs=100,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = Sequential()\n",
    "model4.add(Dense(768,input_dim=192, activation='tanh'))\n",
    "model4.add(Dropout(0.6))\n",
    "model4.add(Dense(768, activation='tanh'))\n",
    "model4.add(Dropout(0.6))\n",
    "model4.add(Dense(99, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4.compile(loss='categorical_crossentropy',optimizer='rmsprop', metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 792 samples, validate on 198 samples\n",
      "Epoch 1/100\n",
      "792/792 [==============================] - 0s 551us/step - loss: 4.5509 - acc: 0.0556 - val_loss: 4.4737 - val_acc: 0.1465\n",
      "Epoch 2/100\n",
      "792/792 [==============================] - 0s 74us/step - loss: 4.3798 - acc: 0.2172 - val_loss: 4.3503 - val_acc: 0.1566\n",
      "Epoch 3/100\n",
      "792/792 [==============================] - 0s 86us/step - loss: 4.1849 - acc: 0.2942 - val_loss: 4.1663 - val_acc: 0.1465\n",
      "Epoch 4/100\n",
      "792/792 [==============================] - 0s 80us/step - loss: 3.9338 - acc: 0.2891 - val_loss: 3.9301 - val_acc: 0.1566\n",
      "Epoch 5/100\n",
      "792/792 [==============================] - 0s 81us/step - loss: 3.6280 - acc: 0.2955 - val_loss: 3.6823 - val_acc: 0.1465\n",
      "Epoch 6/100\n",
      "792/792 [==============================] - 0s 78us/step - loss: 3.3452 - acc: 0.3056 - val_loss: 3.4314 - val_acc: 0.1667\n",
      "Epoch 7/100\n",
      "792/792 [==============================] - 0s 78us/step - loss: 3.0703 - acc: 0.3510 - val_loss: 3.1930 - val_acc: 0.2020\n",
      "Epoch 8/100\n",
      "792/792 [==============================] - 0s 79us/step - loss: 2.8174 - acc: 0.3699 - val_loss: 2.9642 - val_acc: 0.2475\n",
      "Epoch 9/100\n",
      "792/792 [==============================] - 0s 85us/step - loss: 2.6184 - acc: 0.4141 - val_loss: 2.7591 - val_acc: 0.3081\n",
      "Epoch 10/100\n",
      "792/792 [==============================] - 0s 76us/step - loss: 2.4377 - acc: 0.4571 - val_loss: 2.5796 - val_acc: 0.3485\n",
      "Epoch 11/100\n",
      "792/792 [==============================] - 0s 77us/step - loss: 2.2526 - acc: 0.4912 - val_loss: 2.4106 - val_acc: 0.4343\n",
      "Epoch 12/100\n",
      "792/792 [==============================] - 0s 78us/step - loss: 2.1115 - acc: 0.5379 - val_loss: 2.2337 - val_acc: 0.5303\n",
      "Epoch 13/100\n",
      "792/792 [==============================] - 0s 76us/step - loss: 1.9405 - acc: 0.5732 - val_loss: 2.1059 - val_acc: 0.5707\n",
      "Epoch 14/100\n",
      "792/792 [==============================] - 0s 81us/step - loss: 1.8308 - acc: 0.5922 - val_loss: 1.9632 - val_acc: 0.6111\n",
      "Epoch 15/100\n",
      "792/792 [==============================] - 0s 77us/step - loss: 1.7283 - acc: 0.6237 - val_loss: 1.8276 - val_acc: 0.6465\n",
      "Epoch 16/100\n",
      "792/792 [==============================] - 0s 74us/step - loss: 1.5890 - acc: 0.6465 - val_loss: 1.7272 - val_acc: 0.6768\n",
      "Epoch 17/100\n",
      "792/792 [==============================] - 0s 81us/step - loss: 1.5211 - acc: 0.6679 - val_loss: 1.6097 - val_acc: 0.7020\n",
      "Epoch 18/100\n",
      "792/792 [==============================] - 0s 78us/step - loss: 1.4033 - acc: 0.6970 - val_loss: 1.4810 - val_acc: 0.7273\n",
      "Epoch 19/100\n",
      "792/792 [==============================] - 0s 75us/step - loss: 1.3131 - acc: 0.7083 - val_loss: 1.3869 - val_acc: 0.7576\n",
      "Epoch 20/100\n",
      "792/792 [==============================] - 0s 75us/step - loss: 1.2190 - acc: 0.7551 - val_loss: 1.2911 - val_acc: 0.7828\n",
      "Epoch 21/100\n",
      "792/792 [==============================] - 0s 78us/step - loss: 1.1561 - acc: 0.7437 - val_loss: 1.2099 - val_acc: 0.7727\n",
      "Epoch 22/100\n",
      "792/792 [==============================] - 0s 78us/step - loss: 1.0808 - acc: 0.7652 - val_loss: 1.1400 - val_acc: 0.7879\n",
      "Epoch 23/100\n",
      "792/792 [==============================] - 0s 77us/step - loss: 0.9896 - acc: 0.8093 - val_loss: 1.0745 - val_acc: 0.7879\n",
      "Epoch 24/100\n",
      "792/792 [==============================] - 0s 71us/step - loss: 0.9173 - acc: 0.8093 - val_loss: 1.0138 - val_acc: 0.7980\n",
      "Epoch 25/100\n",
      "792/792 [==============================] - 0s 75us/step - loss: 0.8979 - acc: 0.8093 - val_loss: 0.9367 - val_acc: 0.8081\n",
      "Epoch 26/100\n",
      "792/792 [==============================] - 0s 76us/step - loss: 0.8383 - acc: 0.8270 - val_loss: 0.8744 - val_acc: 0.8384\n",
      "Epoch 27/100\n",
      "792/792 [==============================] - 0s 76us/step - loss: 0.7865 - acc: 0.8207 - val_loss: 0.8160 - val_acc: 0.8434\n",
      "Epoch 28/100\n",
      "792/792 [==============================] - 0s 78us/step - loss: 0.7472 - acc: 0.8460 - val_loss: 0.7832 - val_acc: 0.8434\n",
      "Epoch 29/100\n",
      "792/792 [==============================] - 0s 78us/step - loss: 0.6756 - acc: 0.8788 - val_loss: 0.7556 - val_acc: 0.8384\n",
      "Epoch 30/100\n",
      "792/792 [==============================] - 0s 75us/step - loss: 0.6426 - acc: 0.8687 - val_loss: 0.7193 - val_acc: 0.8687\n",
      "Epoch 31/100\n",
      "792/792 [==============================] - 0s 78us/step - loss: 0.5846 - acc: 0.8775 - val_loss: 0.6818 - val_acc: 0.8485\n",
      "Epoch 32/100\n",
      "792/792 [==============================] - 0s 76us/step - loss: 0.6073 - acc: 0.8687 - val_loss: 0.6369 - val_acc: 0.8737\n",
      "Epoch 33/100\n",
      "792/792 [==============================] - 0s 75us/step - loss: 0.5609 - acc: 0.8801 - val_loss: 0.6122 - val_acc: 0.8788\n",
      "Epoch 34/100\n",
      "792/792 [==============================] - 0s 76us/step - loss: 0.5218 - acc: 0.8838 - val_loss: 0.5760 - val_acc: 0.8737\n",
      "Epoch 35/100\n",
      "792/792 [==============================] - 0s 75us/step - loss: 0.5184 - acc: 0.8889 - val_loss: 0.5674 - val_acc: 0.8939\n",
      "Epoch 36/100\n",
      "792/792 [==============================] - 0s 73us/step - loss: 0.4562 - acc: 0.9078 - val_loss: 0.5570 - val_acc: 0.8889\n",
      "Epoch 37/100\n",
      "792/792 [==============================] - 0s 75us/step - loss: 0.4484 - acc: 0.9066 - val_loss: 0.5414 - val_acc: 0.8788\n",
      "Epoch 38/100\n",
      "792/792 [==============================] - 0s 81us/step - loss: 0.4144 - acc: 0.9242 - val_loss: 0.5163 - val_acc: 0.8838\n",
      "Epoch 39/100\n",
      "792/792 [==============================] - 0s 76us/step - loss: 0.3985 - acc: 0.9217 - val_loss: 0.4746 - val_acc: 0.8838\n",
      "Epoch 40/100\n",
      "792/792 [==============================] - 0s 76us/step - loss: 0.3731 - acc: 0.9217 - val_loss: 0.4319 - val_acc: 0.9141\n",
      "Epoch 41/100\n",
      "792/792 [==============================] - 0s 74us/step - loss: 0.3633 - acc: 0.9242 - val_loss: 0.4362 - val_acc: 0.9141\n",
      "Epoch 42/100\n",
      "792/792 [==============================] - 0s 75us/step - loss: 0.3390 - acc: 0.9217 - val_loss: 0.4282 - val_acc: 0.9091\n",
      "Epoch 43/100\n",
      "792/792 [==============================] - 0s 74us/step - loss: 0.3116 - acc: 0.9407 - val_loss: 0.4348 - val_acc: 0.8939\n",
      "Epoch 44/100\n",
      "792/792 [==============================] - 0s 76us/step - loss: 0.2974 - acc: 0.9444 - val_loss: 0.4138 - val_acc: 0.8990\n",
      "Epoch 45/100\n",
      "792/792 [==============================] - 0s 78us/step - loss: 0.2942 - acc: 0.9457 - val_loss: 0.4059 - val_acc: 0.9040\n",
      "Epoch 46/100\n",
      "792/792 [==============================] - 0s 77us/step - loss: 0.2859 - acc: 0.9369 - val_loss: 0.3760 - val_acc: 0.9141\n",
      "Epoch 47/100\n",
      "792/792 [==============================] - 0s 76us/step - loss: 0.2578 - acc: 0.9444 - val_loss: 0.3649 - val_acc: 0.9192\n",
      "Epoch 48/100\n",
      "792/792 [==============================] - 0s 73us/step - loss: 0.2489 - acc: 0.9583 - val_loss: 0.3694 - val_acc: 0.9040\n",
      "Epoch 49/100\n",
      "792/792 [==============================] - 0s 81us/step - loss: 0.2542 - acc: 0.9470 - val_loss: 0.3505 - val_acc: 0.9141\n",
      "Epoch 50/100\n",
      "792/792 [==============================] - 0s 80us/step - loss: 0.2400 - acc: 0.9508 - val_loss: 0.3440 - val_acc: 0.9141\n",
      "Epoch 51/100\n",
      "792/792 [==============================] - 0s 78us/step - loss: 0.2297 - acc: 0.9545 - val_loss: 0.3457 - val_acc: 0.8889\n",
      "Epoch 52/100\n",
      "792/792 [==============================] - 0s 74us/step - loss: 0.2338 - acc: 0.9495 - val_loss: 0.3193 - val_acc: 0.9192\n",
      "Epoch 53/100\n",
      "792/792 [==============================] - 0s 77us/step - loss: 0.2018 - acc: 0.9609 - val_loss: 0.3336 - val_acc: 0.9141\n",
      "Epoch 54/100\n",
      "792/792 [==============================] - 0s 78us/step - loss: 0.2032 - acc: 0.9545 - val_loss: 0.3139 - val_acc: 0.9242\n",
      "Epoch 55/100\n",
      "792/792 [==============================] - 0s 75us/step - loss: 0.2067 - acc: 0.9571 - val_loss: 0.3131 - val_acc: 0.9192\n",
      "Epoch 56/100\n",
      "792/792 [==============================] - 0s 75us/step - loss: 0.1701 - acc: 0.9722 - val_loss: 0.2839 - val_acc: 0.9293\n",
      "Epoch 57/100\n",
      "792/792 [==============================] - 0s 78us/step - loss: 0.1710 - acc: 0.9697 - val_loss: 0.2777 - val_acc: 0.9242\n",
      "Epoch 58/100\n",
      "792/792 [==============================] - 0s 74us/step - loss: 0.1750 - acc: 0.9760 - val_loss: 0.2813 - val_acc: 0.9343\n",
      "Epoch 59/100\n",
      "792/792 [==============================] - 0s 75us/step - loss: 0.1535 - acc: 0.9747 - val_loss: 0.2575 - val_acc: 0.9293\n",
      "Epoch 60/100\n",
      "792/792 [==============================] - 0s 78us/step - loss: 0.1542 - acc: 0.9773 - val_loss: 0.2661 - val_acc: 0.9343\n",
      "Epoch 61/100\n",
      "792/792 [==============================] - 0s 78us/step - loss: 0.1507 - acc: 0.9747 - val_loss: 0.2764 - val_acc: 0.9242\n",
      "Epoch 62/100\n",
      "792/792 [==============================] - 0s 78us/step - loss: 0.1506 - acc: 0.9785 - val_loss: 0.2617 - val_acc: 0.9242\n",
      "Epoch 63/100\n",
      "792/792 [==============================] - 0s 75us/step - loss: 0.1395 - acc: 0.9747 - val_loss: 0.2665 - val_acc: 0.9293\n",
      "Epoch 64/100\n",
      "792/792 [==============================] - 0s 73us/step - loss: 0.1313 - acc: 0.9760 - val_loss: 0.2539 - val_acc: 0.9394\n",
      "Epoch 65/100\n",
      "792/792 [==============================] - 0s 82us/step - loss: 0.1217 - acc: 0.9798 - val_loss: 0.2587 - val_acc: 0.9192\n",
      "Epoch 66/100\n",
      "792/792 [==============================] - 0s 81us/step - loss: 0.1250 - acc: 0.9798 - val_loss: 0.2377 - val_acc: 0.9293\n",
      "Epoch 67/100\n",
      "792/792 [==============================] - 0s 77us/step - loss: 0.1277 - acc: 0.9722 - val_loss: 0.2482 - val_acc: 0.9192\n",
      "Epoch 68/100\n",
      "792/792 [==============================] - 0s 76us/step - loss: 0.1127 - acc: 0.9811 - val_loss: 0.2589 - val_acc: 0.9242\n",
      "Epoch 69/100\n",
      "792/792 [==============================] - 0s 77us/step - loss: 0.1152 - acc: 0.9848 - val_loss: 0.2508 - val_acc: 0.9293\n",
      "Epoch 70/100\n",
      "792/792 [==============================] - 0s 79us/step - loss: 0.1001 - acc: 0.9874 - val_loss: 0.2283 - val_acc: 0.9293\n",
      "Epoch 71/100\n",
      "792/792 [==============================] - 0s 79us/step - loss: 0.1157 - acc: 0.9722 - val_loss: 0.2481 - val_acc: 0.9242\n",
      "Epoch 72/100\n",
      "792/792 [==============================] - 0s 83us/step - loss: 0.0888 - acc: 0.9924 - val_loss: 0.2319 - val_acc: 0.9293\n",
      "Epoch 73/100\n",
      "792/792 [==============================] - 0s 75us/step - loss: 0.0959 - acc: 0.9848 - val_loss: 0.2458 - val_acc: 0.9242\n",
      "Epoch 74/100\n",
      "792/792 [==============================] - 0s 76us/step - loss: 0.0896 - acc: 0.9861 - val_loss: 0.2209 - val_acc: 0.9293\n",
      "Epoch 75/100\n",
      "792/792 [==============================] - 0s 75us/step - loss: 0.0995 - acc: 0.9785 - val_loss: 0.2332 - val_acc: 0.9242\n",
      "Epoch 76/100\n",
      "792/792 [==============================] - 0s 73us/step - loss: 0.0749 - acc: 0.9899 - val_loss: 0.2430 - val_acc: 0.9192\n",
      "Epoch 77/100\n",
      "792/792 [==============================] - 0s 76us/step - loss: 0.0965 - acc: 0.9836 - val_loss: 0.2490 - val_acc: 0.9242\n",
      "Epoch 78/100\n",
      "792/792 [==============================] - 0s 76us/step - loss: 0.0896 - acc: 0.9836 - val_loss: 0.2427 - val_acc: 0.9242\n",
      "Epoch 79/100\n",
      "792/792 [==============================] - 0s 76us/step - loss: 0.0851 - acc: 0.9848 - val_loss: 0.2419 - val_acc: 0.9444\n",
      "Epoch 80/100\n",
      "792/792 [==============================] - 0s 74us/step - loss: 0.0714 - acc: 0.9886 - val_loss: 0.2245 - val_acc: 0.9394\n",
      "Epoch 81/100\n",
      "792/792 [==============================] - 0s 79us/step - loss: 0.0787 - acc: 0.9886 - val_loss: 0.2202 - val_acc: 0.9343\n",
      "Epoch 82/100\n",
      "792/792 [==============================] - 0s 82us/step - loss: 0.0721 - acc: 0.9899 - val_loss: 0.2297 - val_acc: 0.9293\n",
      "Epoch 83/100\n",
      "792/792 [==============================] - 0s 83us/step - loss: 0.0678 - acc: 0.9886 - val_loss: 0.2206 - val_acc: 0.9343\n",
      "Epoch 84/100\n",
      "792/792 [==============================] - 0s 79us/step - loss: 0.0717 - acc: 0.9874 - val_loss: 0.2314 - val_acc: 0.9293\n",
      "Epoch 85/100\n",
      "792/792 [==============================] - 0s 76us/step - loss: 0.0519 - acc: 0.9949 - val_loss: 0.2164 - val_acc: 0.9293\n",
      "Epoch 86/100\n",
      "792/792 [==============================] - 0s 73us/step - loss: 0.0629 - acc: 0.9899 - val_loss: 0.2225 - val_acc: 0.9293\n",
      "Epoch 87/100\n",
      "792/792 [==============================] - 0s 75us/step - loss: 0.0629 - acc: 0.9886 - val_loss: 0.2325 - val_acc: 0.9293\n",
      "Epoch 88/100\n",
      "792/792 [==============================] - 0s 75us/step - loss: 0.0559 - acc: 0.9949 - val_loss: 0.2144 - val_acc: 0.9293\n",
      "Epoch 89/100\n",
      "792/792 [==============================] - 0s 77us/step - loss: 0.0661 - acc: 0.9886 - val_loss: 0.2185 - val_acc: 0.9394\n",
      "Epoch 90/100\n",
      "792/792 [==============================] - 0s 79us/step - loss: 0.0589 - acc: 0.9912 - val_loss: 0.2196 - val_acc: 0.9343\n",
      "Epoch 91/100\n",
      "792/792 [==============================] - 0s 75us/step - loss: 0.0500 - acc: 0.9937 - val_loss: 0.2104 - val_acc: 0.9444\n",
      "Epoch 92/100\n",
      "792/792 [==============================] - 0s 74us/step - loss: 0.0596 - acc: 0.9886 - val_loss: 0.2270 - val_acc: 0.9293\n",
      "Epoch 93/100\n",
      "792/792 [==============================] - 0s 80us/step - loss: 0.0565 - acc: 0.9886 - val_loss: 0.2239 - val_acc: 0.9293\n",
      "Epoch 94/100\n",
      "792/792 [==============================] - 0s 76us/step - loss: 0.0509 - acc: 0.9949 - val_loss: 0.2354 - val_acc: 0.9293\n",
      "Epoch 95/100\n",
      "792/792 [==============================] - 0s 79us/step - loss: 0.0377 - acc: 0.9975 - val_loss: 0.2032 - val_acc: 0.9394\n",
      "Epoch 96/100\n",
      "792/792 [==============================] - 0s 77us/step - loss: 0.0462 - acc: 0.9937 - val_loss: 0.2175 - val_acc: 0.9293\n",
      "Epoch 97/100\n",
      "792/792 [==============================] - 0s 76us/step - loss: 0.0470 - acc: 0.9937 - val_loss: 0.2391 - val_acc: 0.9242\n",
      "Epoch 98/100\n",
      "792/792 [==============================] - 0s 84us/step - loss: 0.0512 - acc: 0.9924 - val_loss: 0.2174 - val_acc: 0.9242\n",
      "Epoch 99/100\n",
      "792/792 [==============================] - 0s 77us/step - loss: 0.0389 - acc: 0.9949 - val_loss: 0.2151 - val_acc: 0.9293\n",
      "Epoch 100/100\n",
      "792/792 [==============================] - 0s 73us/step - loss: 0.0426 - acc: 0.9962 - val_loss: 0.2275 - val_acc: 0.9293\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x1e5f1c3acf8>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.fit(raw_data,label_cat,batch_size=150,epochs=100,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to fight with over fitting by changing the framework and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "###let try notmalize the data\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_nor=preprocessing.MinMaxScaler().fit_transform(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.08888282, 0.11428711, 0.1500032 , ..., 0.01298739, 0.        ,\n",
       "        0.17931497],\n",
       "       [0.06666212, 0.        , 0.2       , ..., 0.00259854, 0.44943277,\n",
       "        0.15862288],\n",
       "       [0.06666212, 0.04762044, 0.1249984 , ..., 0.        , 0.23595738,\n",
       "        0.02069209],\n",
       "       ...,\n",
       "       [0.02222071, 0.01904623, 0.        , ..., 0.07272727, 0.        ,\n",
       "        0.01379237],\n",
       "       [0.        , 0.        , 0.3       , ..., 0.        , 0.02247049,\n",
       "        0.02069209],\n",
       "       [0.26667122, 0.09523601, 0.2       , ..., 0.06233842, 0.29213936,\n",
       "        0.15862288]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data_nor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "###seems not that bad, but very likely overfit a little bit\n",
    "model5 = Sequential()\n",
    "model5.add(Dense(360,input_dim=192,activation='relu'))\n",
    "model5.add(Dropout(0.5))   ##change to 0.5 make it better\n",
    "model5.add(Dense(180,activation='relu'))\n",
    "model5.add(Dense(CN, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5.compile(loss='categorical_crossentropy',optimizer='rmsprop', metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 792 samples, validate on 198 samples\n",
      "Epoch 1/100\n",
      "792/792 [==============================] - 0s 544us/step - loss: 4.5490 - acc: 0.0177 - val_loss: 4.4691 - val_acc: 0.0404\n",
      "Epoch 2/100\n",
      "792/792 [==============================] - 0s 64us/step - loss: 4.3303 - acc: 0.0783 - val_loss: 4.3059 - val_acc: 0.0404\n",
      "Epoch 3/100\n",
      "792/792 [==============================] - 0s 70us/step - loss: 4.1025 - acc: 0.1199 - val_loss: 4.0707 - val_acc: 0.1162\n",
      "Epoch 4/100\n",
      "792/792 [==============================] - 0s 67us/step - loss: 3.8194 - acc: 0.2184 - val_loss: 3.8315 - val_acc: 0.1263\n",
      "Epoch 5/100\n",
      "792/792 [==============================] - 0s 67us/step - loss: 3.5571 - acc: 0.2412 - val_loss: 3.5635 - val_acc: 0.2525\n",
      "Epoch 6/100\n",
      "792/792 [==============================] - 0s 64us/step - loss: 3.2718 - acc: 0.3333 - val_loss: 3.3103 - val_acc: 0.2778\n",
      "Epoch 7/100\n",
      "792/792 [==============================] - 0s 60us/step - loss: 2.9356 - acc: 0.4040 - val_loss: 2.9743 - val_acc: 0.3889\n",
      "Epoch 8/100\n",
      "792/792 [==============================] - 0s 63us/step - loss: 2.6739 - acc: 0.4571 - val_loss: 2.6647 - val_acc: 0.4949\n",
      "Epoch 9/100\n",
      "792/792 [==============================] - 0s 61us/step - loss: 2.4642 - acc: 0.5139 - val_loss: 2.4345 - val_acc: 0.4798\n",
      "Epoch 10/100\n",
      "792/792 [==============================] - 0s 64us/step - loss: 2.1844 - acc: 0.5606 - val_loss: 2.2263 - val_acc: 0.5152\n",
      "Epoch 11/100\n",
      "792/792 [==============================] - 0s 60us/step - loss: 2.0248 - acc: 0.6061 - val_loss: 1.9967 - val_acc: 0.6313\n",
      "Epoch 12/100\n",
      "792/792 [==============================] - 0s 79us/step - loss: 1.7912 - acc: 0.6604 - val_loss: 1.7797 - val_acc: 0.6768\n",
      "Epoch 13/100\n",
      "792/792 [==============================] - 0s 65us/step - loss: 1.6431 - acc: 0.6705 - val_loss: 1.6368 - val_acc: 0.6465\n",
      "Epoch 14/100\n",
      "792/792 [==============================] - 0s 69us/step - loss: 1.4842 - acc: 0.7184 - val_loss: 1.4703 - val_acc: 0.7525\n",
      "Epoch 15/100\n",
      "792/792 [==============================] - 0s 61us/step - loss: 1.3574 - acc: 0.7462 - val_loss: 1.3215 - val_acc: 0.7525\n",
      "Epoch 16/100\n",
      "792/792 [==============================] - 0s 63us/step - loss: 1.2566 - acc: 0.7576 - val_loss: 1.2036 - val_acc: 0.7475\n",
      "Epoch 17/100\n",
      "792/792 [==============================] - 0s 61us/step - loss: 1.1189 - acc: 0.7803 - val_loss: 1.1232 - val_acc: 0.7576\n",
      "Epoch 18/100\n",
      "792/792 [==============================] - 0s 60us/step - loss: 1.0423 - acc: 0.7828 - val_loss: 0.9549 - val_acc: 0.8283\n",
      "Epoch 19/100\n",
      "792/792 [==============================] - 0s 59us/step - loss: 0.9625 - acc: 0.8169 - val_loss: 0.8580 - val_acc: 0.8737\n",
      "Epoch 20/100\n",
      "792/792 [==============================] - 0s 66us/step - loss: 0.8599 - acc: 0.8497 - val_loss: 0.9146 - val_acc: 0.8131\n",
      "Epoch 21/100\n",
      "792/792 [==============================] - 0s 70us/step - loss: 0.8018 - acc: 0.8586 - val_loss: 0.7231 - val_acc: 0.9242\n",
      "Epoch 22/100\n",
      "792/792 [==============================] - 0s 63us/step - loss: 0.7288 - acc: 0.8636 - val_loss: 0.6939 - val_acc: 0.8636\n",
      "Epoch 23/100\n",
      "792/792 [==============================] - 0s 61us/step - loss: 0.6777 - acc: 0.8712 - val_loss: 0.6737 - val_acc: 0.8586\n",
      "Epoch 24/100\n",
      "792/792 [==============================] - 0s 63us/step - loss: 0.6130 - acc: 0.8977 - val_loss: 0.6137 - val_acc: 0.8788\n",
      "Epoch 25/100\n",
      "792/792 [==============================] - 0s 62us/step - loss: 0.5789 - acc: 0.8876 - val_loss: 0.5577 - val_acc: 0.8939\n",
      "Epoch 26/100\n",
      "792/792 [==============================] - 0s 59us/step - loss: 0.5488 - acc: 0.8952 - val_loss: 0.5385 - val_acc: 0.9141\n",
      "Epoch 27/100\n",
      "792/792 [==============================] - 0s 63us/step - loss: 0.4854 - acc: 0.9230 - val_loss: 0.4310 - val_acc: 0.9040\n",
      "Epoch 28/100\n",
      "792/792 [==============================] - 0s 59us/step - loss: 0.4284 - acc: 0.9230 - val_loss: 0.4214 - val_acc: 0.9141\n",
      "Epoch 29/100\n",
      "792/792 [==============================] - 0s 59us/step - loss: 0.4388 - acc: 0.9104 - val_loss: 0.3682 - val_acc: 0.9545\n",
      "Epoch 30/100\n",
      "792/792 [==============================] - 0s 63us/step - loss: 0.4099 - acc: 0.9091 - val_loss: 0.3717 - val_acc: 0.9394\n",
      "Epoch 31/100\n",
      "792/792 [==============================] - 0s 61us/step - loss: 0.3765 - acc: 0.9318 - val_loss: 0.3475 - val_acc: 0.9394\n",
      "Epoch 32/100\n",
      "792/792 [==============================] - 0s 61us/step - loss: 0.3408 - acc: 0.9407 - val_loss: 0.3393 - val_acc: 0.9242\n",
      "Epoch 33/100\n",
      "792/792 [==============================] - 0s 63us/step - loss: 0.3123 - acc: 0.9432 - val_loss: 0.2877 - val_acc: 0.9444\n",
      "Epoch 34/100\n",
      "792/792 [==============================] - 0s 69us/step - loss: 0.3183 - acc: 0.9331 - val_loss: 0.2743 - val_acc: 0.9444\n",
      "Epoch 35/100\n",
      "792/792 [==============================] - 0s 59us/step - loss: 0.2912 - acc: 0.9407 - val_loss: 0.2532 - val_acc: 0.9697\n",
      "Epoch 36/100\n",
      "792/792 [==============================] - 0s 61us/step - loss: 0.2820 - acc: 0.9495 - val_loss: 0.2466 - val_acc: 0.9444\n",
      "Epoch 37/100\n",
      "792/792 [==============================] - 0s 59us/step - loss: 0.2440 - acc: 0.9583 - val_loss: 0.2406 - val_acc: 0.9545\n",
      "Epoch 38/100\n",
      "792/792 [==============================] - 0s 58us/step - loss: 0.2336 - acc: 0.9508 - val_loss: 0.1942 - val_acc: 0.9949\n",
      "Epoch 39/100\n",
      "792/792 [==============================] - 0s 59us/step - loss: 0.2238 - acc: 0.9596 - val_loss: 0.1957 - val_acc: 0.9545\n",
      "Epoch 40/100\n",
      "792/792 [==============================] - 0s 59us/step - loss: 0.2066 - acc: 0.9596 - val_loss: 0.1878 - val_acc: 0.9596\n",
      "Epoch 41/100\n",
      "792/792 [==============================] - 0s 66us/step - loss: 0.1915 - acc: 0.9646 - val_loss: 0.1572 - val_acc: 0.9747\n",
      "Epoch 42/100\n",
      "792/792 [==============================] - 0s 59us/step - loss: 0.1876 - acc: 0.9659 - val_loss: 0.2065 - val_acc: 0.9545\n",
      "Epoch 43/100\n",
      "792/792 [==============================] - 0s 61us/step - loss: 0.1822 - acc: 0.9621 - val_loss: 0.1539 - val_acc: 0.9697\n",
      "Epoch 44/100\n",
      "792/792 [==============================] - 0s 63us/step - loss: 0.1545 - acc: 0.9659 - val_loss: 0.1943 - val_acc: 0.9545\n",
      "Epoch 45/100\n",
      "792/792 [==============================] - 0s 60us/step - loss: 0.1701 - acc: 0.9722 - val_loss: 0.1335 - val_acc: 0.9798\n",
      "Epoch 46/100\n",
      "792/792 [==============================] - 0s 66us/step - loss: 0.1761 - acc: 0.9596 - val_loss: 0.1193 - val_acc: 0.9949\n",
      "Epoch 47/100\n",
      "792/792 [==============================] - 0s 61us/step - loss: 0.1461 - acc: 0.9747 - val_loss: 0.1571 - val_acc: 0.9646\n",
      "Epoch 48/100\n",
      "792/792 [==============================] - 0s 54us/step - loss: 0.1322 - acc: 0.9760 - val_loss: 0.1021 - val_acc: 0.9899\n",
      "Epoch 49/100\n",
      "792/792 [==============================] - 0s 58us/step - loss: 0.1305 - acc: 0.9684 - val_loss: 0.1173 - val_acc: 0.9747\n",
      "Epoch 50/100\n",
      "792/792 [==============================] - 0s 57us/step - loss: 0.1250 - acc: 0.9773 - val_loss: 0.1086 - val_acc: 0.9949\n",
      "Epoch 51/100\n",
      "792/792 [==============================] - 0s 58us/step - loss: 0.1191 - acc: 0.9785 - val_loss: 0.1101 - val_acc: 0.9949\n",
      "Epoch 52/100\n",
      "792/792 [==============================] - 0s 64us/step - loss: 0.1132 - acc: 0.9747 - val_loss: 0.1187 - val_acc: 0.9697\n",
      "Epoch 53/100\n",
      "792/792 [==============================] - 0s 59us/step - loss: 0.0949 - acc: 0.9811 - val_loss: 0.1242 - val_acc: 0.9747\n",
      "Epoch 54/100\n",
      "792/792 [==============================] - 0s 72us/step - loss: 0.1046 - acc: 0.9773 - val_loss: 0.0933 - val_acc: 0.9848\n",
      "Epoch 55/100\n",
      "792/792 [==============================] - 0s 62us/step - loss: 0.0984 - acc: 0.9785 - val_loss: 0.0855 - val_acc: 1.0000\n",
      "Epoch 56/100\n",
      "792/792 [==============================] - 0s 59us/step - loss: 0.0942 - acc: 0.9836 - val_loss: 0.0791 - val_acc: 0.9899\n",
      "Epoch 57/100\n",
      "792/792 [==============================] - 0s 58us/step - loss: 0.0842 - acc: 0.9886 - val_loss: 0.0778 - val_acc: 0.9848\n",
      "Epoch 58/100\n",
      "792/792 [==============================] - 0s 56us/step - loss: 0.0928 - acc: 0.9785 - val_loss: 0.1125 - val_acc: 0.9747\n",
      "Epoch 59/100\n",
      "792/792 [==============================] - 0s 56us/step - loss: 0.0830 - acc: 0.9836 - val_loss: 0.0771 - val_acc: 0.9899\n",
      "Epoch 60/100\n",
      "792/792 [==============================] - 0s 54us/step - loss: 0.0616 - acc: 0.9899 - val_loss: 0.0708 - val_acc: 0.9949\n",
      "Epoch 61/100\n",
      "792/792 [==============================] - 0s 54us/step - loss: 0.0745 - acc: 0.9836 - val_loss: 0.1006 - val_acc: 0.9697\n",
      "Epoch 62/100\n",
      "792/792 [==============================] - 0s 55us/step - loss: 0.0666 - acc: 0.9949 - val_loss: 0.0713 - val_acc: 0.9848\n",
      "Epoch 63/100\n",
      "792/792 [==============================] - 0s 53us/step - loss: 0.0702 - acc: 0.9886 - val_loss: 0.0686 - val_acc: 0.9949\n",
      "Epoch 64/100\n",
      "792/792 [==============================] - 0s 55us/step - loss: 0.0697 - acc: 0.9861 - val_loss: 0.0609 - val_acc: 0.9899\n",
      "Epoch 65/100\n",
      "792/792 [==============================] - 0s 54us/step - loss: 0.0575 - acc: 0.9912 - val_loss: 0.0650 - val_acc: 0.9899\n",
      "Epoch 66/100\n",
      "792/792 [==============================] - 0s 54us/step - loss: 0.0706 - acc: 0.9811 - val_loss: 0.0841 - val_acc: 0.9798\n",
      "Epoch 67/100\n",
      "792/792 [==============================] - 0s 55us/step - loss: 0.0654 - acc: 0.9848 - val_loss: 0.0946 - val_acc: 0.9747\n",
      "Epoch 68/100\n",
      "792/792 [==============================] - 0s 55us/step - loss: 0.0644 - acc: 0.9924 - val_loss: 0.0607 - val_acc: 0.9949\n",
      "Epoch 69/100\n",
      "792/792 [==============================] - 0s 53us/step - loss: 0.0598 - acc: 0.9912 - val_loss: 0.0498 - val_acc: 0.9899\n",
      "Epoch 70/100\n",
      "792/792 [==============================] - 0s 56us/step - loss: 0.0665 - acc: 0.9836 - val_loss: 0.0618 - val_acc: 0.9848\n",
      "Epoch 71/100\n",
      "792/792 [==============================] - 0s 56us/step - loss: 0.0475 - acc: 0.9899 - val_loss: 0.0433 - val_acc: 0.9949\n",
      "Epoch 72/100\n",
      "792/792 [==============================] - 0s 54us/step - loss: 0.0504 - acc: 0.9912 - val_loss: 0.0600 - val_acc: 0.9899\n",
      "Epoch 73/100\n",
      "792/792 [==============================] - 0s 57us/step - loss: 0.0459 - acc: 0.9949 - val_loss: 0.0482 - val_acc: 0.9949\n",
      "Epoch 74/100\n",
      "792/792 [==============================] - 0s 58us/step - loss: 0.0513 - acc: 0.9912 - val_loss: 0.0426 - val_acc: 1.0000\n",
      "Epoch 75/100\n",
      "792/792 [==============================] - 0s 55us/step - loss: 0.0461 - acc: 0.9899 - val_loss: 0.0670 - val_acc: 0.9899\n",
      "Epoch 76/100\n",
      "792/792 [==============================] - 0s 61us/step - loss: 0.0396 - acc: 0.9962 - val_loss: 0.0524 - val_acc: 0.9949\n",
      "Epoch 77/100\n",
      "792/792 [==============================] - 0s 59us/step - loss: 0.0460 - acc: 0.9924 - val_loss: 0.0585 - val_acc: 0.9949\n",
      "Epoch 78/100\n",
      "792/792 [==============================] - 0s 54us/step - loss: 0.0399 - acc: 0.9924 - val_loss: 0.0452 - val_acc: 0.9949\n",
      "Epoch 79/100\n",
      "792/792 [==============================] - 0s 54us/step - loss: 0.0424 - acc: 0.9899 - val_loss: 0.0474 - val_acc: 0.9899\n",
      "Epoch 80/100\n",
      "792/792 [==============================] - 0s 53us/step - loss: 0.0463 - acc: 0.9886 - val_loss: 0.0513 - val_acc: 0.9899\n",
      "Epoch 81/100\n",
      "792/792 [==============================] - 0s 54us/step - loss: 0.0452 - acc: 0.9886 - val_loss: 0.0397 - val_acc: 1.0000\n",
      "Epoch 82/100\n",
      "792/792 [==============================] - 0s 54us/step - loss: 0.0313 - acc: 0.9962 - val_loss: 0.0352 - val_acc: 0.9899\n",
      "Epoch 83/100\n",
      "792/792 [==============================] - 0s 53us/step - loss: 0.0303 - acc: 0.9975 - val_loss: 0.0401 - val_acc: 0.9899\n",
      "Epoch 84/100\n",
      "792/792 [==============================] - 0s 55us/step - loss: 0.0422 - acc: 0.9874 - val_loss: 0.0508 - val_acc: 0.9899\n",
      "Epoch 85/100\n",
      "792/792 [==============================] - 0s 55us/step - loss: 0.0222 - acc: 0.9987 - val_loss: 0.0341 - val_acc: 0.9899\n",
      "Epoch 86/100\n",
      "792/792 [==============================] - 0s 54us/step - loss: 0.0229 - acc: 0.9975 - val_loss: 0.0323 - val_acc: 1.0000\n",
      "Epoch 87/100\n",
      "792/792 [==============================] - 0s 56us/step - loss: 0.0278 - acc: 0.9962 - val_loss: 0.0471 - val_acc: 0.9899\n",
      "Epoch 88/100\n",
      "792/792 [==============================] - 0s 54us/step - loss: 0.0288 - acc: 0.9937 - val_loss: 0.0263 - val_acc: 0.9949\n",
      "Epoch 89/100\n",
      "792/792 [==============================] - 0s 54us/step - loss: 0.0288 - acc: 0.9949 - val_loss: 0.0248 - val_acc: 1.0000\n",
      "Epoch 90/100\n",
      "792/792 [==============================] - 0s 54us/step - loss: 0.0274 - acc: 0.9949 - val_loss: 0.0458 - val_acc: 0.9949\n",
      "Epoch 91/100\n",
      "792/792 [==============================] - 0s 53us/step - loss: 0.0351 - acc: 0.9937 - val_loss: 0.0305 - val_acc: 1.0000\n",
      "Epoch 92/100\n",
      "792/792 [==============================] - 0s 55us/step - loss: 0.0255 - acc: 0.9975 - val_loss: 0.0265 - val_acc: 1.0000\n",
      "Epoch 93/100\n",
      "792/792 [==============================] - 0s 54us/step - loss: 0.0236 - acc: 0.9962 - val_loss: 0.0256 - val_acc: 0.9949\n",
      "Epoch 94/100\n",
      "792/792 [==============================] - 0s 56us/step - loss: 0.0211 - acc: 0.9962 - val_loss: 0.0289 - val_acc: 1.0000\n",
      "Epoch 95/100\n",
      "792/792 [==============================] - 0s 56us/step - loss: 0.0185 - acc: 0.9975 - val_loss: 0.0550 - val_acc: 0.9747\n",
      "Epoch 96/100\n",
      "792/792 [==============================] - 0s 57us/step - loss: 0.0241 - acc: 0.9962 - val_loss: 0.0406 - val_acc: 0.9899\n",
      "Epoch 97/100\n",
      "792/792 [==============================] - 0s 54us/step - loss: 0.0249 - acc: 0.9937 - val_loss: 0.0510 - val_acc: 0.9899\n",
      "Epoch 98/100\n",
      "792/792 [==============================] - 0s 54us/step - loss: 0.0238 - acc: 0.9949 - val_loss: 0.0281 - val_acc: 0.9949\n",
      "Epoch 99/100\n",
      "792/792 [==============================] - 0s 57us/step - loss: 0.0194 - acc: 0.9962 - val_loss: 0.0489 - val_acc: 0.9848\n",
      "Epoch 100/100\n",
      "792/792 [==============================] - 0s 56us/step - loss: 0.0184 - acc: 0.9987 - val_loss: 0.0511 - val_acc: 0.9798\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x1e675154518>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model5.fit(raw_data_nor,label_cat,batch_size=150,epochs=100,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 360)               69480     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 360)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 180)               64980     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 99)                17919     \n",
      "=================================================================\n",
      "Total params: 152,379\n",
      "Trainable params: 152,379\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "990/990 [==============================] - 0s 48us/step - loss: 0.0396 - acc: 0.9919\n",
      "Epoch 2/100\n",
      "990/990 [==============================] - 0s 54us/step - loss: 0.0320 - acc: 0.9909\n",
      "Epoch 3/100\n",
      "990/990 [==============================] - 0s 50us/step - loss: 0.0237 - acc: 0.9960\n",
      "Epoch 4/100\n",
      "990/990 [==============================] - 0s 51us/step - loss: 0.0225 - acc: 0.9990\n",
      "Epoch 5/100\n",
      "990/990 [==============================] - 0s 51us/step - loss: 0.0261 - acc: 0.9949\n",
      "Epoch 6/100\n",
      "990/990 [==============================] - 0s 54us/step - loss: 0.0205 - acc: 0.9960\n",
      "Epoch 7/100\n",
      "990/990 [==============================] - 0s 53us/step - loss: 0.0211 - acc: 0.9929\n",
      "Epoch 8/100\n",
      "990/990 [==============================] - 0s 51us/step - loss: 0.0207 - acc: 0.9980\n",
      "Epoch 9/100\n",
      "990/990 [==============================] - 0s 53us/step - loss: 0.0210 - acc: 0.9970\n",
      "Epoch 10/100\n",
      "990/990 [==============================] - 0s 51us/step - loss: 0.0306 - acc: 0.9919\n",
      "Epoch 11/100\n",
      "990/990 [==============================] - 0s 51us/step - loss: 0.0284 - acc: 0.9929\n",
      "Epoch 12/100\n",
      "990/990 [==============================] - 0s 51us/step - loss: 0.0166 - acc: 0.9990\n",
      "Epoch 13/100\n",
      "990/990 [==============================] - 0s 52us/step - loss: 0.0209 - acc: 0.9929\n",
      "Epoch 14/100\n",
      "990/990 [==============================] - 0s 49us/step - loss: 0.0149 - acc: 1.0000\n",
      "Epoch 15/100\n",
      "990/990 [==============================] - 0s 52us/step - loss: 0.0174 - acc: 0.9980\n",
      "Epoch 16/100\n",
      "990/990 [==============================] - 0s 56us/step - loss: 0.0230 - acc: 0.9960\n",
      "Epoch 17/100\n",
      "990/990 [==============================] - 0s 51us/step - loss: 0.0182 - acc: 0.9939\n",
      "Epoch 18/100\n",
      "990/990 [==============================] - 0s 55us/step - loss: 0.0153 - acc: 0.9980\n",
      "Epoch 19/100\n",
      "990/990 [==============================] - 0s 56us/step - loss: 0.0144 - acc: 0.9990\n",
      "Epoch 20/100\n",
      "990/990 [==============================] - 0s 55us/step - loss: 0.0188 - acc: 0.9960\n",
      "Epoch 21/100\n",
      "990/990 [==============================] - 0s 49us/step - loss: 0.0110 - acc: 0.9990\n",
      "Epoch 22/100\n",
      "990/990 [==============================] - 0s 50us/step - loss: 0.0226 - acc: 0.9980\n",
      "Epoch 23/100\n",
      "990/990 [==============================] - 0s 51us/step - loss: 0.0122 - acc: 0.9980\n",
      "Epoch 24/100\n",
      "990/990 [==============================] - 0s 53us/step - loss: 0.0171 - acc: 0.9960\n",
      "Epoch 25/100\n",
      "990/990 [==============================] - 0s 52us/step - loss: 0.0128 - acc: 0.9990\n",
      "Epoch 26/100\n",
      "990/990 [==============================] - 0s 53us/step - loss: 0.0193 - acc: 0.9939\n",
      "Epoch 27/100\n",
      "990/990 [==============================] - 0s 53us/step - loss: 0.0171 - acc: 0.9960\n",
      "Epoch 28/100\n",
      "990/990 [==============================] - 0s 51us/step - loss: 0.0121 - acc: 0.9980\n",
      "Epoch 29/100\n",
      "990/990 [==============================] - 0s 52us/step - loss: 0.0126 - acc: 0.9980\n",
      "Epoch 30/100\n",
      "990/990 [==============================] - 0s 51us/step - loss: 0.0140 - acc: 0.9980\n",
      "Epoch 31/100\n",
      "990/990 [==============================] - 0s 51us/step - loss: 0.0100 - acc: 1.0000\n",
      "Epoch 32/100\n",
      "990/990 [==============================] - 0s 52us/step - loss: 0.0115 - acc: 0.9990\n",
      "Epoch 33/100\n",
      "990/990 [==============================] - 0s 63us/step - loss: 0.0098 - acc: 1.0000\n",
      "Epoch 34/100\n",
      "990/990 [==============================] - 0s 49us/step - loss: 0.0083 - acc: 0.9990\n",
      "Epoch 35/100\n",
      "990/990 [==============================] - 0s 51us/step - loss: 0.0108 - acc: 0.9970\n",
      "Epoch 36/100\n",
      "990/990 [==============================] - 0s 50us/step - loss: 0.0189 - acc: 0.9970\n",
      "Epoch 37/100\n",
      "990/990 [==============================] - 0s 56us/step - loss: 0.0094 - acc: 0.9980\n",
      "Epoch 38/100\n",
      "990/990 [==============================] - 0s 53us/step - loss: 0.0107 - acc: 0.9970\n",
      "Epoch 39/100\n",
      "990/990 [==============================] - 0s 48us/step - loss: 0.0097 - acc: 0.9980\n",
      "Epoch 40/100\n",
      "990/990 [==============================] - 0s 50us/step - loss: 0.0113 - acc: 0.9970\n",
      "Epoch 41/100\n",
      "990/990 [==============================] - 0s 48us/step - loss: 0.0144 - acc: 0.9949\n",
      "Epoch 42/100\n",
      "990/990 [==============================] - 0s 53us/step - loss: 0.0112 - acc: 0.9960\n",
      "Epoch 43/100\n",
      "990/990 [==============================] - 0s 49us/step - loss: 0.0103 - acc: 0.9980\n",
      "Epoch 44/100\n",
      "990/990 [==============================] - 0s 53us/step - loss: 0.0088 - acc: 0.9980\n",
      "Epoch 45/100\n",
      "990/990 [==============================] - 0s 47us/step - loss: 0.0080 - acc: 0.9990\n",
      "Epoch 46/100\n",
      "990/990 [==============================] - 0s 50us/step - loss: 0.0069 - acc: 0.9990\n",
      "Epoch 47/100\n",
      "990/990 [==============================] - 0s 51us/step - loss: 0.0052 - acc: 1.0000\n",
      "Epoch 48/100\n",
      "990/990 [==============================] - 0s 49us/step - loss: 0.0097 - acc: 0.9990\n",
      "Epoch 49/100\n",
      "990/990 [==============================] - 0s 45us/step - loss: 0.0103 - acc: 0.9970\n",
      "Epoch 50/100\n",
      "990/990 [==============================] - 0s 47us/step - loss: 0.0071 - acc: 1.0000\n",
      "Epoch 51/100\n",
      "990/990 [==============================] - 0s 50us/step - loss: 0.0098 - acc: 0.9980\n",
      "Epoch 52/100\n",
      "990/990 [==============================] - 0s 47us/step - loss: 0.0120 - acc: 0.9970\n",
      "Epoch 53/100\n",
      "990/990 [==============================] - 0s 49us/step - loss: 0.0061 - acc: 0.9990\n",
      "Epoch 54/100\n",
      "990/990 [==============================] - 0s 46us/step - loss: 0.0077 - acc: 0.9970\n",
      "Epoch 55/100\n",
      "990/990 [==============================] - 0s 45us/step - loss: 0.0083 - acc: 0.9980\n",
      "Epoch 56/100\n",
      "990/990 [==============================] - 0s 48us/step - loss: 0.0060 - acc: 0.9990\n",
      "Epoch 57/100\n",
      "990/990 [==============================] - 0s 51us/step - loss: 0.0031 - acc: 1.0000\n",
      "Epoch 58/100\n",
      "990/990 [==============================] - 0s 49us/step - loss: 0.0043 - acc: 0.9990\n",
      "Epoch 59/100\n",
      "990/990 [==============================] - 0s 47us/step - loss: 0.0070 - acc: 0.9990\n",
      "Epoch 60/100\n",
      "990/990 [==============================] - 0s 47us/step - loss: 0.0055 - acc: 1.0000\n",
      "Epoch 61/100\n",
      "990/990 [==============================] - 0s 47us/step - loss: 0.0084 - acc: 0.9990\n",
      "Epoch 62/100\n",
      "990/990 [==============================] - 0s 46us/step - loss: 0.0063 - acc: 0.9990\n",
      "Epoch 63/100\n",
      "990/990 [==============================] - 0s 46us/step - loss: 0.0059 - acc: 0.9990\n",
      "Epoch 64/100\n",
      "990/990 [==============================] - 0s 47us/step - loss: 0.0055 - acc: 0.9990\n",
      "Epoch 65/100\n",
      "990/990 [==============================] - 0s 46us/step - loss: 0.0052 - acc: 1.0000\n",
      "Epoch 66/100\n",
      "990/990 [==============================] - 0s 45us/step - loss: 0.0042 - acc: 0.9990\n",
      "Epoch 67/100\n",
      "990/990 [==============================] - 0s 45us/step - loss: 0.0098 - acc: 0.9980\n",
      "Epoch 68/100\n",
      "990/990 [==============================] - 0s 48us/step - loss: 0.0076 - acc: 0.9990\n",
      "Epoch 69/100\n",
      "990/990 [==============================] - 0s 47us/step - loss: 0.0058 - acc: 0.9990\n",
      "Epoch 70/100\n",
      "990/990 [==============================] - 0s 46us/step - loss: 0.0042 - acc: 1.0000\n",
      "Epoch 71/100\n",
      "990/990 [==============================] - 0s 46us/step - loss: 0.0092 - acc: 0.9970\n",
      "Epoch 72/100\n",
      "990/990 [==============================] - 0s 45us/step - loss: 0.0055 - acc: 1.0000\n",
      "Epoch 73/100\n",
      "990/990 [==============================] - 0s 45us/step - loss: 0.0057 - acc: 0.9990\n",
      "Epoch 74/100\n",
      "990/990 [==============================] - 0s 46us/step - loss: 0.0048 - acc: 1.0000\n",
      "Epoch 75/100\n",
      "990/990 [==============================] - 0s 45us/step - loss: 0.0041 - acc: 1.0000\n",
      "Epoch 76/100\n",
      "990/990 [==============================] - 0s 45us/step - loss: 0.0068 - acc: 0.9980\n",
      "Epoch 77/100\n",
      "990/990 [==============================] - 0s 45us/step - loss: 0.0039 - acc: 1.0000\n",
      "Epoch 78/100\n",
      "990/990 [==============================] - 0s 49us/step - loss: 0.0057 - acc: 0.9980\n",
      "Epoch 79/100\n",
      "990/990 [==============================] - 0s 52us/step - loss: 0.0049 - acc: 0.9990\n",
      "Epoch 80/100\n",
      "990/990 [==============================] - 0s 45us/step - loss: 0.0040 - acc: 1.0000\n",
      "Epoch 81/100\n",
      "990/990 [==============================] - 0s 47us/step - loss: 0.0083 - acc: 0.9990\n",
      "Epoch 82/100\n",
      "990/990 [==============================] - 0s 45us/step - loss: 0.0052 - acc: 0.9990\n",
      "Epoch 83/100\n",
      "990/990 [==============================] - 0s 46us/step - loss: 0.0045 - acc: 1.0000\n",
      "Epoch 84/100\n",
      "990/990 [==============================] - 0s 46us/step - loss: 0.0033 - acc: 1.0000\n",
      "Epoch 85/100\n",
      "990/990 [==============================] - 0s 45us/step - loss: 0.0045 - acc: 0.9990\n",
      "Epoch 86/100\n",
      "990/990 [==============================] - 0s 45us/step - loss: 0.0026 - acc: 1.0000\n",
      "Epoch 87/100\n",
      "990/990 [==============================] - 0s 46us/step - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 88/100\n",
      "990/990 [==============================] - 0s 46us/step - loss: 0.0057 - acc: 0.9980\n",
      "Epoch 89/100\n",
      "990/990 [==============================] - 0s 46us/step - loss: 0.0034 - acc: 1.0000\n",
      "Epoch 90/100\n",
      "990/990 [==============================] - 0s 46us/step - loss: 0.0038 - acc: 0.9990\n",
      "Epoch 91/100\n",
      "990/990 [==============================] - 0s 46us/step - loss: 0.0031 - acc: 1.0000\n",
      "Epoch 92/100\n",
      "990/990 [==============================] - 0s 45us/step - loss: 0.0145 - acc: 0.9919\n",
      "Epoch 93/100\n",
      "990/990 [==============================] - 0s 45us/step - loss: 0.0033 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "990/990 [==============================] - 0s 45us/step - loss: 0.0026 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "990/990 [==============================] - 0s 46us/step - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "990/990 [==============================] - 0s 46us/step - loss: 0.0038 - acc: 0.9990\n",
      "Epoch 97/100\n",
      "990/990 [==============================] - 0s 45us/step - loss: 0.0043 - acc: 0.9990\n",
      "Epoch 98/100\n",
      "990/990 [==============================] - 0s 47us/step - loss: 0.0055 - acc: 0.9980\n",
      "Epoch 99/100\n",
      "990/990 [==============================] - 0s 47us/step - loss: 0.0033 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "990/990 [==============================] - 0s 51us/step - loss: 0.0050 - acc: 0.9990\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x1e6756d89b0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###great job done!!!train it and apply to test set\n",
    "model5.summary()    ###drop out didn't influence the number of neurons in latter layer \n",
    "model5.fit(raw_data_nor,label_cat,batch_size=150,epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply to test set and ready for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>margin1</th>\n",
       "      <th>margin2</th>\n",
       "      <th>margin3</th>\n",
       "      <th>margin4</th>\n",
       "      <th>margin5</th>\n",
       "      <th>margin6</th>\n",
       "      <th>margin7</th>\n",
       "      <th>margin8</th>\n",
       "      <th>margin9</th>\n",
       "      <th>...</th>\n",
       "      <th>texture55</th>\n",
       "      <th>texture56</th>\n",
       "      <th>texture57</th>\n",
       "      <th>texture58</th>\n",
       "      <th>texture59</th>\n",
       "      <th>texture60</th>\n",
       "      <th>texture61</th>\n",
       "      <th>texture62</th>\n",
       "      <th>texture63</th>\n",
       "      <th>texture64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.053711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.044922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.128910</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.036133</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089844</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042969</td>\n",
       "      <td>0.016602</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.007812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.049805</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>19</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.042969</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.030273</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>23</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.050781</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.117190</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030273</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.048828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>24</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.074219</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102540</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>28</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.195310</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>33</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.056641</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.076172</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>36</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049805</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081055</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.012695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>39</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026367</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>41</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.060547</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.033203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>44</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.016602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>46</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.111330</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.045898</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050781</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.009766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>47</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.132810</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.288090</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>51</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022461</td>\n",
       "      <td>0.030273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>52</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.018555</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032227</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.005859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>53</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082031</td>\n",
       "      <td>0.111330</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.001953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>57</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.083984</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.047852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>59</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.048828</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051758</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014648</td>\n",
       "      <td>0.044922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>62</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.158200</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.203130</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.531250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>65</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.056641</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.124020</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.043945</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>68</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.119140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053711</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>70</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040039</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.007812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>74</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.138670</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>77</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095703</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070312</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>79</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.113280</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>86</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.048828</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.002930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>1493</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.016602</td>\n",
       "      <td>0.066406</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.009766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>1495</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.058594</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.056641</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>1497</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.074219</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038086</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.002930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>1498</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.044922</td>\n",
       "      <td>0.048828</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>1503</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.142580</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.154300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.208980</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060547</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>1510</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.047852</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038086</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>1513</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.050781</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>1517</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.097656</td>\n",
       "      <td>0.052734</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043945</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.024414</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.002930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>1522</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024414</td>\n",
       "      <td>0.088867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>1526</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.074219</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022461</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.034180</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.031250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>1528</td>\n",
       "      <td>0.074219</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.244140</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>1533</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.014648</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.047852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>1534</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.204100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049805</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>1535</td>\n",
       "      <td>0.042969</td>\n",
       "      <td>0.144530</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.130860</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>...</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022461</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>1537</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.043945</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.028320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>1540</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.052734</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106450</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.001953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>1542</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036133</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>1546</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106450</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.011719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>1553</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.044922</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.052734</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032227</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.028320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.046875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>1558</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.152340</td>\n",
       "      <td>0.024414</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>1560</td>\n",
       "      <td>0.042969</td>\n",
       "      <td>0.107420</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.115230</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.346680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.020508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>1564</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.095703</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>1565</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.028320</td>\n",
       "      <td>0.060547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>1567</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.042969</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.086914</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>1573</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.060547</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.126950</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.154300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.123050</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>1576</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.098633</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590</th>\n",
       "      <td>1577</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090820</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591</th>\n",
       "      <td>1579</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.073242</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042969</td>\n",
       "      <td>0.006836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>1580</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.060547</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.018555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>1583</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.117190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.136720</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.107420</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.016602</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>594 rows × 193 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id   margin1   margin2   margin3   margin4   margin5   margin6  \\\n",
       "0       4  0.019531  0.009766  0.078125  0.011719  0.003906  0.015625   \n",
       "1       7  0.007812  0.005859  0.064453  0.009766  0.003906  0.013672   \n",
       "2       9  0.000000  0.000000  0.001953  0.021484  0.041016  0.000000   \n",
       "3      12  0.000000  0.000000  0.009766  0.011719  0.017578  0.000000   \n",
       "4      13  0.001953  0.000000  0.015625  0.009766  0.039062  0.000000   \n",
       "5      16  0.021484  0.033203  0.021484  0.009766  0.015625  0.035156   \n",
       "6      19  0.015625  0.025391  0.046875  0.009766  0.005859  0.027344   \n",
       "7      23  0.007812  0.031250  0.011719  0.050781  0.000000  0.117190   \n",
       "8      24  0.003906  0.007812  0.074219  0.017578  0.015625  0.003906   \n",
       "9      28  0.000000  0.000000  0.005859  0.021484  0.054688  0.000000   \n",
       "10     33  0.025391  0.041016  0.033203  0.017578  0.009766  0.056641   \n",
       "11     36  0.001953  0.009766  0.025391  0.003906  0.015625  0.003906   \n",
       "12     39  0.013672  0.000000  0.046875  0.019531  0.005859  0.000000   \n",
       "13     41  0.007812  0.005859  0.060547  0.013672  0.011719  0.000000   \n",
       "14     44  0.017578  0.039062  0.031250  0.005859  0.005859  0.039062   \n",
       "15     46  0.011719  0.017578  0.111330  0.009766  0.007812  0.011719   \n",
       "16     47  0.025391  0.015625  0.132810  0.003906  0.001953  0.009766   \n",
       "17     51  0.007812  0.007812  0.019531  0.039062  0.031250  0.021484   \n",
       "18     52  0.001953  0.007812  0.031250  0.005859  0.013672  0.003906   \n",
       "19     53  0.000000  0.000000  0.082031  0.111330  0.017578  0.000000   \n",
       "20     57  0.013672  0.023438  0.007812  0.054688  0.001953  0.083984   \n",
       "21     59  0.011719  0.011719  0.048828  0.005859  0.007812  0.015625   \n",
       "22     62  0.037109  0.158200  0.013672  0.009766  0.000000  0.203130   \n",
       "23     65  0.027344  0.056641  0.021484  0.011719  0.005859  0.019531   \n",
       "24     68  0.000000  0.000000  0.003906  0.021484  0.023438  0.000000   \n",
       "25     70  0.015625  0.023438  0.025391  0.011719  0.005859  0.005859   \n",
       "26     74  0.019531  0.064453  0.009766  0.039062  0.000000  0.138670   \n",
       "27     77  0.000000  0.095703  0.000000  0.017578  0.000000  0.000000   \n",
       "28     79  0.001953  0.000000  0.015625  0.113280  0.023438  0.000000   \n",
       "29     86  0.017578  0.078125  0.023438  0.003906  0.001953  0.048828   \n",
       "..    ...       ...       ...       ...       ...       ...       ...   \n",
       "564  1493  0.005859  0.000000  0.021484  0.039062  0.011719  0.003906   \n",
       "565  1495  0.007812  0.015625  0.058594  0.023438  0.027344  0.013672   \n",
       "566  1497  0.005859  0.005859  0.009766  0.005859  0.074219  0.001953   \n",
       "567  1498  0.021484  0.015625  0.078125  0.019531  0.003906  0.044922   \n",
       "568  1503  0.041016  0.142580  0.021484  0.001953  0.000000  0.154300   \n",
       "569  1510  0.001953  0.000000  0.009766  0.015625  0.027344  0.000000   \n",
       "570  1513  0.011719  0.007812  0.021484  0.001953  0.041016  0.001953   \n",
       "571  1517  0.000000  0.000000  0.097656  0.052734  0.005859  0.000000   \n",
       "572  1522  0.001953  0.005859  0.025391  0.011719  0.007812  0.000000   \n",
       "573  1526  0.015625  0.017578  0.074219  0.011719  0.001953  0.015625   \n",
       "574  1528  0.074219  0.064453  0.013672  0.003906  0.000000  0.244140   \n",
       "575  1533  0.017578  0.017578  0.031250  0.009766  0.003906  0.015625   \n",
       "576  1534  0.001953  0.000000  0.017578  0.011719  0.027344  0.003906   \n",
       "577  1535  0.042969  0.144530  0.035156  0.005859  0.000000  0.130860   \n",
       "578  1537  0.003906  0.003906  0.021484  0.011719  0.046875  0.003906   \n",
       "579  1540  0.000000  0.001953  0.015625  0.013672  0.029297  0.001953   \n",
       "580  1542  0.005859  0.011719  0.033203  0.001953  0.035156  0.015625   \n",
       "581  1546  0.011719  0.013672  0.017578  0.007812  0.007812  0.005859   \n",
       "582  1553  0.009766  0.001953  0.044922  0.015625  0.021484  0.003906   \n",
       "583  1558  0.001953  0.000000  0.009766  0.001953  0.021484  0.005859   \n",
       "584  1560  0.042969  0.107420  0.025391  0.011719  0.003906  0.115230   \n",
       "585  1564  0.009766  0.025391  0.064453  0.005859  0.001953  0.095703   \n",
       "586  1565  0.007812  0.019531  0.013672  0.031250  0.011719  0.003906   \n",
       "587  1567  0.003906  0.017578  0.042969  0.033203  0.041016  0.007812   \n",
       "588  1573  0.064453  0.039062  0.009766  0.015625  0.003906  0.060547   \n",
       "589  1576  0.000000  0.000000  0.003906  0.015625  0.041016  0.000000   \n",
       "590  1577  0.000000  0.003906  0.003906  0.005859  0.017578  0.000000   \n",
       "591  1579  0.017578  0.029297  0.015625  0.013672  0.003906  0.015625   \n",
       "592  1580  0.013672  0.009766  0.060547  0.025391  0.035156  0.025391   \n",
       "593  1583  0.000000  0.117190  0.000000  0.019531  0.000000  0.136720   \n",
       "\n",
       "      margin7   margin8   margin9    ...      texture55  texture56  texture57  \\\n",
       "0    0.005859  0.000000  0.005859    ...       0.006836   0.000000   0.015625   \n",
       "1    0.007812  0.000000  0.033203    ...       0.000000   0.000000   0.006836   \n",
       "2    0.023438  0.000000  0.011719    ...       0.128910   0.000000   0.000977   \n",
       "3    0.003906  0.000000  0.003906    ...       0.012695   0.015625   0.002930   \n",
       "4    0.009766  0.000000  0.005859    ...       0.000000   0.042969   0.016602   \n",
       "5    0.039062  0.000000  0.003906    ...       0.000000   0.000000   0.000000   \n",
       "6    0.042969  0.000000  0.000000    ...       0.001953   0.000000   0.000000   \n",
       "7    0.003906  0.000000  0.011719    ...       0.047852   0.000000   0.030273   \n",
       "8    0.011719  0.000000  0.009766    ...       0.102540   0.000000   0.023438   \n",
       "9    0.015625  0.000000  0.011719    ...       0.195310   0.039062   0.003906   \n",
       "10   0.023438  0.000000  0.005859    ...       0.000000   0.000000   0.000000   \n",
       "11   0.041016  0.000000  0.005859    ...       0.049805   0.000000   0.081055   \n",
       "12   0.041016  0.000000  0.013672    ...       0.000000   0.000000   0.002930   \n",
       "13   0.037109  0.000000  0.000000    ...       0.002930   0.000000   0.001953   \n",
       "14   0.013672  0.000000  0.007812    ...       0.000000   0.000000   0.007812   \n",
       "15   0.027344  0.000000  0.000000    ...       0.000000   0.000000   0.000000   \n",
       "16   0.001953  0.000000  0.000000    ...       0.288090   0.000000   0.046875   \n",
       "17   0.039062  0.000000  0.001953    ...       0.009766   0.000000   0.000977   \n",
       "18   0.015625  0.000000  0.000000    ...       0.000000   0.000000   0.000000   \n",
       "19   0.000000  0.000000  0.013672    ...       0.005859   0.000000   0.000000   \n",
       "20   0.019531  0.000000  0.001953    ...       0.009766   0.000000   0.064453   \n",
       "21   0.046875  0.005859  0.000000    ...       0.035156   0.000000   0.051758   \n",
       "22   0.000000  0.000000  0.005859    ...       0.000000   0.000000   0.000000   \n",
       "23   0.029297  0.000000  0.000000    ...       0.000000   0.124020   0.000000   \n",
       "24   0.003906  0.000000  0.005859    ...       0.119140   0.000000   0.002930   \n",
       "25   0.011719  0.000000  0.000000    ...       0.040039   0.000000   0.029297   \n",
       "26   0.001953  0.000000  0.009766    ...       0.001953   0.000000   0.010742   \n",
       "27   0.000000  0.005859  0.000000    ...       0.031250   0.007812   0.000000   \n",
       "28   0.000000  0.000000  0.017578    ...       0.018555   0.000000   0.000000   \n",
       "29   0.029297  0.000000  0.000000    ...       0.015625   0.000977   0.023438   \n",
       "..        ...       ...       ...    ...            ...        ...        ...   \n",
       "564  0.003906  0.000000  0.005859    ...       0.000000   0.000977   0.016602   \n",
       "565  0.056641  0.003906  0.009766    ...       0.000000   0.000000   0.001953   \n",
       "566  0.011719  0.000000  0.005859    ...       0.038086   0.000000   0.006836   \n",
       "567  0.048828  0.000000  0.005859    ...       0.000000   0.000000   0.000977   \n",
       "568  0.000000  0.000000  0.005859    ...       0.208980   0.000000   0.060547   \n",
       "569  0.001953  0.000000  0.005859    ...       0.000000   0.033203   0.003906   \n",
       "570  0.050781  0.000000  0.005859    ...       0.008789   0.000977   0.011719   \n",
       "571  0.000000  0.000000  0.005859    ...       0.043945   0.000000   0.002930   \n",
       "572  0.033203  0.000000  0.000000    ...       0.000977   0.000000   0.005859   \n",
       "573  0.019531  0.000000  0.000000    ...       0.022461   0.000000   0.006836   \n",
       "574  0.005859  0.000000  0.005859    ...       0.064453   0.000000   0.013672   \n",
       "575  0.015625  0.000000  0.007812    ...       0.000000   0.000000   0.006836   \n",
       "576  0.015625  0.000000  0.011719    ...       0.000000   0.000000   0.000000   \n",
       "577  0.003906  0.000000  0.009766    ...       0.078125   0.000000   0.022461   \n",
       "578  0.033203  0.000000  0.005859    ...       0.004883   0.009766   0.000977   \n",
       "579  0.052734  0.000000  0.007812    ...       0.106450   0.000000   0.012695   \n",
       "580  0.027344  0.000000  0.009766    ...       0.001953   0.000000   0.046875   \n",
       "581  0.019531  0.000000  0.005859    ...       0.106450   0.000000   0.033203   \n",
       "582  0.052734  0.000000  0.000000    ...       0.032227   0.000000   0.008789   \n",
       "583  0.037109  0.000000  0.005859    ...       0.000000   0.000000   0.000000   \n",
       "584  0.025391  0.000000  0.005859    ...       0.346680   0.000000   0.047852   \n",
       "585  0.064453  0.000000  0.005859    ...       0.000000   0.000000   0.005859   \n",
       "586  0.023438  0.000000  0.021484    ...       0.000000   0.000000   0.006836   \n",
       "587  0.013672  0.000000  0.007812    ...       0.000000   0.000000   0.000000   \n",
       "588  0.013672  0.000000  0.005859    ...       0.000000   0.000000   0.000000   \n",
       "589  0.017578  0.000000  0.005859    ...       0.098633   0.000000   0.004883   \n",
       "590  0.017578  0.005859  0.000000    ...       0.012695   0.004883   0.004883   \n",
       "591  0.025391  0.000000  0.000000    ...       0.073242   0.000000   0.028320   \n",
       "592  0.039062  0.000000  0.003906    ...       0.003906   0.000000   0.000977   \n",
       "593  0.001953  0.005859  0.000000    ...       0.107420   0.012695   0.016602   \n",
       "\n",
       "     texture58  texture59  texture60  texture61  texture62  texture63  \\\n",
       "0     0.000977   0.015625   0.000000   0.000000   0.000000   0.003906   \n",
       "1     0.001953   0.013672   0.000000   0.000000   0.000977   0.037109   \n",
       "2     0.000000   0.000000   0.000000   0.000000   0.015625   0.000000   \n",
       "3     0.036133   0.013672   0.000000   0.000000   0.089844   0.000000   \n",
       "4     0.010742   0.041016   0.000000   0.000000   0.007812   0.009766   \n",
       "5     0.000977   0.049805   0.000000   0.000000   0.027344   0.000000   \n",
       "6     0.004883   0.030273   0.000000   0.000000   0.000977   0.000000   \n",
       "7     0.000000   0.011719   0.000000   0.000000   0.003906   0.002930   \n",
       "8     0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "9     0.007812   0.013672   0.001953   0.000000   0.029297   0.000000   \n",
       "10    0.006836   0.076172   0.000000   0.000000   0.000000   0.035156   \n",
       "11    0.000977   0.009766   0.000000   0.000000   0.000000   0.010742   \n",
       "12    0.000000   0.026367   0.000000   0.000000   0.000000   0.000000   \n",
       "13    0.000977   0.020508   0.000000   0.000000   0.002930   0.002930   \n",
       "14    0.000977   0.016602   0.000000   0.000000   0.003906   0.000000   \n",
       "15    0.039062   0.045898   0.000000   0.000000   0.050781   0.005859   \n",
       "16    0.000000   0.007812   0.000000   0.000000   0.000000   0.000000   \n",
       "17    0.000000   0.005859   0.000000   0.000000   0.000000   0.022461   \n",
       "18    0.004883   0.018555   0.019531   0.000000   0.032227   0.013672   \n",
       "19    0.000000   0.010742   0.000000   0.000000   0.000000   0.031250   \n",
       "20    0.000000   0.005859   0.000000   0.000000   0.000000   0.011719   \n",
       "21    0.000000   0.007812   0.000000   0.000000   0.000000   0.014648   \n",
       "22    0.015625   0.000000   0.531250   0.000000   0.035156   0.000000   \n",
       "23    0.033203   0.005859   0.000977   0.000000   0.062500   0.043945   \n",
       "24    0.003906   0.000000   0.000000   0.000000   0.053711   0.000000   \n",
       "25    0.000000   0.006836   0.000000   0.004883   0.000977   0.009766   \n",
       "26    0.033203   0.019531   0.001953   0.000000   0.029297   0.000000   \n",
       "27    0.002930   0.005859   0.000000   0.000000   0.070312   0.000000   \n",
       "28    0.000000   0.012695   0.000000   0.000000   0.000000   0.000000   \n",
       "29    0.000000   0.009766   0.000000   0.000977   0.000000   0.008789   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "564   0.066406   0.027344   0.000000   0.000977   0.064453   0.005859   \n",
       "565   0.000977   0.000000   0.000000   0.000000   0.001953   0.000000   \n",
       "566   0.001953   0.012695   0.000000   0.000000   0.033203   0.005859   \n",
       "567   0.041016   0.002930   0.035156   0.000000   0.085938   0.000000   \n",
       "568   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "569   0.013672   0.047852   0.010742   0.000000   0.038086   0.015625   \n",
       "570   0.001953   0.023438   0.000000   0.000000   0.017578   0.000000   \n",
       "571   0.003906   0.024414   0.000977   0.000000   0.027344   0.007812   \n",
       "572   0.000000   0.017578   0.000000   0.000000   0.000000   0.024414   \n",
       "573   0.007812   0.034180   0.000000   0.000000   0.004883   0.000977   \n",
       "574   0.000000   0.002930   0.000000   0.000000   0.000000   0.007812   \n",
       "575   0.000977   0.014648   0.000000   0.000000   0.000000   0.037109   \n",
       "576   0.000977   0.000000   0.204100   0.000000   0.049805   0.000000   \n",
       "577   0.000000   0.000977   0.000000   0.000000   0.000000   0.000000   \n",
       "578   0.004883   0.043945   0.000000   0.000000   0.017578   0.005859   \n",
       "579   0.000000   0.012695   0.000000   0.000000   0.000000   0.037109   \n",
       "580   0.000000   0.036133   0.000000   0.000000   0.000000   0.000000   \n",
       "581   0.000977   0.010742   0.000000   0.000000   0.000000   0.035156   \n",
       "582   0.002930   0.028320   0.000000   0.000000   0.004883   0.008789   \n",
       "583   0.152340   0.024414   0.001953   0.000000   0.046875   0.004883   \n",
       "584   0.000000   0.007812   0.000000   0.000000   0.000000   0.001953   \n",
       "585   0.011719   0.008789   0.000000   0.000000   0.008789   0.000000   \n",
       "586   0.000000   0.021484   0.000000   0.000000   0.000977   0.028320   \n",
       "587   0.004883   0.013672   0.000000   0.000000   0.000977   0.086914   \n",
       "588   0.126950   0.000977   0.154300   0.000000   0.123050   0.000000   \n",
       "589   0.000000   0.003906   0.000000   0.000000   0.018555   0.000000   \n",
       "590   0.002930   0.009766   0.000000   0.000000   0.090820   0.000000   \n",
       "591   0.000000   0.001953   0.000000   0.000000   0.000000   0.042969   \n",
       "592   0.000000   0.011719   0.000000   0.000000   0.000000   0.011719   \n",
       "593   0.000977   0.004883   0.000000   0.000000   0.015625   0.000000   \n",
       "\n",
       "     texture64  \n",
       "0     0.053711  \n",
       "1     0.044922  \n",
       "2     0.000000  \n",
       "3     0.008789  \n",
       "4     0.007812  \n",
       "5     0.021484  \n",
       "6     0.024414  \n",
       "7     0.048828  \n",
       "8     0.008789  \n",
       "9     0.007812  \n",
       "10    0.000000  \n",
       "11    0.012695  \n",
       "12    0.018555  \n",
       "13    0.033203  \n",
       "14    0.011719  \n",
       "15    0.009766  \n",
       "16    0.014648  \n",
       "17    0.030273  \n",
       "18    0.005859  \n",
       "19    0.001953  \n",
       "20    0.047852  \n",
       "21    0.044922  \n",
       "22    0.000000  \n",
       "23    0.000000  \n",
       "24    0.031250  \n",
       "25    0.007812  \n",
       "26    0.026367  \n",
       "27    0.000000  \n",
       "28    0.000000  \n",
       "29    0.002930  \n",
       "..         ...  \n",
       "564   0.009766  \n",
       "565   0.006836  \n",
       "566   0.002930  \n",
       "567   0.000000  \n",
       "568   0.000977  \n",
       "569   0.000977  \n",
       "570   0.031250  \n",
       "571   0.002930  \n",
       "572   0.088867  \n",
       "573   0.031250  \n",
       "574   0.000977  \n",
       "575   0.047852  \n",
       "576   0.000000  \n",
       "577   0.017578  \n",
       "578   0.028320  \n",
       "579   0.001953  \n",
       "580   0.011719  \n",
       "581   0.011719  \n",
       "582   0.046875  \n",
       "583   0.000000  \n",
       "584   0.020508  \n",
       "585   0.016602  \n",
       "586   0.060547  \n",
       "587   0.000000  \n",
       "588   0.000000  \n",
       "589   0.000977  \n",
       "590   0.016602  \n",
       "591   0.006836  \n",
       "592   0.018555  \n",
       "593   0.017578  \n",
       "\n",
       "[594 rows x 193 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_id=test_data.pop('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_norm=preprocessing.MinMaxScaler().fit(raw_data).transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "593"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_norm\n",
    "len(test_data_norm)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pred = model5.predict_proba(test_data_norm,batch_size=36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result=pd.DataFrame(Pred,index=test_data_id,columns=col_n.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['margin1', 'margin2', 'margin3', 'margin4', 'margin5', 'margin6',\n",
       "       'margin7', 'margin8', 'margin9', 'margin10', 'margin11',\n",
       "       'margin12', 'margin13', 'margin14', 'margin15', 'margin16',\n",
       "       'margin17', 'margin18', 'margin19', 'margin20', 'margin21',\n",
       "       'margin22', 'margin23', 'margin24', 'margin25', 'margin26',\n",
       "       'margin27', 'margin28', 'margin29', 'margin30', 'margin31',\n",
       "       'margin32', 'margin33', 'margin34', 'margin35', 'margin36',\n",
       "       'margin37', 'margin38', 'margin39', 'margin40', 'margin41',\n",
       "       'margin42', 'margin43', 'margin44', 'margin45', 'margin46',\n",
       "       'margin47', 'margin48', 'margin49', 'margin50', 'margin51',\n",
       "       'margin52', 'margin53', 'margin54', 'margin55', 'margin56',\n",
       "       'margin57', 'margin58', 'margin59', 'margin60', 'margin61',\n",
       "       'margin62', 'margin63', 'margin64', 'shape1', 'shape2', 'shape3',\n",
       "       'shape4', 'shape5', 'shape6', 'shape7', 'shape8', 'shape9',\n",
       "       'shape10', 'shape11', 'shape12', 'shape13', 'shape14', 'shape15',\n",
       "       'shape16', 'shape17', 'shape18', 'shape19', 'shape20', 'shape21',\n",
       "       'shape22', 'shape23', 'shape24', 'shape25', 'shape26', 'shape27',\n",
       "       'shape28', 'shape29', 'shape30', 'shape31', 'shape32', 'shape33',\n",
       "       'shape34', 'shape35', 'shape36', 'shape37', 'shape38', 'shape39',\n",
       "       'shape40', 'shape41', 'shape42', 'shape43', 'shape44', 'shape45',\n",
       "       'shape46', 'shape47', 'shape48', 'shape49', 'shape50', 'shape51',\n",
       "       'shape52', 'shape53', 'shape54', 'shape55', 'shape56', 'shape57',\n",
       "       'shape58', 'shape59', 'shape60', 'shape61', 'shape62', 'shape63',\n",
       "       'shape64', 'texture1', 'texture2', 'texture3', 'texture4',\n",
       "       'texture5', 'texture6', 'texture7', 'texture8', 'texture9',\n",
       "       'texture10', 'texture11', 'texture12', 'texture13', 'texture14',\n",
       "       'texture15', 'texture16', 'texture17', 'texture18', 'texture19',\n",
       "       'texture20', 'texture21', 'texture22', 'texture23', 'texture24',\n",
       "       'texture25', 'texture26', 'texture27', 'texture28', 'texture29',\n",
       "       'texture30', 'texture31', 'texture32', 'texture33', 'texture34',\n",
       "       'texture35', 'texture36', 'texture37', 'texture38', 'texture39',\n",
       "       'texture40', 'texture41', 'texture42', 'texture43', 'texture44',\n",
       "       'texture45', 'texture46', 'texture47', 'texture48', 'texture49',\n",
       "       'texture50', 'texture51', 'texture52', 'texture53', 'texture54',\n",
       "       'texture55', 'texture56', 'texture57', 'texture58', 'texture59',\n",
       "       'texture60', 'texture61', 'texture62', 'texture63', 'texture64'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(594,)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_id.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(594, 192)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Acer_Capillipes', 'Acer_Circinatum', 'Acer_Mono', 'Acer_Opalus',\n",
       "       'Acer_Palmatum', 'Acer_Pictum', 'Acer_Platanoids', 'Acer_Rubrum',\n",
       "       'Acer_Rufinerve', 'Acer_Saccharinum', 'Alnus_Cordata',\n",
       "       'Alnus_Maximowiczii', 'Alnus_Rubra', 'Alnus_Sieboldiana',\n",
       "       'Alnus_Viridis', 'Arundinaria_Simonii', 'Betula_Austrosinensis',\n",
       "       'Betula_Pendula', 'Callicarpa_Bodinieri', 'Castanea_Sativa',\n",
       "       'Celtis_Koraiensis', 'Cercis_Siliquastrum', 'Cornus_Chinensis',\n",
       "       'Cornus_Controversa', 'Cornus_Macrophylla', 'Cotinus_Coggygria',\n",
       "       'Crataegus_Monogyna', 'Cytisus_Battandieri',\n",
       "       'Eucalyptus_Glaucescens', 'Eucalyptus_Neglecta',\n",
       "       'Eucalyptus_Urnigera', 'Fagus_Sylvatica', 'Ginkgo_Biloba',\n",
       "       'Ilex_Aquifolium', 'Ilex_Cornuta', 'Liquidambar_Styraciflua',\n",
       "       'Liriodendron_Tulipifera', 'Lithocarpus_Cleistocarpus',\n",
       "       'Lithocarpus_Edulis', 'Magnolia_Heptapeta', 'Magnolia_Salicifolia',\n",
       "       'Morus_Nigra', 'Olea_Europaea', 'Phildelphus', 'Populus_Adenopoda',\n",
       "       'Populus_Grandidentata', 'Populus_Nigra', 'Prunus_Avium',\n",
       "       'Prunus_X_Shmittii', 'Pterocarya_Stenoptera', 'Quercus_Afares',\n",
       "       'Quercus_Agrifolia', 'Quercus_Alnifolia', 'Quercus_Brantii',\n",
       "       'Quercus_Canariensis', 'Quercus_Castaneifolia', 'Quercus_Cerris',\n",
       "       'Quercus_Chrysolepis', 'Quercus_Coccifera', 'Quercus_Coccinea',\n",
       "       'Quercus_Crassifolia', 'Quercus_Crassipes', 'Quercus_Dolicholepis',\n",
       "       'Quercus_Ellipsoidalis', 'Quercus_Greggii', 'Quercus_Hartwissiana',\n",
       "       'Quercus_Ilex', 'Quercus_Imbricaria', 'Quercus_Infectoria_sub',\n",
       "       'Quercus_Kewensis', 'Quercus_Nigra', 'Quercus_Palustris',\n",
       "       'Quercus_Phellos', 'Quercus_Phillyraeoides', 'Quercus_Pontica',\n",
       "       'Quercus_Pubescens', 'Quercus_Pyrenaica', 'Quercus_Rhysophylla',\n",
       "       'Quercus_Rubra', 'Quercus_Semecarpifolia', 'Quercus_Shumardii',\n",
       "       'Quercus_Suber', 'Quercus_Texana', 'Quercus_Trojana',\n",
       "       'Quercus_Variabilis', 'Quercus_Vulcanica', 'Quercus_x_Hispanica',\n",
       "       'Quercus_x_Turneri', 'Rhododendron_x_Russellianum',\n",
       "       'Salix_Fragilis', 'Salix_Intergra', 'Sorbus_Aria', 'Tilia_Oliveri',\n",
       "       'Tilia_Platyphyllos', 'Tilia_Tomentosa', 'Ulmus_Bergmanniana',\n",
       "       'Viburnum_Tinus', 'Viburnum_x_Rhytidophylloides',\n",
       "       'Zelkova_Serrata'], dtype=object)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_n.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = open('submission_leaf.csv','w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = open('submission_leaf.csv','w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "799700"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp.write(final_result.to_csv())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####the part below is about the combination model without full impliment\n",
    "\n",
    "\n",
    "I found that combination models somewhat interest so I decided to dig into more about it; \n",
    "\n",
    "since the model seems to be the hardest part, lets get it done first! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.layers import Conv2D,Concatenate,MaxPool2D,Input,concatenate\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.preprocessing.image import img_to_array, load_img,ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import os,cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    2\n",
       "2    3\n",
       "3    5\n",
       "4    6\n",
       "Name: id, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_ID.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1584, 4)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##first let's look at those images    %%%I couldn't figure out a way to modify the value in pandas series to address string\n",
    "Im_dir='D:\\CNN_practice\\leaf identification\\images\\\\'\n",
    "#next_image_train=next(iter(D_ID.value))\n",
    "#for i in range (990):\n",
    "#    all_image=load_img(os.path.join(Im_dir+str(D_ID)+'.jpg'))  \n",
    "##one by one read might work but I somehow feel unhappy about this \n",
    "##since it isn't that efficient, so I take the function from dogs and cats competition\n",
    "\n",
    "\n",
    "###try to solve this reversly\n",
    "##read the name inside of direactory and make a hash table\n",
    "def gen_image_label(directory):\n",
    "    ''' A generator that yields (label, id, jpg_filename) tuple.'''\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for f in files:\n",
    "            _, ext = os.path.splitext(f)\n",
    "            if ext != '.jpg':\n",
    "                continue\n",
    "            basename = os.path.basename(f)\n",
    "            splits = basename.split('.')\n",
    "            if len(splits) == 3:\n",
    "                label, id_, ext = splits\n",
    "            else:\n",
    "                label = None\n",
    "                id_, ext = splits\n",
    "            fullname = os.path.join(root, f)\n",
    "            yield label, int(id_), fullname, f\n",
    "lst = list(gen_image_label(Im_dir))\n",
    "train_df = pd.DataFrame(lst, columns=['label', 'id', 'filename','files'])\n",
    "train_df = train_df.sort_values(by=['label', 'id'])\n",
    "train_df.head(3)\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pull out the list of image\n",
    "train_dirt=train_df[np.isin(train_df.id,D_ID.values)]\n",
    "type(train_df.filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-f171c073c84a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dirt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Python3\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    765\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    768\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Python3\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_value\u001b[1;34m(self, series, key)\u001b[0m\n\u001b[0;32m   3102\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3103\u001b[0m             return self._engine.get_value(s, k,\n\u001b[1;32m-> 3104\u001b[1;33m                                           tz=getattr(series.dtype, 'tz', None))\n\u001b[0m\u001b[0;32m   3105\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3106\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minferred_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'integer'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'boolean'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "#plt.imshow(plt.imread(train_dirt.filename[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##reorganize data\n",
    "#import os\n",
    "#current_path = \"\" ## source path\n",
    "#new_path = \"D:\\CNN_practice\\leaf identification\\images\\\\train\\\\\" ## destination path\n",
    "\n",
    "#file_name=iter(train_dirt.files)\n",
    "#for files in train_dirt.filename:\n",
    "#    os.rename(files, new_path+'{}'.format(next(file_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>id</th>\n",
       "      <th>filename</th>\n",
       "      <th>files</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>D:\\CNN_practice\\leaf identification\\images\\tra...</td>\n",
       "      <td>1.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012</th>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>D:\\CNN_practice\\leaf identification\\images\\tra...</td>\n",
       "      <td>2.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1079</th>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>D:\\CNN_practice\\leaf identification\\images\\tra...</td>\n",
       "      <td>3.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1229</th>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>D:\\CNN_practice\\leaf identification\\images\\tra...</td>\n",
       "      <td>5.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1293</th>\n",
       "      <td>None</td>\n",
       "      <td>6</td>\n",
       "      <td>D:\\CNN_practice\\leaf identification\\images\\tra...</td>\n",
       "      <td>6.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1435</th>\n",
       "      <td>None</td>\n",
       "      <td>8</td>\n",
       "      <td>D:\\CNN_practice\\leaf identification\\images\\tra...</td>\n",
       "      <td>8.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>D:\\CNN_practice\\leaf identification\\images\\tra...</td>\n",
       "      <td>10.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>None</td>\n",
       "      <td>11</td>\n",
       "      <td>D:\\CNN_practice\\leaf identification\\images\\tra...</td>\n",
       "      <td>11.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>None</td>\n",
       "      <td>14</td>\n",
       "      <td>D:\\CNN_practice\\leaf identification\\images\\tra...</td>\n",
       "      <td>14.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>None</td>\n",
       "      <td>15</td>\n",
       "      <td>D:\\CNN_practice\\leaf identification\\images\\tra...</td>\n",
       "      <td>15.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>None</td>\n",
       "      <td>17</td>\n",
       "      <td>D:\\CNN_practice\\leaf identification\\images\\tra...</td>\n",
       "      <td>17.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>None</td>\n",
       "      <td>18</td>\n",
       "      <td>D:\\CNN_practice\\leaf identification\\images\\tra...</td>\n",
       "      <td>18.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>None</td>\n",
       "      <td>20</td>\n",
       "      <td>D:\\CNN_practice\\leaf identification\\images\\tra...</td>\n",
       "      <td>20.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>None</td>\n",
       "      <td>21</td>\n",
       "      <td>D:\\CNN_practice\\leaf identification\\images\\tra...</td>\n",
       "      <td>21.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>None</td>\n",
       "      <td>22</td>\n",
       "      <td>D:\\CNN_practice\\leaf identification\\images\\tra...</td>\n",
       "      <td>22.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048</th>\n",
       "      <td>None</td>\n",
       "      <td>25</td>\n",
       "      <td>D:\\CNN_practice\\leaf identification\\images\\tra...</td>\n",
       "      <td>25.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1055</th>\n",
       "      <td>None</td>\n",
       "      <td>26</td>\n",
       "      <td>D:\\CNN_practice\\leaf identification\\images\\tra...</td>\n",
       "      <td>26.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>None</td>\n",
       "      <td>27</td>\n",
       "      <td>D:\\CNN_practice\\leaf identification\\images\\tra...</td>\n",
       "      <td>27.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1074</th>\n",
       "      <td>None</td>\n",
       "      <td>29</td>\n",
       "      <td>D:\\CNN_practice\\leaf identification\\images\\tra...</td>\n",
       "      <td>29.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1080</th>\n",
       "      <td>None</td>\n",
       "      <td>30</td>\n",
       "      <td>D:\\CNN_practice\\leaf identification\\images\\tra...</td>\n",
       "      <td>30.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1087</th>\n",
       "      <td>None</td>\n",
       "      <td>31</td>\n",
       "      <td>D:\\CNN_practice\\leaf identification\\images\\tra...</td>\n",
       "      <td>31.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1094</th>\n",
       "      <td>None</td>\n",
       "      <td>32</td>\n",
       "      <td>D:\\CNN_practice\\leaf identification\\images\\tra...</td>\n",
       "      <td>32.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1112</th>\n",
       "      <td>None</td>\n",
       "      <td>34</td>\n",
       "      <td>D:\\CNN_practice\\leaf identification\\images\\tra...</td>\n",
       "      <td>34.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1120</th>\n",
       "      <td>None</td>\n",
       "      <td>35</td>\n",
       "      <td>D:\\CNN_practice\\leaf identification\\images\\tra...</td>\n",
       "      <td>35.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1135</th>\n",
       "      <td>None</td>\n",
       "      <td>37</td>\n",
       "      <td>D:\\CNN_practice\\leaf identification\\images\\tra...</td>\n",
       "      <td>37.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1142</th>\n",
       "      <td>None</td>\n",
       "      <td>38</td>\n",
       "      <td>D:\\CNN_practice\\leaf identification\\images\\tra...</td>\n",
       "      <td>38.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1157</th>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>D:\\CNN_practice\\leaf identification\\images\\tra...</td>\n",
       "      <td>40.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1170</th>\n",
       "      <td>None</td>\n",
       "      <td>42</td>\n",
       "      <td>D:\\CNN_practice\\leaf identification\\images\\tra...</td>\n",
       "      <td>42.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1177</th>\n",
       "      <td>None</td>\n",
       "      <td>43</td>\n",
       "      <td>D:\\CNN_practice\\leaf identification\\images\\tra...</td>\n",
       "      <td>43.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1191</th>\n",
       "      <td>None</td>\n",
       "      <td>45</td>\n",
       "      <td>D:\\CNN_practice\\leaf identification\\images\\tra...</td>\n",
       "      <td>45.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>957</th>\n",
       "      <td>None</td>\n",
       "      <td>1541</td>\n",
       "      <td>D:\\CNN_practice\\leaf identification\\images\\tra...</td>\n",
       "      <td>1541.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958</th>\n",
       "      <td>None</td>\n",
       "      <td>1543</td>\n",
       "      <td>D:\\CNN_practice\\leaf identification\\images\\tra...</td>\n",
       "      <td>1543.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959</th>\n",
       "      <td>None</td>\n",
       "      <td>1544</td>\n",
       "      <td>D:\\CNN_practice\\leaf identification\\images\\tra...</td>\n",
       "      <td>1544.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>None</td>\n",
       "      <td>1545</td>\n",
       "      <td>D:\\CNN_practice\\leaf identification\\images\\tra...</td>\n",
       "      <td>1545.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961</th>\n",
       "      <td>None</td>\n",
       "      <td>1547</td>\n",
       "      <td>D:\\CNN_practice\\leaf identification\\images\\tra...</td>\n",
       "      <td>1547.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>None</td>\n",
       "      <td>1548</td>\n",
       "      <td>D:\\CNN_practice\\leaf identification\\images\\tra...</td>\n",
       "      <td>1548.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963</th>\n",
       "      <td>None</td>\n",
       "      <td>1549</td>\n",
       "      <td>D:\\CNN_practice\\leaf identification\\images\\tra...</td>\n",
       "      <td>1549.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>965</th>\n",
       "      <td>None</td>\n",
       "      <td>1550</td>\n",
       "      <td>D:\\CNN_practice\\leaf identification\\images\\tra...</td>\n",
       "      <td>1550.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>966</th>\n",
       "      <td>None</td>\n",
       "      <td>1551</td>\n",
       "      <td>D:\\CNN_practice\\leaf identification\\images\\tra...</td>\n",
       "      <td>1551.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967</th>\n",
       "      <td>None</td>\n",
       "      <td>1552</td>\n",
       "      <td>D:\\CNN_practice\\leaf identification\\images\\tra...</td>\n",
       "      <td>1552.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>968</th>\n",
       "      <td>None</td>\n",
       "      <td>1554</td>\n",
       "      <td>D:\\CNN_practice\\leaf identification\\images\\tra...</td>\n",
       "      <td>1554.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>969</th>\n",
       "      <td>None</td>\n",
       "      <td>1555</td>\n",
       "      <td>D:\\CNN_practice\\leaf identification\\images\\tra...</td>\n",
       "      <td>1555.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>None</td>\n",
       "      <td>1556</td>\n",
       "      <td>D:\\CNN_practice\\leaf identification\\images\\tra...</td>\n",
       "      <td>1556.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>None</td>\n",
       "      <td>1557</td>\n",
       "      <td>D:\\CNN_practice\\leaf identification\\images\\tra...</td>\n",
       "      <td>1557.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>None</td>\n",
       "      <td>1559</td>\n",
       "      <td>D:\\CNN_practice\\leaf identification\\images\\tra...</td>\n",
       "      <td>1559.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>None</td>\n",
       "      <td>1561</td>\n",
       "      <td>D:\\CNN_practice\\leaf identification\\images\\tra...</td>\n",
       "      <td>1561.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>None</td>\n",
       "      <td>1562</td>\n",
       "      <td>D:\\CNN_practice\\leaf identification\\images\\tra...</td>\n",
       "      <td>1562.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>None</td>\n",
       "      <td>1563</td>\n",
       "      <td>D:\\CNN_practice\\leaf identification\\images\\tra...</td>\n",
       "      <td>1563.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>None</td>\n",
       "      <td>1566</td>\n",
       "      <td>D:\\CNN_practice\\leaf identification\\images\\tra...</td>\n",
       "      <td>1566.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>None</td>\n",
       "      <td>1568</td>\n",
       "      <td>D:\\CNN_practice\\leaf identification\\images\\tra...</td>\n",
       "      <td>1568.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>None</td>\n",
       "      <td>1569</td>\n",
       "      <td>D:\\CNN_practice\\leaf identification\\images\\tra...</td>\n",
       "      <td>1569.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>None</td>\n",
       "      <td>1570</td>\n",
       "      <td>D:\\CNN_practice\\leaf identification\\images\\tra...</td>\n",
       "      <td>1570.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>None</td>\n",
       "      <td>1571</td>\n",
       "      <td>D:\\CNN_practice\\leaf identification\\images\\tra...</td>\n",
       "      <td>1571.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>None</td>\n",
       "      <td>1572</td>\n",
       "      <td>D:\\CNN_practice\\leaf identification\\images\\tra...</td>\n",
       "      <td>1572.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>None</td>\n",
       "      <td>1574</td>\n",
       "      <td>D:\\CNN_practice\\leaf identification\\images\\tra...</td>\n",
       "      <td>1574.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>None</td>\n",
       "      <td>1575</td>\n",
       "      <td>D:\\CNN_practice\\leaf identification\\images\\tra...</td>\n",
       "      <td>1575.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>None</td>\n",
       "      <td>1578</td>\n",
       "      <td>D:\\CNN_practice\\leaf identification\\images\\tra...</td>\n",
       "      <td>1578.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>None</td>\n",
       "      <td>1581</td>\n",
       "      <td>D:\\CNN_practice\\leaf identification\\images\\tra...</td>\n",
       "      <td>1581.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>None</td>\n",
       "      <td>1582</td>\n",
       "      <td>D:\\CNN_practice\\leaf identification\\images\\tra...</td>\n",
       "      <td>1582.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>None</td>\n",
       "      <td>1584</td>\n",
       "      <td>D:\\CNN_practice\\leaf identification\\images\\tra...</td>\n",
       "      <td>1584.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>990 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label    id                                           filename     files\n",
       "594   None     1  D:\\CNN_practice\\leaf identification\\images\\tra...     1.jpg\n",
       "1012  None     2  D:\\CNN_practice\\leaf identification\\images\\tra...     2.jpg\n",
       "1079  None     3  D:\\CNN_practice\\leaf identification\\images\\tra...     3.jpg\n",
       "1229  None     5  D:\\CNN_practice\\leaf identification\\images\\tra...     5.jpg\n",
       "1293  None     6  D:\\CNN_practice\\leaf identification\\images\\tra...     6.jpg\n",
       "1435  None     8  D:\\CNN_practice\\leaf identification\\images\\tra...     8.jpg\n",
       "595   None    10  D:\\CNN_practice\\leaf identification\\images\\tra...    10.jpg\n",
       "662   None    11  D:\\CNN_practice\\leaf identification\\images\\tra...    11.jpg\n",
       "862   None    14  D:\\CNN_practice\\leaf identification\\images\\tra...    14.jpg\n",
       "925   None    15  D:\\CNN_practice\\leaf identification\\images\\tra...    15.jpg\n",
       "996   None    17  D:\\CNN_practice\\leaf identification\\images\\tra...    17.jpg\n",
       "1002  None    18  D:\\CNN_practice\\leaf identification\\images\\tra...    18.jpg\n",
       "1013  None    20  D:\\CNN_practice\\leaf identification\\images\\tra...    20.jpg\n",
       "1019  None    21  D:\\CNN_practice\\leaf identification\\images\\tra...    21.jpg\n",
       "1028  None    22  D:\\CNN_practice\\leaf identification\\images\\tra...    22.jpg\n",
       "1048  None    25  D:\\CNN_practice\\leaf identification\\images\\tra...    25.jpg\n",
       "1055  None    26  D:\\CNN_practice\\leaf identification\\images\\tra...    26.jpg\n",
       "1063  None    27  D:\\CNN_practice\\leaf identification\\images\\tra...    27.jpg\n",
       "1074  None    29  D:\\CNN_practice\\leaf identification\\images\\tra...    29.jpg\n",
       "1080  None    30  D:\\CNN_practice\\leaf identification\\images\\tra...    30.jpg\n",
       "1087  None    31  D:\\CNN_practice\\leaf identification\\images\\tra...    31.jpg\n",
       "1094  None    32  D:\\CNN_practice\\leaf identification\\images\\tra...    32.jpg\n",
       "1112  None    34  D:\\CNN_practice\\leaf identification\\images\\tra...    34.jpg\n",
       "1120  None    35  D:\\CNN_practice\\leaf identification\\images\\tra...    35.jpg\n",
       "1135  None    37  D:\\CNN_practice\\leaf identification\\images\\tra...    37.jpg\n",
       "1142  None    38  D:\\CNN_practice\\leaf identification\\images\\tra...    38.jpg\n",
       "1157  None    40  D:\\CNN_practice\\leaf identification\\images\\tra...    40.jpg\n",
       "1170  None    42  D:\\CNN_practice\\leaf identification\\images\\tra...    42.jpg\n",
       "1177  None    43  D:\\CNN_practice\\leaf identification\\images\\tra...    43.jpg\n",
       "1191  None    45  D:\\CNN_practice\\leaf identification\\images\\tra...    45.jpg\n",
       "...    ...   ...                                                ...       ...\n",
       "957   None  1541  D:\\CNN_practice\\leaf identification\\images\\tra...  1541.jpg\n",
       "958   None  1543  D:\\CNN_practice\\leaf identification\\images\\tra...  1543.jpg\n",
       "959   None  1544  D:\\CNN_practice\\leaf identification\\images\\tra...  1544.jpg\n",
       "960   None  1545  D:\\CNN_practice\\leaf identification\\images\\tra...  1545.jpg\n",
       "961   None  1547  D:\\CNN_practice\\leaf identification\\images\\tra...  1547.jpg\n",
       "962   None  1548  D:\\CNN_practice\\leaf identification\\images\\tra...  1548.jpg\n",
       "963   None  1549  D:\\CNN_practice\\leaf identification\\images\\tra...  1549.jpg\n",
       "965   None  1550  D:\\CNN_practice\\leaf identification\\images\\tra...  1550.jpg\n",
       "966   None  1551  D:\\CNN_practice\\leaf identification\\images\\tra...  1551.jpg\n",
       "967   None  1552  D:\\CNN_practice\\leaf identification\\images\\tra...  1552.jpg\n",
       "968   None  1554  D:\\CNN_practice\\leaf identification\\images\\tra...  1554.jpg\n",
       "969   None  1555  D:\\CNN_practice\\leaf identification\\images\\tra...  1555.jpg\n",
       "970   None  1556  D:\\CNN_practice\\leaf identification\\images\\tra...  1556.jpg\n",
       "971   None  1557  D:\\CNN_practice\\leaf identification\\images\\tra...  1557.jpg\n",
       "972   None  1559  D:\\CNN_practice\\leaf identification\\images\\tra...  1559.jpg\n",
       "973   None  1561  D:\\CNN_practice\\leaf identification\\images\\tra...  1561.jpg\n",
       "974   None  1562  D:\\CNN_practice\\leaf identification\\images\\tra...  1562.jpg\n",
       "975   None  1563  D:\\CNN_practice\\leaf identification\\images\\tra...  1563.jpg\n",
       "976   None  1566  D:\\CNN_practice\\leaf identification\\images\\tra...  1566.jpg\n",
       "977   None  1568  D:\\CNN_practice\\leaf identification\\images\\tra...  1568.jpg\n",
       "978   None  1569  D:\\CNN_practice\\leaf identification\\images\\tra...  1569.jpg\n",
       "979   None  1570  D:\\CNN_practice\\leaf identification\\images\\tra...  1570.jpg\n",
       "980   None  1571  D:\\CNN_practice\\leaf identification\\images\\tra...  1571.jpg\n",
       "981   None  1572  D:\\CNN_practice\\leaf identification\\images\\tra...  1572.jpg\n",
       "982   None  1574  D:\\CNN_practice\\leaf identification\\images\\tra...  1574.jpg\n",
       "983   None  1575  D:\\CNN_practice\\leaf identification\\images\\tra...  1575.jpg\n",
       "984   None  1578  D:\\CNN_practice\\leaf identification\\images\\tra...  1578.jpg\n",
       "985   None  1581  D:\\CNN_practice\\leaf identification\\images\\tra...  1581.jpg\n",
       "986   None  1582  D:\\CNN_practice\\leaf identification\\images\\tra...  1582.jpg\n",
       "987   None  1584  D:\\CNN_practice\\leaf identification\\images\\tra...  1584.jpg\n",
       "\n",
       "[990 rows x 4 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dirt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##load in image data \n",
    "\n",
    "data_generator_with_aug = ImageDataGenerator(\n",
    "                                   horizontal_flip=True,\n",
    "                                   width_shift_range = 0.2,\n",
    "                                   height_shift_range = 0.2)\n",
    "train_generator = data_generator_with_aug.flow_from_directory(\n",
    "        new_path,\n",
    "        target_size=(sz, sz),\n",
    "        batch_size=24,\n",
    "        class_mode='categorical')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##let's make combination models\n",
    "sz=224\n",
    "train_dirt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name=next(iter(train_dirt.files))\n",
    "file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name=next(iter(train_dirt.files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = 'D:\\CNN_practice\\leaf identification\\images\\\\train\\\\'\n",
    "ROWS = 64\n",
    "COLS = 64\n",
    "CHANNELS = 1\n",
    "train_images = [TRAIN_DIR+i for i in os.listdir(TRAIN_DIR)] # use this for full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\10.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\100.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1000.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1001.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1002.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1003.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1004.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1005.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1006.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1007.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\101.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1010.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1011.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1013.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1014.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1016.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1017.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1019.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1021.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1023.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1024.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1025.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1027.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\103.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1030.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1031.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1032.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1034.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1036.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1037.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1039.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1040.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1041.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1042.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1046.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1048.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1049.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1051.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1052.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1056.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1059.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\106.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1061.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1062.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1065.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1066.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\107.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1072.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1073.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1076.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1077.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\108.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1080.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1081.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1083.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1085.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1087.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1088.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1089.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\109.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1090.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1093.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1094.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1095.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1096.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1097.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1098.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\11.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1100.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1101.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1110.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1112.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1113.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1117.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1118.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1119.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1120.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1121.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1123.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1124.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1125.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1128.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1130.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1131.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1132.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1134.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1135.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1136.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1142.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1144.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1145.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1147.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1148.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\115.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1152.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1153.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1154.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1156.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1157.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1159.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\116.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1160.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1163.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1165.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1166.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1167.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1168.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1169.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1170.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1171.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1172.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1173.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1174.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1175.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1176.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1179.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\118.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1180.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1182.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1184.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1186.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1187.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1194.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1196.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1197.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1198.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\120.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1200.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1201.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1202.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1205.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1206.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1208.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1210.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1211.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1213.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1216.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1217.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1218.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1219.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\122.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1220.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1221.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1222.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1223.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1225.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1226.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1228.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1229.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1231.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1232.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1235.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1236.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1238.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1239.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\124.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1240.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1242.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1243.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1244.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1245.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1249.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1250.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1251.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1253.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1254.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1255.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1256.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1257.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1258.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1262.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1263.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1265.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1268.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1269.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\127.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1270.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1271.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1273.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1275.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1276.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1277.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1278.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1281.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1282.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1283.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1284.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1286.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1287.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1289.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\129.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1291.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1292.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1294.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1296.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1299.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\130.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1300.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1301.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1302.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1305.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1308.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1309.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1310.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1311.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1312.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1317.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1319.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\132.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1320.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1322.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1323.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1324.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1325.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1326.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1327.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1328.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1329.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\133.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1330.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1335.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1337.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1339.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\134.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1340.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1341.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1342.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1344.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1345.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1346.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1347.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1348.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1349.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1350.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1352.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1353.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1355.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1356.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1358.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1359.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1360.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1365.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1366.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1367.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1370.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1372.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1373.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1374.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1375.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1377.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1378.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1379.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1380.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1381.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1384.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1386.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1388.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\139.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1390.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1391.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1392.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1393.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1394.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1395.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1396.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1398.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1399.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\14.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\140.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1402.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1403.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1405.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1408.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1410.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1411.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1413.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1414.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1417.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1419.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\142.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1420.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1423.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1424.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1425.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\143.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1431.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1432.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1435.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1436.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1438.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1440.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1441.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1442.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1443.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1444.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1446.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1448.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1449.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\145.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1450.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1452.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1454.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1457.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1459.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\146.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1460.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1461.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1463.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1467.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1468.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1469.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1471.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1472.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1473.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1474.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1475.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1476.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1477.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1479.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\148.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1480.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1482.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1483.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1488.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\149.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1491.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1492.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1494.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1496.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1499.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\15.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1500.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1501.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1502.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1504.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1505.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1506.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1507.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1508.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1509.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1511.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1512.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1514.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1515.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1516.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1518.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1519.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\152.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1520.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1521.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1523.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1524.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1525.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1527.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1529.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\153.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1530.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1531.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1532.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1536.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1538.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1539.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1541.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1543.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1544.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1545.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1547.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1548.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1549.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\155.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1550.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1551.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1552.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1554.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1555.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1556.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1557.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1559.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1561.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1562.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1563.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1566.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1568.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1569.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1570.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1571.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1572.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1574.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1575.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1578.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1581.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1582.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\1584.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\160.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\163.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\164.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\165.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\166.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\167.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\168.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\169.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\17.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\171.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\173.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\175.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\178.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\179.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\18.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\183.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\186.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\188.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\189.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\190.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\194.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\196.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\198.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\199.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\2.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\20.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\201.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\202.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\203.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\204.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\206.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\21.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\211.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\212.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\214.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\215.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\216.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\217.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\218.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\219.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\22.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\224.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\225.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\227.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\228.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\231.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\233.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\235.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\237.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\238.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\239.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\240.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\241.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\242.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\243.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\245.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\246.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\247.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\248.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\249.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\25.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\252.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\253.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\256.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\257.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\258.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\259.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\26.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\260.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\262.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\263.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\265.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\267.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\268.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\269.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\27.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\270.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\273.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\275.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\278.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\280.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\282.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\283.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\286.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\288.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\289.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\29.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\290.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\291.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\294.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\298.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\3.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\30.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\302.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\303.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\304.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\306.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\307.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\309.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\31.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\310.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\311.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\314.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\315.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\317.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\319.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\32.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\321.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\322.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\323.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\324.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\325.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\326.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\327.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\328.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\329.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\330.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\331.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\333.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\334.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\335.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\336.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\338.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\339.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\34.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\340.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\341.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\342.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\344.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\345.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\348.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\349.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\35.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\350.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\354.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\355.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\356.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\357.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\358.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\360.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\362.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\363.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\364.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\365.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\366.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\367.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\369.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\37.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\371.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\374.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\375.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\376.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\377.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\379.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\38.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\380.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\382.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\383.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\386.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\387.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\388.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\389.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\390.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\392.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\393.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\394.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\396.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\397.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\398.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\40.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\404.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\408.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\409.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\410.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\411.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\412.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\413.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\415.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\416.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\417.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\418.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\419.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\42.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\420.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\423.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\425.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\426.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\427.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\428.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\43.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\430.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\431.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\433.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\435.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\436.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\437.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\438.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\440.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\443.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\444.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\445.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\446.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\449.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\45.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\450.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\451.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\452.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\454.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\455.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\456.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\458.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\459.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\460.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\461.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\462.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\463.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\464.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\466.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\467.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\468.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\469.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\470.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\475.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\478.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\48.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\481.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\482.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\485.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\487.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\488.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\489.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\49.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\490.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\491.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\492.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\493.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\494.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\496.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\497.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\498.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\499.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\5.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\50.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\501.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\502.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\505.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\506.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\508.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\510.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\511.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\513.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\514.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\516.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\517.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\519.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\520.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\521.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\522.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\523.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\524.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\528.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\530.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\532.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\535.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\538.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\539.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\54.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\542.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\543.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\545.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\547.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\548.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\55.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\550.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\551.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\552.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\553.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\556.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\558.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\559.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\56.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\561.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\562.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\563.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\564.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\566.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\568.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\569.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\570.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\571.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\572.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\573.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\576.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\58.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\581.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\582.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\585.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\588.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\589.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\592.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\593.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\595.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\596.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\597.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\598.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\6.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\60.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\600.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\601.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\603.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\605.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\606.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\609.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\61.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\610.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\614.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\615.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\616.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\618.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\619.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\620.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\621.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\622.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\623.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\628.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\629.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\63.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\630.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\631.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\632.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\633.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\634.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\635.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\636.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\637.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\64.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\641.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\642.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\645.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\646.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\647.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\649.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\651.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\652.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\654.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\656.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\657.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\66.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\660.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\661.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\662.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\663.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\664.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\667.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\669.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\67.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\671.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\672.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\673.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\675.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\676.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\677.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\678.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\680.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\681.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\682.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\684.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\685.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\689.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\69.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\692.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\693.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\694.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\695.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\697.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\698.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\700.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\704.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\706.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\709.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\71.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\710.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\711.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\712.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\713.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\714.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\716.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\717.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\718.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\72.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\720.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\721.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\722.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\724.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\726.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\727.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\728.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\73.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\730.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\732.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\733.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\737.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\738.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\740.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\741.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\742.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\745.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\747.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\748.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\749.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\75.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\752.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\753.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\754.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\755.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\756.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\757.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\758.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\759.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\76.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\760.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\762.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\763.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\765.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\766.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\767.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\768.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\769.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\771.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\772.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\774.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\776.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\777.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\778.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\779.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\78.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\784.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\785.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\787.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\788.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\789.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\792.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\793.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\794.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\796.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\797.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\798.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\8.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\80.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\800.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\801.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\802.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\803.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\805.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\806.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\807.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\808.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\809.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\81.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\810.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\811.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\812.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\813.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\815.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\816.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\82.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\821.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\822.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\823.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\824.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\825.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\826.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\827.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\828.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\83.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\831.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\837.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\838.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\839.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\84.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\840.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\841.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\844.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\847.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\848.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\849.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\85.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\850.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\851.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\852.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\853.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\854.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\855.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\856.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\858.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\859.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\860.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\861.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\862.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\863.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\864.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\865.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\866.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\867.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\868.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\869.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\87.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\870.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\871.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\873.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\874.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\875.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\876.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\877.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\878.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\879.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\88.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\881.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\882.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\883.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\885.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\886.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\892.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\893.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\894.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\896.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\897.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\898.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\899.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\900.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\901.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\904.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\906.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\907.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\908.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\910.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\911.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\912.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\914.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\915.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\916.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\917.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\918.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\919.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\92.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\920.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\923.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\924.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\926.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\927.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\928.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\931.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\932.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\933.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\934.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\936.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\937.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\938.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\939.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\942.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\944.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\948.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\949.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\951.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\952.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\954.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\955.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\956.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\958.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\959.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\960.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\961.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\962.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\963.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\966.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\968.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\969.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\970.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\971.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\972.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\975.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\976.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\978.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\979.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\981.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\983.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\985.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\987.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\989.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\990.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\992.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\993.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\994.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\995.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\996.jpg',\n",
       " 'D:\\\\CNN_practice\\\\leaf identification\\\\images\\\\train\\\\999.jpg']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_image(file_path):\n",
    "    img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE) #cv2.IMREAD_GRAYSCALE\n",
    "    return cv2.resize(img, (ROWS, COLS), interpolation=cv2.INTER_CUBIC)\n",
    "def prep_data(images):\n",
    "    count = len(images)\n",
    "    data = np.ndarray((count, CHANNELS, ROWS, COLS), dtype=np.uint8)\n",
    "\n",
    "    for i, image_file in enumerate(images):\n",
    "        image = read_image(image_file)\n",
    "        data[i] = image.T\n",
    "        if i%250 == 0: print('Processed {} of {}'.format(i, count))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 of 990\n",
      "Processed 250 of 990\n",
      "Processed 500 of 990\n",
      "Processed 750 of 990\n"
     ]
    }
   ],
   "source": [
    "train = prep_data(train_images).transpose((0,2,3,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(990, 64, 64, 1)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_59 (Conv2D)           (None, 64, 64, 32)        320       \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_60 (Conv2D)           (None, 64, 64, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 89)                2916441   \n",
      "=================================================================\n",
      "Total params: 2,926,009\n",
      "Trainable params: 2,926,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "image_model = Sequential()\n",
    "image_model.add(Conv2D(32,(3,3),input_shape=(ROWS,COLS,1),activation='relu',padding='same'))\n",
    "image_model.add(Dropout(0.5))\n",
    "image_model.add(Conv2D(32,(3,3),activation='relu',padding='same'))\n",
    "image_model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "#     model.add(Convolution2D(256, 3, 3, border_mode='same', activation='relu'))\n",
    "image_model.add(Flatten())\n",
    "image_model.add(Dense(89,input_dim=192,activation='relu'))\n",
    "image_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_21 (Dense)             (None, 360)               69480     \n",
      "=================================================================\n",
      "Total params: 69,480\n",
      "Trainable params: 69,480\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model8 = Sequential()\n",
    "model8.add(Dense(360,input_shape=(192,),activation='relu'))\n",
    "model8.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A `Concatenate` layer should be called on a list of at least 2 inputs",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-119-f260d33d7f64>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodelf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimage_model\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Python3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\layers\\merge.py\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(inputs, axis, **kwargs)\u001b[0m\n\u001b[0;32m    682\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mconcatenation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minputs\u001b[0m \u001b[0malongside\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    683\u001b[0m   \"\"\"\n\u001b[1;32m--> 684\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mConcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Python3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    312\u001b[0m     \"\"\"\n\u001b[0;32m    313\u001b[0m     \u001b[1;31m# Actually call the layer (optionally building it).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0margs\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_uses_inputs_arg'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Python3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    697\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'get_shape'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minput_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m             \u001b[0minput_shapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    700\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m           \u001b[1;31m# Note: not all sub-classes of Layer call Layer.__init__ (especially\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Python3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(instance, input_shape)\u001b[0m\n\u001b[0;32m    583\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m         \u001b[0minput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 585\u001b[1;33m     \u001b[0moutput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    586\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0moutput_shape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Python3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\layers\\merge.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    375\u001b[0m     \u001b[1;31m# Used purely for shape validation.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 377\u001b[1;33m       raise ValueError('A `Concatenate` layer should be called '\n\u001b[0m\u001b[0;32m    378\u001b[0m                        'on a list of at least 2 inputs')\n\u001b[0;32m    379\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: A `Concatenate` layer should be called on a list of at least 2 inputs"
     ]
    }
   ],
   "source": [
    "modelf=concatenate([model8,image_model],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Concatenate' object has no attribute 'add'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-96-e0158b8abf93>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodelf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m180\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodelf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'softmax'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Concatenate' object has no attribute 'add'"
     ]
    }
   ],
   "source": [
    "modelf.add(Dense(180,activation='relu'))\n",
    "modelf.add(Dense(CN, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_model():\n",
    "    # Define the image input\n",
    "    image = Input(shape=(ROWS, COLS, 1), name='image')\n",
    "    # Pass it through the first convolutional layer\n",
    "    x = Conv2D(8, (5, 5), input_shape=(96, 96, 1), padding='same',activation='relu')(image)\n",
    "    x = (MaxPool2D(pool_size=(2, 2), strides=(2, 2)))(x)\n",
    "\n",
    "    # Now through the second convolutional layer\n",
    "    x = (Conv2D(32, 5, 5, padding='same',activation='relu'))(x)\n",
    "    x = (MaxPool2D(pool_size=(2, 2), strides=(2, 2)))(x)\n",
    "\n",
    "    # Flatten our array\n",
    "    x = Flatten()(x)\n",
    "    # Define the pre-extracted feature input\n",
    "    numerical = Input(shape=(192,), name='numerical')\n",
    "    # Concatenate the output of our convnet with our pre-extracted feature input\n",
    "    concatenated = concatenate([x, numerical], axis=1)\n",
    "\n",
    "    # Add a fully connected layer just like in a normal MLP\n",
    "    x = Dense(100, activation='relu')(concatenated)\n",
    "    x = Dropout(.5)(x)\n",
    "\n",
    "    # Get the final output\n",
    "    out = Dense(99, activation='softmax')(x)\n",
    "    # How we create models with the Functional API\n",
    "    model = Model(inputs=[image, numerical], outputs=out)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelf = combined_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "image (InputLayer)              (None, 64, 64, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 64, 64, 8)    208         image[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_27 (MaxPooling2D) (None, 32, 32, 8)    0           conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 7, 7, 32)     6432        max_pooling2d_27[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_28 (MaxPooling2D) (None, 3, 3, 32)     0           conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_19 (Flatten)            (None, 288)          0           max_pooling2d_28[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "numerical (InputLayer)          (None, 192)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 480)          0           flatten_19[0][0]                 \n",
      "                                                                 numerical[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_30 (Dense)                (None, 100)          48100       concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, 100)          0           dense_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_31 (Dense)                (None, 99)           9999        dropout_22[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 64,739\n",
      "Trainable params: 64,739\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
