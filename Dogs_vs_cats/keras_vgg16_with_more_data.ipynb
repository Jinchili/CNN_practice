{
  "cells": [
    {
      "metadata": {
        "_cell_guid": "764fe135-c09e-9769-5794-500867154d93",
        "_uuid": "23ec68e73e3f1724f2fa3c7b06892e753180598f"
      },
      "cell_type": "markdown",
      "source": "Starter EDA and ConvNet implementation using Keras. \n\nInspiration for this notebook comes from this [Keras blog post](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html) and the [VGG ConvNet paper](https://arxiv.org/pdf/1409.1556.pdf). \n"
    },
    {
      "metadata": {
        "_cell_guid": "3d458c15-e131-f3c4-f756-843d6454bb37",
        "_uuid": "140f2311e321a245ff6edf2c536a3525ec9788fb",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "import os, cv2, random\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import ticker\nimport seaborn as sns\n%matplotlib inline \n\nfrom keras.models import Sequential\nfrom keras.layers import Input, Dropout, Flatten, MaxPooling2D, Dense, Activation,Conv2D\nfrom keras.optimizers import RMSprop\nfrom keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\nfrom keras.utils import np_utils",
      "execution_count": 19,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "04681981-55ac-7820-a0ae-38f98c851c39",
        "_uuid": "15078bf400f9584cbf956f9f46e04bf1c62dcca8"
      },
      "cell_type": "markdown",
      "source": "## Preparing the Data\n\nThis function resizes the images to 64x64 and samples 2000 images (8%) of the data to run efficiently as a Kaggle Kernel. I also separated cats and dogs for exploratory analysis. "
    },
    {
      "metadata": {
        "_cell_guid": "663d335e-1b84-a8cb-19ee-8f04839cf4e5",
        "_uuid": "9cd158880c26f19e5811749747d3f15b60ee0ae9",
        "trusted": true
      },
      "cell_type": "code",
      "source": "TRAIN_DIR = '../input/train/'\nTEST_DIR = '../input/test/'\n\nROWS = 64\nCOLS = 64\nCHANNELS = 3\n\ntrain_images = [TRAIN_DIR+i for i in os.listdir(TRAIN_DIR)] # use this for full dataset\ntrain_dogs =   [TRAIN_DIR+i for i in os.listdir(TRAIN_DIR) if 'dog' in i]\ntrain_cats =   [TRAIN_DIR+i for i in os.listdir(TRAIN_DIR) if 'cat' in i]\n\ntest_images =  [TEST_DIR+i for i in os.listdir(TEST_DIR)]\n\n\n# slice datasets for memory efficiency on Kaggle Kernels, delete if using full dataset\n#try to use more images this time\ntrain_images = train_dogs[:2500] + train_cats[:2500]\nrandom.shuffle(train_images)\n#test_images =  test_images[:25]\n\ndef read_image(file_path):\n    img = cv2.imread(file_path, cv2.IMREAD_COLOR) #cv2.IMREAD_GRAYSCALE\n    return cv2.resize(img, (ROWS, COLS), interpolation=cv2.INTER_CUBIC)\n\n\ndef prep_data(images):\n    count = len(images)\n    data = np.ndarray((count, CHANNELS, ROWS, COLS), dtype=np.uint8)\n\n    for i, image_file in enumerate(images):\n        image = read_image(image_file)\n        data[i] = image.T\n        if i%250 == 0: print('Processed {} of {}'.format(i, count))\n    \n    return data\n\n\n\ntrain = prep_data(train_images).transpose((0,2,3,1))\ntrain_length=train.shape[0]\ntest = prep_data(test_images).transpose((0,2,3,1))\nprint(\"Train shape: {}\".format(train.shape))\nprint(\"Test shape: {}\".format(test.shape))\n",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Processed 0 of 5000\nProcessed 250 of 5000\nProcessed 500 of 5000\nProcessed 750 of 5000\nProcessed 1000 of 5000\nProcessed 1250 of 5000\nProcessed 1500 of 5000\nProcessed 1750 of 5000\nProcessed 2000 of 5000\nProcessed 2250 of 5000\nProcessed 2500 of 5000\nProcessed 2750 of 5000\nProcessed 3000 of 5000\nProcessed 3250 of 5000\nProcessed 3500 of 5000\nProcessed 3750 of 5000\nProcessed 4000 of 5000\nProcessed 4250 of 5000\nProcessed 4500 of 5000\nProcessed 4750 of 5000\nProcessed 0 of 12500\nProcessed 250 of 12500\nProcessed 500 of 12500\nProcessed 750 of 12500\nProcessed 1000 of 12500\nProcessed 1250 of 12500\nProcessed 1500 of 12500\nProcessed 1750 of 12500\nProcessed 2000 of 12500\nProcessed 2250 of 12500\nProcessed 2500 of 12500\nProcessed 2750 of 12500\nProcessed 3000 of 12500\nProcessed 3250 of 12500\nProcessed 3500 of 12500\nProcessed 3750 of 12500\nProcessed 4000 of 12500\nProcessed 4250 of 12500\nProcessed 4500 of 12500\nProcessed 4750 of 12500\nProcessed 5000 of 12500\nProcessed 5250 of 12500\nProcessed 5500 of 12500\nProcessed 5750 of 12500\nProcessed 6000 of 12500\nProcessed 6250 of 12500\nProcessed 6500 of 12500\nProcessed 6750 of 12500\nProcessed 7000 of 12500\nProcessed 7250 of 12500\nProcessed 7500 of 12500\nProcessed 7750 of 12500\nProcessed 8000 of 12500\nProcessed 8250 of 12500\nProcessed 8500 of 12500\nProcessed 8750 of 12500\nProcessed 9000 of 12500\nProcessed 9250 of 12500\nProcessed 9500 of 12500\nProcessed 9750 of 12500\nProcessed 10000 of 12500\nProcessed 10250 of 12500\nProcessed 10500 of 12500\nProcessed 10750 of 12500\nProcessed 11000 of 12500\nProcessed 11250 of 12500\nProcessed 11500 of 12500\nProcessed 11750 of 12500\nProcessed 12000 of 12500\nProcessed 12250 of 12500\nTrain shape: (5000, 64, 64, 3)\nTest shape: (12500, 64, 64, 3)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "ed0fc95a-53bc-3517-245d-cf1f5122bc2d",
        "_uuid": "85cfcaa4bf73ffaed0411cdcc8cb4fdc3f078dcd"
      },
      "cell_type": "markdown",
      "source": "### Generating the Labels\n\nWe're dealing with a binary classification problem here - (1) dog (0) cat. The lables can be created by looping over the file names in the train directory. It's nice to see the training data is perfectly balanced. "
    },
    {
      "metadata": {
        "_cell_guid": "c0f2fbf6-8d78-7d42-6579-5486a36c1e60",
        "_uuid": "192cca4ebe4a72968bc66d4885221614cfb14f72",
        "trusted": true
      },
      "cell_type": "code",
      "source": "labels = []\nfor i in train_images:\n    if 'dog' in i:\n        labels.append(1)\n    else:\n        labels.append(0)\n\nsns.countplot(labels)\nplt.title('Cats and Dogs')",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 23,
          "data": {
            "text/plain": "Text(0.5,1,'Cats and Dogs')"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<matplotlib.figure.Figure at 0x7fb15629bef0>",
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEICAYAAABI7RO5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "4bb0bd2a-0512-aa72-c628-5b6ac946d97c",
        "_uuid": "36d0408d79d985f0ba098ce268dbd80ef54b2c91"
      },
      "cell_type": "markdown",
      "source": "### Checking out Cats and Dogs\nA quick side-by-side comparison of the animals."
    },
    {
      "metadata": {
        "_cell_guid": "0b7c79ae-6543-2ed8-e3f5-af4f5beb9cf7",
        "_uuid": "68808c3e929216a1376035ffa4a536a3e6e90827",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#def show_cats_and_dogs(idx):\n#    cat = read_image(train_cats[idx])\n#    dog = read_image(train_dogs[idx])\n#    pair = np.concatenate((cat, dog), axis=1)\n#    plt.figure(figsize=(10,5))\n#    plt.imshow(pair)\n#    plt.show()\n#    \n#for idx in range(train_length,train_length-5,):\n#    show_cats_and_dogs(idx)",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "599a11ba-aaa4-8266-03ea-d816518e2a81",
        "_uuid": "1dc579808f8513d12a0763a2ee2f3151e1481f18"
      },
      "cell_type": "markdown",
      "source": "### Your Average Cat and Dog Photo\n\nJust for fun, the mean pixel values for cats and dogs. I can almost see a resemblance, a ghost, if you will..."
    },
    {
      "metadata": {
        "_cell_guid": "7d3c6eb7-c370-9752-67c8-c780bab14abe",
        "_uuid": "3792f4031a1ee70c89be5b4b782c05e0832b90f8",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "dog_avg = np.array([dog[0].T for i, dog in enumerate(train) if labels[i]==1]).mean(axis=0)\nplt.imshow(dog_avg)\nplt.title('Your Average Dog')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "a5a93063-de92-18ce-b48f-68a193e90002",
        "_uuid": "23a06da03fe17cb2e1dab0e032f57a52bf64fc7f",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "cat_avg = np.array([cat[0].T for i, cat in enumerate(train) if labels[i]==0]).mean(axis=0)\nplt.imshow(cat_avg)\nplt.title('Your Average Cat')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "51e403b6-bcfc-b4fa-3770-9850cc86bae3",
        "_uuid": "f0a1716fc0cf0fbbf595d21b40cf9409d5852976"
      },
      "cell_type": "markdown",
      "source": "## CatdogNet-16\n\nA scaled down version of the VGG-16, with a few notable changes.\n\n- Number of convolution filters cut in half, fully connected (dense) layers scaled down. \n- Optimizer changed to `RMSprop`. \n- Output layer activation set to `sigmoid` for binary crossentropy. \n- Some layers commented out for efficiency.\n\nThe full network takes about 80s per epoch on a GTX1070 (or 2hr+ on CPU) on the full dataset.  (This script only trains on 8% of the 25K images. )"
    },
    {
      "metadata": {
        "_cell_guid": "4c477612-41d3-3112-5176-3c4ad9633080",
        "_uuid": "efbaf1aff4515371873ee97253791af3cd4ca879",
        "trusted": true
      },
      "cell_type": "code",
      "source": "optimizer = RMSprop(lr=1e-4)\nobjective = 'binary_crossentropy'\n\n\ndef catdog():\n    \n    model = Sequential()\n\n    model.add(Conv2D(32, (3, 3),input_shape=(ROWS, COLS,3), activation='relu',padding='same'))\n    model.add(Conv2D(32, (3, 3), activation='relu',padding='same'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n\n    model.add(Conv2D(64, (3, 3),activation='relu',padding='same'))\n    model.add(Conv2D(64, (3, 3),activation='relu',padding='same'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    \n    model.add(Conv2D(128, (3, 3), activation='relu',padding='same'))\n    model.add(Conv2D(128, (3, 3), activation='relu',padding='same'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    \n    model.add(Conv2D(256, (3, 3), activation='relu',padding='same'))\n    model.add(Dropout(0.5))\n    model.add(Conv2D(256, (3, 3), activation='relu',padding='same'))\n#     model.add(Convolution2D(256, 3, 3, border_mode='same', activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n\n#     model.add(Convolution2D(256, 3, 3, border_mode='same', activation='relu'))\n#     model.add(Convolution2D(256, 3, 3, border_mode='same', activation='relu'))\n#     model.add(Convolution2D(256, 3, 3, border_mode='same', activation='relu'))\n#     model.add(MaxPooling2D(pool_size=(2, 2)))\n\n    model.add(Flatten())\n    model.add(Dense(256, activation='relu'))\n    \n    model.add(Dense(256, activation='relu'))\n    model.add(Dropout(0.5))\n\n    model.add(Dense(1))\n    model.add(Activation('sigmoid'))\n\n    model.compile(loss=objective, optimizer=optimizer, metrics=['accuracy'])\n    return model\n\n\nmodel = catdog()\nmodel.summary()",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_49 (Conv2D)           (None, 64, 64, 32)        896       \n_________________________________________________________________\nconv2d_50 (Conv2D)           (None, 64, 64, 32)        9248      \n_________________________________________________________________\nmax_pooling2d_25 (MaxPooling (None, 32, 32, 32)        0         \n_________________________________________________________________\nconv2d_51 (Conv2D)           (None, 32, 32, 64)        18496     \n_________________________________________________________________\nconv2d_52 (Conv2D)           (None, 32, 32, 64)        36928     \n_________________________________________________________________\nmax_pooling2d_26 (MaxPooling (None, 16, 16, 64)        0         \n_________________________________________________________________\nconv2d_53 (Conv2D)           (None, 16, 16, 128)       73856     \n_________________________________________________________________\nconv2d_54 (Conv2D)           (None, 16, 16, 128)       147584    \n_________________________________________________________________\nmax_pooling2d_27 (MaxPooling (None, 8, 8, 128)         0         \n_________________________________________________________________\nconv2d_55 (Conv2D)           (None, 8, 8, 256)         295168    \n_________________________________________________________________\ndropout_19 (Dropout)         (None, 8, 8, 256)         0         \n_________________________________________________________________\nconv2d_56 (Conv2D)           (None, 8, 8, 256)         590080    \n_________________________________________________________________\nmax_pooling2d_28 (MaxPooling (None, 4, 4, 256)         0         \n_________________________________________________________________\nflatten_6 (Flatten)          (None, 4096)              0         \n_________________________________________________________________\ndense_16 (Dense)             (None, 256)               1048832   \n_________________________________________________________________\ndense_17 (Dense)             (None, 256)               65792     \n_________________________________________________________________\ndropout_20 (Dropout)         (None, 256)               0         \n_________________________________________________________________\ndense_18 (Dense)             (None, 1)                 257       \n_________________________________________________________________\nactivation_6 (Activation)    (None, 1)                 0         \n=================================================================\nTotal params: 2,287,137\nTrainable params: 2,287,137\nNon-trainable params: 0\n_________________________________________________________________\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "b7fbf156-2268-62ce-5fed-52a09af5e6f7",
        "_uuid": "a0c4f56f93d838852bda7aa82a43fe8b38cf948b"
      },
      "cell_type": "markdown",
      "source": "### Train and Predict\n\nI'm using Keras's early stopping callback to end training when the validation loss stops improving, otherwise the model will overfit. I will also be tracking the loss history on each epoch to visualize the overfitting trend. \n\nNote: A slice of 1000 images was used to fit the model for CPU efficency. The model's perfrmance improves significantly when used on the entire dataset. "
    },
    {
      "metadata": {
        "_cell_guid": "ca613169-4b41-e9d5-27c7-6629f8e035c8",
        "_uuid": "b5909d713b8ba08d5d29f766a5c361f6413422d7",
        "trusted": true
      },
      "cell_type": "code",
      "source": "nb_epoch = 10\nbatch_size = 10\n\n## Callback for loss logging per epoch\nclass LossHistory(Callback):\n    def on_train_begin(self, logs={}):\n        self.losses = []\n        self.val_losses = []\n        \n    def on_epoch_end(self, batch, logs={}):\n        self.losses.append(logs.get('loss'))\n        self.val_losses.append(logs.get('val_loss'))\n\nearly_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, mode='auto')        \n        \n#def run_catdog():\n    \n#    history = LossHistory()\n#    model.fit(train, labels, batch_size=batch_size, nb_epoch=nb_epoch,\n#              validation_split=0.25, verbose=0, shuffle=True, callbacks=[history, early_stopping])\n    \n\n#    predictions = model.predict(test, verbose=0)\n#    return predictions, history\n\n#predictions, history = run_catdog()\n\nmodel.fit(train,\n          labels,\n          epochs=nb_epoch,\n          batch_size=batch_size,\n          validation_split=0.25)\n",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Train on 3750 samples, validate on 1250 samples\nEpoch 1/10\n3000/3750 [=======================>......] - ETA: 28s - loss: 0.7592 - acc: 0.5037",
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-2a0542f765c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m           validation_split=0.25)\n\u001b[0m",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/Keras-2.1.6-py3.6.egg/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/Keras-2.1.6-py3.6.egg/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/Keras-2.1.6-py3.6.egg/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2631\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2633\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2634\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/Keras-2.1.6-py3.6.egg/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2601\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2602\u001b[0m                                 session)\n\u001b[0;32m-> 2603\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2604\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "6266d01c-9d4c-a1dc-8cb9-ce7b3107b536",
        "_uuid": "d9ff6321f3d293e0fb18430686d9c8e53a728082",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "loss = history.losses\nval_loss = history.val_losses\n\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('VGG-16 Loss Trend')\nplt.plot(loss, 'blue', label='Training Loss')\nplt.plot(val_loss, 'green', label='Validation Loss')\nplt.xticks(range(0,nb_epoch)[0::2])\nplt.legend()\nplt.show()",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "d9382e7d-31db-0d75-6876-aaccb126e4f6",
        "_uuid": "37a4464f2f4697f488ea839ca73f329846975130"
      },
      "cell_type": "markdown",
      "source": "## How'd We Do?\n\nI'm pretty sure I can distinguish a cat from a dog 100% of the time, but how confident is the model?...\n\nTip: Run on the full dataset with a GPU for a LB logloss of ~0.4 and accuracy at approx 90%. "
    },
    {
      "metadata": {
        "_cell_guid": "c665d01b-a5a7-2f7e-e5d4-09e108920e40",
        "_uuid": "b2e3a0b0102f59ef1746aaebee384d28d1b710b1",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#for i in range(0,10):\n#    if predictions[i, 0] >= 0.5: \n#        print('I am {:.2%} sure this is a Dog'.format(predictions[i][0]))\n#    else: \n#        print('I am {:.2%} sure this is a Cat'.format(1-predictions[i][0]))\n#        \n#    plt.imshow(test[i])\n#    plt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "287fbe4fbe3b25ca70c81208cb11daa38bbeb466",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "predictions = model.predict(test, batch_size=16, verbose=1)",
      "execution_count": 16,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "1b0113772e1c9a6c3bff5166f08b005f6bcbd691"
      },
      "cell_type": "code",
      "source": "from os.path import join, basename",
      "execution_count": 17,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a948f9249a31c0dac8bb1549efc0c68516fc43e2",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "with open('submission.csv','w') as f:\n    f.write('id,label\\n')\n    for index in range(len(test_images)):\n        img_id =basename(test_images[index]).split(\".\")[0]\n        prob = (predictions[index,0])\n        #print(\"index: {}, img_id: {}, prob:{}\".format(index,img_id, prob))\n        f.write(\"{},{}\\n\".format(img_id, prob))",
      "execution_count": 21,
      "outputs": []
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}